{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNG3qJflEHibPrNSh+GKNSh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alivarastepour/diabetes_prediction/blob/master/diabetes_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purpose of this notebook\n",
        "This notebook aims to build a model that determines whether a person is prone to diabetes or not. Additionally, it seeks to identify a subset of features (risk factors) that can accurately predict the risk of diabetes. The weights of the optimal solution will be utilized in another project, where they will be applied to users' inputs in real time.\n",
        "\n",
        "## Dataset\n",
        "This notebook makes use of a subset of a larger dataset which aimed to collect uniform, state-specific data on preventive health practices and risk behaviors that are associated with chronic diseases, injuries, and preventable infectious diseases in the adult population. The subset used in this notebook can be accessed [here](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_binary_5050split_health_indicators_BRFSS2015.csv)."
      ],
      "metadata": {
        "id": "q2IpptVI_N1M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "EtkfdEZs9h7u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LeakyReLU,Dropout\n",
        "from keras.optimizers import Adagrad, RMSprop, Adam\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.initializers import he_normal\n",
        "from keras.activations import selu\n",
        "from keras.metrics import Precision, Recall\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/drive')\n",
        "DATASET_ADDRESS = '/drive/MyDrive/diabetes_info.csv'\n",
        "raw_dataset = pd.read_csv(DATASET_ADDRESS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF0Coc72--j8",
        "outputId": "45ad41f7-f0b3-41c6-8eb5-93f8ea054ba1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5AdQ7V9_LDr",
        "outputId": "43025026-497b-4f8a-ad61-da79b01eaa52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 70692 entries, 0 to 70691\n",
            "Data columns (total 22 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   Diabetes_binary       70692 non-null  float64\n",
            " 1   HighBP                70692 non-null  float64\n",
            " 2   HighChol              70692 non-null  float64\n",
            " 3   CholCheck             70692 non-null  float64\n",
            " 4   BMI                   70692 non-null  float64\n",
            " 5   Smoker                70692 non-null  float64\n",
            " 6   Stroke                70692 non-null  float64\n",
            " 7   HeartDiseaseorAttack  70692 non-null  float64\n",
            " 8   PhysActivity          70692 non-null  float64\n",
            " 9   Fruits                70692 non-null  float64\n",
            " 10  Veggies               70692 non-null  float64\n",
            " 11  HvyAlcoholConsump     70692 non-null  float64\n",
            " 12  AnyHealthcare         70692 non-null  float64\n",
            " 13  NoDocbcCost           70692 non-null  float64\n",
            " 14  GenHlth               70692 non-null  float64\n",
            " 15  MentHlth              70692 non-null  float64\n",
            " 16  PhysHlth              70692 non-null  float64\n",
            " 17  DiffWalk              70692 non-null  float64\n",
            " 18  Sex                   70692 non-null  float64\n",
            " 19  Age                   70692 non-null  float64\n",
            " 20  Education             70692 non-null  float64\n",
            " 21  Income                70692 non-null  float64\n",
            "dtypes: float64(22)\n",
            "memory usage: 11.9 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = raw_dataset[\"Diabetes_binary\"]\n",
        "x = raw_dataset.drop(columns=[\"Diabetes_binary\"])"
      ],
      "metadata": {
        "id": "Di2eBqgLFkFT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=9)"
      ],
      "metadata": {
        "id": "SSYmWVclFr_V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection\n",
        "While our data may appear relatively clean, this does not guarantee optimal performance. Therefore, we must leverage a range of machine learning models to assess their effectiveness and identify potential modifications to the original data that can enhance the performance of our models."
      ],
      "metadata": {
        "id": "8k-7qJWsJsml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First model: Gradient boost classifier\n",
        "Boosting algorithms have been widely recognized as effective choices for handling tabular data. Among them, gradient boosting stands out as a prominent technique that leverages decision trees to create a powerful ensemble model. Nonetheless, to ensure its optimal performance, careful consideration should be given to hyperparameter tuning."
      ],
      "metadata": {
        "id": "xpB6qQ79KbGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(dataset):\n",
        "  y = dataset[\"Diabetes_binary\"]\n",
        "  x = dataset.drop(columns=[\"Diabetes_binary\"])\n",
        "  return train_test_split(x, y, test_size=0.20, random_state=9)"
      ],
      "metadata": {
        "id": "2bZnlwkq8_PX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_boost_classifier_model(dataset, learning_rate=0.05, n_estimators=150, subsample=0.8):\n",
        "  x_train, x_test, y_train, y_test = get_data(dataset)\n",
        "  reg = GradientBoostingClassifier(random_state=90,\n",
        "                                loss='deviance',\n",
        "                                learning_rate=learning_rate,\n",
        "                                n_estimators=n_estimators,\n",
        "                                subsample=subsample,\n",
        "                                criterion='friedman_mse',\n",
        "                                verbose=2,\n",
        "                                )\n",
        "  reg.fit(x_train, y_train)\n",
        "  y_pred = reg.predict(x_test)\n",
        "  report = classification_report(y_test, y_pred)\n",
        "  print(report)"
      ],
      "metadata": {
        "id": "WGLlf7wgCSDR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(raw_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJHCJ4TfCzce",
        "outputId": "1c6e98b0-cc2b-410a-89e4-e885fc3e64fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         1           1.3619           0.0245           10.46s\n",
            "         2           1.3400           0.0224            9.90s\n",
            "         3           1.3200           0.0203            9.96s\n",
            "         4           1.3008           0.0178            9.80s\n",
            "         5           1.2841           0.0163            9.61s\n",
            "         6           1.2685           0.0151            9.39s\n",
            "         7           1.2545           0.0136            9.39s\n",
            "         8           1.2417           0.0127            9.24s\n",
            "         9           1.2303           0.0117            9.12s\n",
            "        10           1.2191           0.0112            8.98s\n",
            "        11           1.2090           0.0101            8.87s\n",
            "        12           1.1982           0.0100            8.76s\n",
            "        13           1.1890           0.0094            8.71s\n",
            "        14           1.1793           0.0088            8.61s\n",
            "        15           1.1707           0.0078            8.56s\n",
            "        16           1.1641           0.0067            8.54s\n",
            "        17           1.1571           0.0076            8.46s\n",
            "        18           1.1502           0.0062            8.42s\n",
            "        19           1.1433           0.0058            8.35s\n",
            "        20           1.1379           0.0066            8.26s\n",
            "        21           1.1312           0.0059            8.19s\n",
            "        22           1.1278           0.0050            8.10s\n",
            "        23           1.1231           0.0050            8.04s\n",
            "        24           1.1165           0.0051            7.96s\n",
            "        25           1.1117           0.0040            7.89s\n",
            "        26           1.1087           0.0043            7.80s\n",
            "        27           1.1027           0.0045            7.72s\n",
            "        28           1.0968           0.0040            7.72s\n",
            "        29           1.0943           0.0034            7.66s\n",
            "        30           1.0898           0.0036            7.58s\n",
            "        31           1.0874           0.0029            7.50s\n",
            "        32           1.0858           0.0034            7.46s\n",
            "        33           1.0806           0.0030            7.44s\n",
            "        34           1.0780           0.0027            7.43s\n",
            "        35           1.0782           0.0027            7.35s\n",
            "        36           1.0716           0.0025            7.27s\n",
            "        37           1.0690           0.0025            7.25s\n",
            "        38           1.0686           0.0026            7.20s\n",
            "        39           1.0640           0.0022            7.13s\n",
            "        40           1.0635           0.0019            7.07s\n",
            "        41           1.0597           0.0017            7.01s\n",
            "        42           1.0594           0.0019            6.96s\n",
            "        43           1.0554           0.0017            6.89s\n",
            "        44           1.0557           0.0017            6.83s\n",
            "        45           1.0558           0.0018            6.77s\n",
            "        46           1.0505           0.0015            6.72s\n",
            "        47           1.0505           0.0018            6.65s\n",
            "        48           1.0462           0.0015            6.61s\n",
            "        49           1.0476           0.0012            6.54s\n",
            "        50           1.0454           0.0014            6.48s\n",
            "        51           1.0443           0.0012            6.42s\n",
            "        52           1.0434           0.0012            6.36s\n",
            "        53           1.0391           0.0011            6.30s\n",
            "        54           1.0407           0.0011            6.23s\n",
            "        55           1.0360           0.0011            6.16s\n",
            "        56           1.0381           0.0011            6.12s\n",
            "        57           1.0365           0.0010            6.05s\n",
            "        58           1.0363           0.0012            5.98s\n",
            "        59           1.0311           0.0008            5.91s\n",
            "        60           1.0329           0.0010            5.85s\n",
            "        61           1.0322           0.0009            5.78s\n",
            "        62           1.0295           0.0009            5.71s\n",
            "        63           1.0297           0.0009            5.66s\n",
            "        64           1.0304           0.0007            5.59s\n",
            "        65           1.0277           0.0008            5.53s\n",
            "        66           1.0294           0.0007            5.46s\n",
            "        67           1.0280           0.0009            5.40s\n",
            "        68           1.0247           0.0006            5.33s\n",
            "        69           1.0263           0.0006            5.27s\n",
            "        70           1.0220           0.0005            5.20s\n",
            "        71           1.0246           0.0006            5.13s\n",
            "        72           1.0214           0.0007            5.09s\n",
            "        73           1.0221           0.0007            5.04s\n",
            "        74           1.0198           0.0004            4.98s\n",
            "        75           1.0226           0.0006            4.91s\n",
            "        76           1.0190           0.0004            4.85s\n",
            "        77           1.0218           0.0006            4.79s\n",
            "        78           1.0171           0.0005            4.73s\n",
            "        79           1.0174           0.0004            4.66s\n",
            "        80           1.0173           0.0003            4.59s\n",
            "        81           1.0168           0.0005            4.52s\n",
            "        82           1.0148           0.0004            4.45s\n",
            "        83           1.0181           0.0004            4.38s\n",
            "        84           1.0173           0.0003            4.31s\n",
            "        85           1.0159           0.0003            4.24s\n",
            "        86           1.0146           0.0004            4.17s\n",
            "        87           1.0181           0.0003            4.10s\n",
            "        88           1.0144           0.0004            4.03s\n",
            "        89           1.0153           0.0003            3.96s\n",
            "        90           1.0082           0.0001            3.90s\n",
            "        91           1.0150           0.0004            3.83s\n",
            "        92           1.0121           0.0002            3.76s\n",
            "        93           1.0112           0.0003            3.69s\n",
            "        94           1.0107           0.0003            3.63s\n",
            "        95           1.0098           0.0002            3.56s\n",
            "        96           1.0082           0.0003            3.50s\n",
            "        97           1.0098           0.0003            3.43s\n",
            "        98           1.0100           0.0003            3.36s\n",
            "        99           1.0104           0.0001            3.29s\n",
            "       100           1.0070           0.0002            3.22s\n",
            "       101           1.0056           0.0001            3.16s\n",
            "       102           1.0088           0.0001            3.09s\n",
            "       103           1.0087           0.0002            3.03s\n",
            "       104           1.0112           0.0002            2.96s\n",
            "       105           1.0057           0.0002            2.89s\n",
            "       106           1.0039           0.0002            2.83s\n",
            "       107           1.0075           0.0001            2.76s\n",
            "       108           1.0104           0.0002            2.70s\n",
            "       109           1.0068           0.0001            2.63s\n",
            "       110           1.0056           0.0001            2.57s\n",
            "       111           1.0090           0.0001            2.50s\n",
            "       112           1.0080           0.0001            2.44s\n",
            "       113           1.0031           0.0001            2.38s\n",
            "       114           1.0058           0.0002            2.31s\n",
            "       115           1.0075           0.0002            2.25s\n",
            "       116           1.0070           0.0001            2.18s\n",
            "       117           1.0035           0.0001            2.11s\n",
            "       118           1.0052           0.0001            2.05s\n",
            "       119           1.0044           0.0001            1.99s\n",
            "       120           1.0032           0.0001            1.92s\n",
            "       121           1.0045           0.0001            1.86s\n",
            "       122           1.0009           0.0001            1.80s\n",
            "       123           1.0028           0.0001            1.74s\n",
            "       124           1.0061           0.0002            1.68s\n",
            "       125           0.9986           0.0000            1.62s\n",
            "       126           1.0018           0.0001            1.57s\n",
            "       127           1.0029           0.0001            1.51s\n",
            "       128           1.0038           0.0000            1.45s\n",
            "       129           1.0029           0.0000            1.38s\n",
            "       130           1.0039           0.0001            1.32s\n",
            "       131           1.0027           0.0001            1.26s\n",
            "       132           1.0010           0.0001            1.20s\n",
            "       133           1.0008          -0.0000            1.13s\n",
            "       134           1.0019          -0.0000            1.07s\n",
            "       135           1.0023           0.0000            1.00s\n",
            "       136           1.0003           0.0000            0.94s\n",
            "       137           1.0014           0.0001            0.88s\n",
            "       138           1.0026           0.0000            0.81s\n",
            "       139           0.9997           0.0001            0.75s\n",
            "       140           1.0030           0.0000            0.68s\n",
            "       141           0.9968           0.0000            0.61s\n",
            "       142           1.0005           0.0000            0.55s\n",
            "       143           1.0022           0.0000            0.48s\n",
            "       144           0.9993           0.0001            0.41s\n",
            "       145           1.0011           0.0000            0.34s\n",
            "       146           1.0000           0.0000            0.28s\n",
            "       147           0.9979           0.0000            0.21s\n",
            "       148           1.0005           0.0000            0.14s\n",
            "       149           1.0005           0.0000            0.07s\n",
            "       150           0.9998          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.73      7010\n",
            "         1.0       0.73      0.78      0.76      7129\n",
            "\n",
            "    accuracy                           0.75     14139\n",
            "   macro avg       0.75      0.75      0.75     14139\n",
            "weighted avg       0.75      0.75      0.75     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The deviance loss\n",
        "Deviance loss is a commonly used loss function in binary classification problems. With a glance at its formula, we can easily unserstand why:\n",
        "\n",
        "$$\n",
        "L(y, p) = \\left(y \\log(p) + (1 - y) \\log(1 - p)\\right)\n",
        "$$\n",
        "\n",
        "where y is true class and p is statistical probability.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uZs3QLvqMiLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F-1 score\n",
        "F-1 score uses precision(ratio of true possitives to true possitves and false possitives) and recall(ratio of true possitives to true possitves and false negatives) scores to prvoide a balance between them:\n",
        "\n",
        "$$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n"
      ],
      "metadata": {
        "id": "EQZOaVr-tgUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = cross_val_score(reg, x_train, y_train, cv=5, scoring='f1_macro')\n",
        "# print(\"cross validation scores(F-1) where k=5: \", scores)"
      ],
      "metadata": {
        "id": "DXcnTyapIGf-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = cross_val_score(reg, x_train, y_train, cv=10, scoring='f1_macro')\n",
        "# print(\"cross validation scores(F-1) where k=10: \", scores)"
      ],
      "metadata": {
        "id": "qMnr_XTivGkY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial evaluataion result\n",
        "As demonstrated above, whether employing Gradient Boosting with or without cross-validation, the F1 score hovers around 0.75. While this performance is acceptable, there is room for improvement."
      ],
      "metadata": {
        "id": "k3bKt0KavyOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second model: Logistic regression\n",
        "While Logistic Regression is typically considered a more linear model compared to ensemble methods, it remains a highly prevalent choice in classification problems. It offers several distinct advantages, such as strong interpretability, feature importance insights, and the ability to not only make binary classifications but also provide class probabilities. This probabilistic aspect can prove particularly valuable in certain situations.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nee1jLoqwq3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_model(dataset):\n",
        "  x_train, x_test, y_train, y_test = get_data(dataset)\n",
        "  log_reg = LogisticRegression(random_state=32, solver='sag', multi_class='multinomial', verbose=2, max_iter=500)\n",
        "  log_reg.fit(x_train, y_train)\n",
        "  y_pred_log_reg = log_reg.predict(x_test)\n",
        "  report_log_reg = classification_report(y_test, y_pred_log_reg)\n",
        "  print(log_reg.coef_)\n",
        "  print(report_log_reg)"
      ],
      "metadata": {
        "id": "Wf0DjjTz8ZgA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(raw_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_vmMs0o8ohm",
        "outputId": "3fc8dbde-355b-497c-fc76-adb1505792ab"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 328 epochs took 12 seconds\n",
            "[[ 0.73642276  0.5859419   1.35431797  0.07424616 -0.00842411  0.17406674\n",
            "   0.23032686 -0.02977515 -0.03619828 -0.08796805 -0.72938801  0.05957254\n",
            "   0.01305635  0.58596997 -0.00528506 -0.00804519  0.13615712  0.26243553\n",
            "   0.15226778 -0.02630124 -0.0588881 ]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation result\n",
        "Logistic regression exhibited slightly lower performance compared to Gradient Boosting, indicating that additional data preprocessing may be necessary to enhance model outcomes."
      ],
      "metadata": {
        "id": "4ogNdchF1lL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for class imbalance"
      ],
      "metadata": {
        "id": "Z8cK7DTC4WB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.bincount(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hRAk6_d3_f-",
        "outputId": "d300b919-e539-4333-ef17-45fdd5d2c667"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([35346, 35346])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardizing features\n",
        "In this section we standardize featuers that their domain may mislead oue models."
      ],
      "metadata": {
        "id": "xruScgS86yxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_standardize = list(x.keys())"
      ],
      "metadata": {
        "id": "yhunvldR63yy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "standarized_features = scaler.fit_transform(raw_dataset[columns_to_standardize])\n",
        "standardized_dataset = pd.DataFrame()\n",
        "standardized_dataset[\"Diabetes_binary\"] = raw_dataset[\"Diabetes_binary\"]\n",
        "standardized_dataset[columns_to_standardize] = standarized_features"
      ],
      "metadata": {
        "id": "BwgHyv_V7aZ9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(standardized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAo1SN_EC-WW",
        "outputId": "061b118d-31d5-4f77-a7f6-c3b60422bcd5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3619           0.0245            9.34s\n",
            "         2           1.3400           0.0224            9.54s\n",
            "         3           1.3200           0.0203            9.42s\n",
            "         4           1.3008           0.0178            9.69s\n",
            "         5           1.2841           0.0163            9.68s\n",
            "         6           1.2685           0.0151            9.56s\n",
            "         7           1.2545           0.0136            9.52s\n",
            "         8           1.2417           0.0127            9.55s\n",
            "         9           1.2303           0.0117            9.89s\n",
            "        10           1.2191           0.0112           10.40s\n",
            "        11           1.2090           0.0101           10.18s\n",
            "        12           1.1982           0.0100           10.01s\n",
            "        13           1.1890           0.0094            9.93s\n",
            "        14           1.1793           0.0088            9.77s\n",
            "        15           1.1707           0.0078            9.59s\n",
            "        16           1.1641           0.0067            9.41s\n",
            "        17           1.1571           0.0076            9.32s\n",
            "        18           1.1502           0.0062            9.22s\n",
            "        19           1.1433           0.0058            9.07s\n",
            "        20           1.1379           0.0066            8.99s\n",
            "        21           1.1312           0.0059            8.85s\n",
            "        22           1.1278           0.0050            8.73s\n",
            "        23           1.1231           0.0050            8.61s\n",
            "        24           1.1165           0.0051            8.50s\n",
            "        25           1.1117           0.0040            8.40s\n",
            "        26           1.1087           0.0043            8.35s\n",
            "        27           1.1027           0.0045            8.26s\n",
            "        28           1.0968           0.0040            8.15s\n",
            "        29           1.0943           0.0034            8.10s\n",
            "        30           1.0898           0.0036            8.04s\n",
            "        31           1.0874           0.0029            7.94s\n",
            "        32           1.0858           0.0034            7.85s\n",
            "        33           1.0806           0.0030            7.74s\n",
            "        34           1.0780           0.0027            7.66s\n",
            "        35           1.0782           0.0027            7.64s\n",
            "        36           1.0716           0.0025            7.64s\n",
            "        37           1.0690           0.0025            7.66s\n",
            "        38           1.0686           0.0026            7.66s\n",
            "        39           1.0640           0.0022            7.66s\n",
            "        40           1.0635           0.0019            7.66s\n",
            "        41           1.0597           0.0017            7.64s\n",
            "        42           1.0594           0.0019            7.65s\n",
            "        43           1.0554           0.0017            7.65s\n",
            "        44           1.0557           0.0017            7.63s\n",
            "        45           1.0558           0.0018            7.62s\n",
            "        46           1.0505           0.0015            7.61s\n",
            "        47           1.0505           0.0018            7.59s\n",
            "        48           1.0462           0.0015            7.56s\n",
            "        49           1.0476           0.0012            7.54s\n",
            "        50           1.0454           0.0014            7.50s\n",
            "        51           1.0443           0.0012            7.46s\n",
            "        52           1.0434           0.0012            7.42s\n",
            "        53           1.0391           0.0011            7.41s\n",
            "        54           1.0407           0.0011            7.38s\n",
            "        55           1.0360           0.0011            7.34s\n",
            "        56           1.0381           0.0011            7.35s\n",
            "        57           1.0365           0.0010            7.53s\n",
            "        58           1.0363           0.0012            7.52s\n",
            "        59           1.0311           0.0008            7.46s\n",
            "        60           1.0329           0.0010            7.40s\n",
            "        61           1.0322           0.0009            7.33s\n",
            "        62           1.0295           0.0009            7.27s\n",
            "        63           1.0297           0.0009            7.20s\n",
            "        64           1.0304           0.0007            7.13s\n",
            "        65           1.0277           0.0008            7.07s\n",
            "        66           1.0294           0.0007            7.00s\n",
            "        67           1.0280           0.0009            6.92s\n",
            "        68           1.0247           0.0006            6.85s\n",
            "        69           1.0263           0.0006            6.78s\n",
            "        70           1.0220           0.0005            6.69s\n",
            "        71           1.0246           0.0006            6.62s\n",
            "        72           1.0214           0.0007            6.54s\n",
            "        73           1.0221           0.0007            6.47s\n",
            "        74           1.0198           0.0004            6.41s\n",
            "        75           1.0226           0.0006            6.33s\n",
            "        76           1.0190           0.0004            6.25s\n",
            "        77           1.0218           0.0006            6.18s\n",
            "        78           1.0171           0.0005            6.10s\n",
            "        79           1.0174           0.0004            6.00s\n",
            "        80           1.0173           0.0003            5.90s\n",
            "        81           1.0168           0.0005            5.80s\n",
            "        82           1.0148           0.0004            5.69s\n",
            "        83           1.0181           0.0004            5.59s\n",
            "        84           1.0173           0.0003            5.49s\n",
            "        85           1.0159           0.0003            5.39s\n",
            "        86           1.0146           0.0004            5.29s\n",
            "        87           1.0181           0.0003            5.19s\n",
            "        88           1.0144           0.0004            5.10s\n",
            "        89           1.0153           0.0003            5.01s\n",
            "        90           1.0082           0.0001            4.91s\n",
            "        91           1.0150           0.0004            4.81s\n",
            "        92           1.0121           0.0002            4.72s\n",
            "        93           1.0112           0.0003            4.63s\n",
            "        94           1.0107           0.0003            4.53s\n",
            "        95           1.0098           0.0002            4.44s\n",
            "        96           1.0082           0.0003            4.35s\n",
            "        97           1.0098           0.0003            4.26s\n",
            "        98           1.0100           0.0003            4.17s\n",
            "        99           1.0104           0.0001            4.08s\n",
            "       100           1.0070           0.0002            3.99s\n",
            "       101           1.0056           0.0001            3.90s\n",
            "       102           1.0088           0.0001            3.82s\n",
            "       103           1.0087           0.0002            3.73s\n",
            "       104           1.0112           0.0002            3.64s\n",
            "       105           1.0057           0.0002            3.55s\n",
            "       106           1.0039           0.0002            3.47s\n",
            "       107           1.0075           0.0001            3.38s\n",
            "       108           1.0104           0.0002            3.30s\n",
            "       109           1.0068           0.0001            3.21s\n",
            "       110           1.0056           0.0001            3.13s\n",
            "       111           1.0090           0.0001            3.04s\n",
            "       112           1.0080           0.0001            2.96s\n",
            "       113           1.0031           0.0001            2.87s\n",
            "       114           1.0058           0.0002            2.79s\n",
            "       115           1.0075           0.0002            2.71s\n",
            "       116           1.0070           0.0001            2.62s\n",
            "       117           1.0035           0.0001            2.54s\n",
            "       118           1.0052           0.0001            2.46s\n",
            "       119           1.0044           0.0001            2.38s\n",
            "       120           1.0032           0.0001            2.30s\n",
            "       121           1.0045           0.0001            2.21s\n",
            "       122           1.0009           0.0001            2.13s\n",
            "       123           1.0028           0.0001            2.05s\n",
            "       124           1.0061           0.0002            1.98s\n",
            "       125           0.9986           0.0000            1.90s\n",
            "       126           1.0018           0.0001            1.82s\n",
            "       127           1.0029           0.0001            1.74s\n",
            "       128           1.0038           0.0000            1.66s\n",
            "       129           1.0029           0.0000            1.58s\n",
            "       130           1.0039           0.0001            1.50s\n",
            "       131           1.0027           0.0001            1.43s\n",
            "       132           1.0010           0.0001            1.35s\n",
            "       133           1.0008          -0.0000            1.27s\n",
            "       134           1.0019          -0.0000            1.20s\n",
            "       135           1.0023           0.0000            1.12s\n",
            "       136           1.0003           0.0000            1.04s\n",
            "       137           1.0014           0.0001            0.97s\n",
            "       138           1.0026           0.0000            0.89s\n",
            "       139           0.9997           0.0001            0.82s\n",
            "       140           1.0030           0.0000            0.74s\n",
            "       141           0.9968           0.0000            0.67s\n",
            "       142           1.0005           0.0000            0.59s\n",
            "       143           1.0022           0.0000            0.52s\n",
            "       144           0.9993           0.0001            0.44s\n",
            "       145           1.0011           0.0000            0.37s\n",
            "       146           1.0000           0.0000            0.30s\n",
            "       147           0.9979           0.0000            0.22s\n",
            "       148           1.0005           0.0000            0.15s\n",
            "       149           1.0005           0.0000            0.07s\n",
            "       150           0.9998          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.73      7010\n",
            "         1.0       0.73      0.78      0.76      7129\n",
            "\n",
            "    accuracy                           0.75     14139\n",
            "   macro avg       0.75      0.75      0.75     14139\n",
            "weighted avg       0.75      0.75      0.75     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(standardized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxiEcthP8uY3",
        "outputId": "7acf73fc-53c5-4d10-deeb-c1851ed30f14"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 35 epochs took 2 seconds\n",
            "[[ 0.1826531   0.14632449  0.10666609  0.26429747 -0.0020377   0.02104763\n",
            "   0.04085052 -0.00675311 -0.00880712 -0.01792627 -0.07395524  0.00629132\n",
            "   0.00203814  0.32646677 -0.02152548 -0.04053807  0.02956818  0.06543934\n",
            "   0.21726925 -0.01336101 -0.0640264 ]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing features\n",
        "Standardization helped the convergance of our model, but didn't countribute to the evaluation metrics. Now we try with normalized data."
      ],
      "metadata": {
        "id": "0MOiYC7bBXQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_normalize = list(x.keys())"
      ],
      "metadata": {
        "id": "bO4AayIo_wCw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "normalized_features = min_max_scaler.fit_transform(raw_dataset[columns_to_standardize])\n",
        "normalized_dataset = pd.DataFrame()\n",
        "normalized_dataset[\"Diabetes_binary\"] = raw_dataset[\"Diabetes_binary\"]\n",
        "normalized_dataset[columns_to_standardize] = normalized_features"
      ],
      "metadata": {
        "id": "0LIS3MbUBroL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(normalized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGrBEi4iDFzt",
        "outputId": "2b29da16-4036-47f2-9e2c-4a3f713b59da"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3619           0.0245            9.12s\n",
            "         2           1.3400           0.0224            9.39s\n",
            "         3           1.3200           0.0203            9.43s\n",
            "         4           1.3008           0.0178            9.36s\n",
            "         5           1.2841           0.0163            9.22s\n",
            "         6           1.2685           0.0151            9.36s\n",
            "         7           1.2545           0.0136            9.23s\n",
            "         8           1.2417           0.0127            9.11s\n",
            "         9           1.2303           0.0117            8.96s\n",
            "        10           1.2191           0.0112            8.85s\n",
            "        11           1.2090           0.0101            8.82s\n",
            "        12           1.1982           0.0100            8.74s\n",
            "        13           1.1890           0.0094            8.62s\n",
            "        14           1.1793           0.0088            8.53s\n",
            "        15           1.1707           0.0078            8.47s\n",
            "        16           1.1641           0.0067            8.44s\n",
            "        17           1.1571           0.0076            8.37s\n",
            "        18           1.1502           0.0062            8.32s\n",
            "        19           1.1433           0.0058            8.31s\n",
            "        20           1.1379           0.0066            8.35s\n",
            "        21           1.1312           0.0059            8.28s\n",
            "        22           1.1278           0.0050            8.21s\n",
            "        23           1.1231           0.0050            8.16s\n",
            "        24           1.1165           0.0051            8.10s\n",
            "        25           1.1117           0.0040            8.04s\n",
            "        26           1.1087           0.0043            8.04s\n",
            "        27           1.1027           0.0045            8.11s\n",
            "        28           1.0968           0.0040            8.19s\n",
            "        29           1.0943           0.0034            8.15s\n",
            "        30           1.0898           0.0036            8.07s\n",
            "        31           1.0874           0.0029            7.97s\n",
            "        32           1.0858           0.0034            7.91s\n",
            "        33           1.0806           0.0030            7.84s\n",
            "        34           1.0780           0.0027            7.75s\n",
            "        35           1.0782           0.0027            7.65s\n",
            "        36           1.0716           0.0025            7.55s\n",
            "        37           1.0690           0.0025            7.46s\n",
            "        38           1.0686           0.0026            7.38s\n",
            "        39           1.0640           0.0022            7.29s\n",
            "        40           1.0635           0.0019            7.20s\n",
            "        41           1.0597           0.0017            7.12s\n",
            "        42           1.0594           0.0019            7.05s\n",
            "        43           1.0554           0.0017            6.96s\n",
            "        44           1.0557           0.0017            6.89s\n",
            "        45           1.0558           0.0018            6.82s\n",
            "        46           1.0505           0.0015            6.75s\n",
            "        47           1.0505           0.0018            6.66s\n",
            "        48           1.0462           0.0015            6.58s\n",
            "        49           1.0476           0.0012            6.51s\n",
            "        50           1.0454           0.0014            6.47s\n",
            "        51           1.0443           0.0012            6.40s\n",
            "        52           1.0434           0.0012            6.32s\n",
            "        53           1.0391           0.0011            6.30s\n",
            "        54           1.0407           0.0011            6.27s\n",
            "        55           1.0360           0.0011            6.24s\n",
            "        56           1.0381           0.0011            6.21s\n",
            "        57           1.0365           0.0010            6.17s\n",
            "        58           1.0363           0.0012            6.14s\n",
            "        59           1.0311           0.0008            6.10s\n",
            "        60           1.0329           0.0010            6.06s\n",
            "        61           1.0322           0.0009            6.02s\n",
            "        62           1.0295           0.0009            6.00s\n",
            "        63           1.0297           0.0009            5.97s\n",
            "        64           1.0304           0.0007            5.93s\n",
            "        65           1.0277           0.0008            5.89s\n",
            "        66           1.0294           0.0007            5.85s\n",
            "        67           1.0280           0.0009            5.80s\n",
            "        68           1.0247           0.0006            5.76s\n",
            "        69           1.0263           0.0006            5.71s\n",
            "        70           1.0220           0.0005            5.67s\n",
            "        71           1.0246           0.0006            5.62s\n",
            "        72           1.0214           0.0007            5.57s\n",
            "        73           1.0221           0.0007            5.52s\n",
            "        74           1.0198           0.0004            5.50s\n",
            "        75           1.0226           0.0006            5.44s\n",
            "        76           1.0190           0.0004            5.38s\n",
            "        77           1.0218           0.0006            5.33s\n",
            "        78           1.0171           0.0005            5.27s\n",
            "        79           1.0174           0.0004            5.21s\n",
            "        80           1.0173           0.0003            5.16s\n",
            "        81           1.0168           0.0005            5.10s\n",
            "        82           1.0148           0.0004            5.03s\n",
            "        83           1.0181           0.0004            4.98s\n",
            "        84           1.0173           0.0003            4.91s\n",
            "        85           1.0159           0.0003            4.85s\n",
            "        86           1.0146           0.0004            4.79s\n",
            "        87           1.0181           0.0003            4.73s\n",
            "        88           1.0144           0.0004            4.67s\n",
            "        89           1.0153           0.0003            4.60s\n",
            "        90           1.0082           0.0001            4.54s\n",
            "        91           1.0150           0.0004            4.47s\n",
            "        92           1.0121           0.0002            4.41s\n",
            "        93           1.0112           0.0003            4.41s\n",
            "        94           1.0107           0.0003            4.40s\n",
            "        95           1.0098           0.0002            4.34s\n",
            "        96           1.0082           0.0003            4.27s\n",
            "        97           1.0098           0.0003            4.19s\n",
            "        98           1.0100           0.0003            4.12s\n",
            "        99           1.0104           0.0001            4.03s\n",
            "       100           1.0070           0.0002            3.94s\n",
            "       101           1.0056           0.0001            3.85s\n",
            "       102           1.0088           0.0001            3.77s\n",
            "       103           1.0087           0.0002            3.68s\n",
            "       104           1.0112           0.0002            3.60s\n",
            "       105           1.0057           0.0002            3.51s\n",
            "       106           1.0039           0.0002            3.43s\n",
            "       107           1.0075           0.0001            3.35s\n",
            "       108           1.0104           0.0002            3.27s\n",
            "       109           1.0068           0.0001            3.20s\n",
            "       110           1.0056           0.0001            3.13s\n",
            "       111           1.0090           0.0001            3.04s\n",
            "       112           1.0080           0.0001            2.96s\n",
            "       113           1.0031           0.0001            2.88s\n",
            "       114           1.0058           0.0002            2.80s\n",
            "       115           1.0075           0.0002            2.71s\n",
            "       116           1.0070           0.0001            2.63s\n",
            "       117           1.0035           0.0001            2.55s\n",
            "       118           1.0052           0.0001            2.47s\n",
            "       119           1.0044           0.0001            2.39s\n",
            "       120           1.0032           0.0001            2.31s\n",
            "       121           1.0045           0.0001            2.22s\n",
            "       122           1.0009           0.0001            2.14s\n",
            "       123           1.0028           0.0001            2.06s\n",
            "       124           1.0061           0.0002            1.98s\n",
            "       125           0.9986           0.0000            1.90s\n",
            "       126           1.0018           0.0001            1.83s\n",
            "       127           1.0029           0.0001            1.75s\n",
            "       128           1.0038           0.0000            1.67s\n",
            "       129           1.0029           0.0000            1.59s\n",
            "       130           1.0039           0.0001            1.51s\n",
            "       131           1.0027           0.0001            1.43s\n",
            "       132           1.0010           0.0001            1.36s\n",
            "       133           1.0008          -0.0000            1.28s\n",
            "       134           1.0019          -0.0000            1.20s\n",
            "       135           1.0023           0.0000            1.12s\n",
            "       136           1.0003           0.0000            1.05s\n",
            "       137           1.0014           0.0001            0.97s\n",
            "       138           1.0026           0.0000            0.90s\n",
            "       139           0.9997           0.0001            0.82s\n",
            "       140           1.0030           0.0000            0.74s\n",
            "       141           0.9968           0.0000            0.67s\n",
            "       142           1.0005           0.0000            0.59s\n",
            "       143           1.0022           0.0000            0.52s\n",
            "       144           0.9993           0.0001            0.45s\n",
            "       145           1.0011           0.0000            0.37s\n",
            "       146           1.0000           0.0000            0.30s\n",
            "       147           0.9979           0.0000            0.22s\n",
            "       148           1.0005           0.0000            0.15s\n",
            "       149           1.0005           0.0000            0.07s\n",
            "       150           0.9998          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.73      7010\n",
            "         1.0       0.73      0.78      0.76      7129\n",
            "\n",
            "    accuracy                           0.75     14139\n",
            "   macro avg       0.75      0.75      0.75     14139\n",
            "weighted avg       0.75      0.75      0.75     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(normalized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQZQbxroB6V3",
        "outputId": "59889820-a307-40b6-f4de-f8209fc2a736"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 38 epochs took 2 seconds\n",
            "[[ 0.36918431  0.29307524  0.68381231  3.15904998 -0.00423009  0.08667325\n",
            "   0.11512165 -0.01518912 -0.01825656 -0.04387366 -0.36555518  0.03063572\n",
            "   0.00687011  1.17187081 -0.07924642 -0.1206813   0.06894083  0.13120518\n",
            "   0.91025488 -0.06528242 -0.2056759 ]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What happened?\n",
        "It turned out that algorithms like Logitstic regression and Gradientboost are robust to data scale due to a number of factors like their loss functions, use of decision trees and regularization factors, etc. So we have to find another way to reach our goal.\n",
        "\n",
        "# Next model: DNN\n",
        "neural networks are the master of finding complex relations between featurse. In addition to that, they can be combined with various functionalities to improve model's behavoir even further, e.g. optimizers, regularization factors, etc."
      ],
      "metadata": {
        "id": "UN0ILPX_Bh9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dnn_model(dataset):\n",
        "  x_train, x_test, y_train, y_test = get_data(dataset)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, input_dim=x_train.shape[1], activation=LeakyReLU(alpha=0.1), kernel_initializer=he_normal()))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(32, activation='relu', kernel_regularizer=l1(0.1)))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  adam = Adagrad(learning_rate=0.1)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[Precision(), Recall()])\n",
        "  model.fit(x_train, y_train, epochs=100, verbose=2, validation_split=0.1, batch_size=100,)\n",
        "  res = model.evaluate(x_test, y_test)\n",
        "  print(\"binary cross-entropy loss : \", res[0], \" precision: \", res[1], \" recal: \", res[2])"
      ],
      "metadata": {
        "id": "FcQCSaw-665S"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model(standardized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPxwqurHQ7eu",
        "outputId": "08bf5588-4b1e-4b66-82b9-fdcf7f1847c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "509/509 - 5s - loss: 2.3837 - precision: 0.6401 - recall: 0.7215 - val_loss: 1.4089 - val_precision: 0.7310 - val_recall: 0.7902 - 5s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "509/509 - 2s - loss: 1.2959 - precision: 0.7052 - recall: 0.8169 - val_loss: 1.1773 - val_precision: 0.6942 - val_recall: 0.8593 - 2s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "509/509 - 2s - loss: 1.1039 - precision: 0.7093 - recall: 0.8221 - val_loss: 1.0201 - val_precision: 0.7001 - val_recall: 0.8586 - 2s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "509/509 - 1s - loss: 1.0011 - precision: 0.7103 - recall: 0.8227 - val_loss: 0.9590 - val_precision: 0.7173 - val_recall: 0.8149 - 1s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "509/509 - 1s - loss: 0.9352 - precision: 0.7112 - recall: 0.8237 - val_loss: 0.8928 - val_precision: 0.7160 - val_recall: 0.8142 - 1s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "509/509 - 1s - loss: 0.8932 - precision: 0.7115 - recall: 0.8235 - val_loss: 0.8717 - val_precision: 0.7032 - val_recall: 0.8468 - 1s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "509/509 - 1s - loss: 0.8673 - precision: 0.7124 - recall: 0.8256 - val_loss: 0.8331 - val_precision: 0.7083 - val_recall: 0.8353 - 1s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "509/509 - 1s - loss: 0.8388 - precision: 0.7142 - recall: 0.8254 - val_loss: 0.8141 - val_precision: 0.6959 - val_recall: 0.8636 - 1s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "509/509 - 1s - loss: 0.8102 - precision: 0.7144 - recall: 0.8259 - val_loss: 0.7887 - val_precision: 0.7191 - val_recall: 0.8113 - 1s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "509/509 - 1s - loss: 0.7916 - precision: 0.7124 - recall: 0.8230 - val_loss: 0.7671 - val_precision: 0.7141 - val_recall: 0.8253 - 1s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "509/509 - 1s - loss: 0.7736 - precision: 0.7129 - recall: 0.8244 - val_loss: 0.7579 - val_precision: 0.7113 - val_recall: 0.8346 - 1s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "509/509 - 2s - loss: 0.7605 - precision: 0.7140 - recall: 0.8241 - val_loss: 0.7316 - val_precision: 0.7070 - val_recall: 0.8407 - 2s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "509/509 - 2s - loss: 0.7445 - precision: 0.7138 - recall: 0.8258 - val_loss: 0.7349 - val_precision: 0.7200 - val_recall: 0.8074 - 2s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "509/509 - 2s - loss: 0.7396 - precision: 0.7142 - recall: 0.8228 - val_loss: 0.7207 - val_precision: 0.7118 - val_recall: 0.8328 - 2s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "509/509 - 1s - loss: 0.7302 - precision: 0.7156 - recall: 0.8231 - val_loss: 0.7186 - val_precision: 0.7075 - val_recall: 0.8400 - 1s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "509/509 - 1s - loss: 0.7229 - precision: 0.7162 - recall: 0.8232 - val_loss: 0.7052 - val_precision: 0.7160 - val_recall: 0.8142 - 1s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "509/509 - 1s - loss: 0.7126 - precision: 0.7150 - recall: 0.8210 - val_loss: 0.6943 - val_precision: 0.7135 - val_recall: 0.8249 - 1s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "509/509 - 1s - loss: 0.7034 - precision: 0.7169 - recall: 0.8234 - val_loss: 0.6881 - val_precision: 0.7113 - val_recall: 0.8317 - 1s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "509/509 - 1s - loss: 0.6955 - precision: 0.7148 - recall: 0.8213 - val_loss: 0.6751 - val_precision: 0.7141 - val_recall: 0.8210 - 1s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "509/509 - 1s - loss: 0.6923 - precision: 0.7155 - recall: 0.8231 - val_loss: 0.6804 - val_precision: 0.7131 - val_recall: 0.8285 - 1s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "509/509 - 1s - loss: 0.6884 - precision: 0.7180 - recall: 0.8229 - val_loss: 0.6698 - val_precision: 0.7139 - val_recall: 0.8203 - 1s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "509/509 - 1s - loss: 0.6813 - precision: 0.7163 - recall: 0.8230 - val_loss: 0.6706 - val_precision: 0.7246 - val_recall: 0.8006 - 1s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "509/509 - 1s - loss: 0.6778 - precision: 0.7156 - recall: 0.8194 - val_loss: 0.6674 - val_precision: 0.7186 - val_recall: 0.8099 - 1s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "509/509 - 2s - loss: 0.6722 - precision: 0.7169 - recall: 0.8198 - val_loss: 0.6561 - val_precision: 0.7097 - val_recall: 0.8378 - 2s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "509/509 - 2s - loss: 0.6654 - precision: 0.7164 - recall: 0.8202 - val_loss: 0.6548 - val_precision: 0.7347 - val_recall: 0.7823 - 2s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "509/509 - 1s - loss: 0.6631 - precision: 0.7178 - recall: 0.8214 - val_loss: 0.6559 - val_precision: 0.7110 - val_recall: 0.8349 - 1s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "509/509 - 1s - loss: 0.6604 - precision: 0.7166 - recall: 0.8199 - val_loss: 0.6407 - val_precision: 0.7150 - val_recall: 0.8181 - 1s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "509/509 - 1s - loss: 0.6599 - precision: 0.7173 - recall: 0.8215 - val_loss: 0.6522 - val_precision: 0.7121 - val_recall: 0.8281 - 1s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "509/509 - 1s - loss: 0.6540 - precision: 0.7186 - recall: 0.8196 - val_loss: 0.6356 - val_precision: 0.7100 - val_recall: 0.8407 - 1s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "509/509 - 1s - loss: 0.6514 - precision: 0.7185 - recall: 0.8218 - val_loss: 0.6503 - val_precision: 0.7311 - val_recall: 0.7866 - 1s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "509/509 - 1s - loss: 0.6468 - precision: 0.7185 - recall: 0.8229 - val_loss: 0.6256 - val_precision: 0.7155 - val_recall: 0.8203 - 1s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "509/509 - 1s - loss: 0.6445 - precision: 0.7169 - recall: 0.8193 - val_loss: 0.6410 - val_precision: 0.7155 - val_recall: 0.8178 - 1s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "509/509 - 1s - loss: 0.6425 - precision: 0.7181 - recall: 0.8192 - val_loss: 0.6209 - val_precision: 0.7092 - val_recall: 0.8392 - 1s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "509/509 - 1s - loss: 0.6383 - precision: 0.7169 - recall: 0.8206 - val_loss: 0.6352 - val_precision: 0.7121 - val_recall: 0.8317 - 1s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "509/509 - 2s - loss: 0.6351 - precision: 0.7177 - recall: 0.8181 - val_loss: 0.6165 - val_precision: 0.7113 - val_recall: 0.8346 - 2s/epoch - 4ms/step\n",
            "Epoch 36/100\n",
            "509/509 - 2s - loss: 0.6344 - precision: 0.7188 - recall: 0.8199 - val_loss: 0.6344 - val_precision: 0.7269 - val_recall: 0.7977 - 2s/epoch - 4ms/step\n",
            "Epoch 37/100\n",
            "509/509 - 1s - loss: 0.6316 - precision: 0.7178 - recall: 0.8193 - val_loss: 0.6124 - val_precision: 0.7086 - val_recall: 0.8417 - 1s/epoch - 3ms/step\n",
            "Epoch 38/100\n",
            "509/509 - 1s - loss: 0.6297 - precision: 0.7190 - recall: 0.8191 - val_loss: 0.6247 - val_precision: 0.7119 - val_recall: 0.8289 - 1s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "509/509 - 1s - loss: 0.6287 - precision: 0.7185 - recall: 0.8195 - val_loss: 0.6129 - val_precision: 0.7139 - val_recall: 0.8228 - 1s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "509/509 - 1s - loss: 0.6259 - precision: 0.7183 - recall: 0.8208 - val_loss: 0.6165 - val_precision: 0.7179 - val_recall: 0.8156 - 1s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "509/509 - 1s - loss: 0.6243 - precision: 0.7202 - recall: 0.8191 - val_loss: 0.6169 - val_precision: 0.7159 - val_recall: 0.8228 - 1s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "509/509 - 1s - loss: 0.6301 - precision: 0.7186 - recall: 0.8194 - val_loss: 0.6238 - val_precision: 0.7080 - val_recall: 0.8439 - 1s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "509/509 - 1s - loss: 0.6274 - precision: 0.7187 - recall: 0.8185 - val_loss: 0.6132 - val_precision: 0.7236 - val_recall: 0.8042 - 1s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "509/509 - 1s - loss: 0.6233 - precision: 0.7184 - recall: 0.8218 - val_loss: 0.6142 - val_precision: 0.7268 - val_recall: 0.7981 - 1s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "509/509 - 1s - loss: 0.6202 - precision: 0.7213 - recall: 0.8192 - val_loss: 0.6088 - val_precision: 0.7140 - val_recall: 0.8231 - 1s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "509/509 - 2s - loss: 0.6182 - precision: 0.7194 - recall: 0.8188 - val_loss: 0.6078 - val_precision: 0.7190 - val_recall: 0.8117 - 2s/epoch - 3ms/step\n",
            "Epoch 47/100\n",
            "509/509 - 2s - loss: 0.6173 - precision: 0.7190 - recall: 0.8171 - val_loss: 0.6059 - val_precision: 0.7117 - val_recall: 0.8306 - 2s/epoch - 3ms/step\n",
            "Epoch 48/100\n",
            "509/509 - 2s - loss: 0.6139 - precision: 0.7189 - recall: 0.8195 - val_loss: 0.6033 - val_precision: 0.7221 - val_recall: 0.8049 - 2s/epoch - 3ms/step\n",
            "Epoch 49/100\n",
            "509/509 - 1s - loss: 0.6125 - precision: 0.7177 - recall: 0.8180 - val_loss: 0.6013 - val_precision: 0.7152 - val_recall: 0.8199 - 1s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "509/509 - 1s - loss: 0.6099 - precision: 0.7204 - recall: 0.8199 - val_loss: 0.6048 - val_precision: 0.7244 - val_recall: 0.8016 - 1s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "509/509 - 1s - loss: 0.6104 - precision: 0.7200 - recall: 0.8185 - val_loss: 0.6001 - val_precision: 0.7239 - val_recall: 0.8016 - 1s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "509/509 - 1s - loss: 0.6088 - precision: 0.7203 - recall: 0.8191 - val_loss: 0.5995 - val_precision: 0.7145 - val_recall: 0.8235 - 1s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "509/509 - 1s - loss: 0.6077 - precision: 0.7197 - recall: 0.8186 - val_loss: 0.5954 - val_precision: 0.7228 - val_recall: 0.8077 - 1s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "509/509 - 1s - loss: 0.6046 - precision: 0.7197 - recall: 0.8181 - val_loss: 0.5952 - val_precision: 0.7297 - val_recall: 0.7927 - 1s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "509/509 - 1s - loss: 0.6029 - precision: 0.7197 - recall: 0.8174 - val_loss: 0.5924 - val_precision: 0.7163 - val_recall: 0.8153 - 1s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "509/509 - 1s - loss: 0.6011 - precision: 0.7198 - recall: 0.8204 - val_loss: 0.5924 - val_precision: 0.7109 - val_recall: 0.8375 - 1s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "509/509 - 1s - loss: 0.6001 - precision: 0.7193 - recall: 0.8166 - val_loss: 0.5921 - val_precision: 0.7202 - val_recall: 0.8120 - 1s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "509/509 - 2s - loss: 0.5981 - precision: 0.7205 - recall: 0.8182 - val_loss: 0.5872 - val_precision: 0.7139 - val_recall: 0.8274 - 2s/epoch - 3ms/step\n",
            "Epoch 59/100\n",
            "509/509 - 2s - loss: 0.5955 - precision: 0.7189 - recall: 0.8168 - val_loss: 0.5844 - val_precision: 0.7149 - val_recall: 0.8206 - 2s/epoch - 3ms/step\n",
            "Epoch 60/100\n",
            "509/509 - 1s - loss: 0.5928 - precision: 0.7206 - recall: 0.8170 - val_loss: 0.5853 - val_precision: 0.7098 - val_recall: 0.8407 - 1s/epoch - 3ms/step\n",
            "Epoch 61/100\n",
            "509/509 - 1s - loss: 0.5919 - precision: 0.7186 - recall: 0.8175 - val_loss: 0.5819 - val_precision: 0.7148 - val_recall: 0.8238 - 1s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "509/509 - 1s - loss: 0.5907 - precision: 0.7197 - recall: 0.8173 - val_loss: 0.5826 - val_precision: 0.7088 - val_recall: 0.8428 - 1s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "509/509 - 1s - loss: 0.5896 - precision: 0.7205 - recall: 0.8180 - val_loss: 0.5824 - val_precision: 0.7150 - val_recall: 0.8203 - 1s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "509/509 - 1s - loss: 0.5880 - precision: 0.7218 - recall: 0.8174 - val_loss: 0.5765 - val_precision: 0.7238 - val_recall: 0.8052 - 1s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "509/509 - 1s - loss: 0.5869 - precision: 0.7208 - recall: 0.8178 - val_loss: 0.5760 - val_precision: 0.7163 - val_recall: 0.8163 - 1s/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "509/509 - 1s - loss: 0.5863 - precision: 0.7206 - recall: 0.8175 - val_loss: 0.5778 - val_precision: 0.7159 - val_recall: 0.8156 - 1s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "509/509 - 1s - loss: 0.5846 - precision: 0.7205 - recall: 0.8180 - val_loss: 0.5725 - val_precision: 0.7144 - val_recall: 0.8238 - 1s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "509/509 - 1s - loss: 0.5840 - precision: 0.7198 - recall: 0.8175 - val_loss: 0.5769 - val_precision: 0.7147 - val_recall: 0.8253 - 1s/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "509/509 - 1s - loss: 0.5828 - precision: 0.7210 - recall: 0.8179 - val_loss: 0.5717 - val_precision: 0.7233 - val_recall: 0.8077 - 1s/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "509/509 - 2s - loss: 0.5803 - precision: 0.7212 - recall: 0.8177 - val_loss: 0.5729 - val_precision: 0.7150 - val_recall: 0.8228 - 2s/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "509/509 - 2s - loss: 0.5793 - precision: 0.7202 - recall: 0.8173 - val_loss: 0.5688 - val_precision: 0.7105 - val_recall: 0.8382 - 2s/epoch - 3ms/step\n",
            "Epoch 72/100\n",
            "509/509 - 1s - loss: 0.5787 - precision: 0.7203 - recall: 0.8169 - val_loss: 0.5714 - val_precision: 0.7152 - val_recall: 0.8192 - 1s/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "509/509 - 1s - loss: 0.5767 - precision: 0.7215 - recall: 0.8175 - val_loss: 0.5644 - val_precision: 0.7151 - val_recall: 0.8178 - 1s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "509/509 - 1s - loss: 0.5757 - precision: 0.7216 - recall: 0.8166 - val_loss: 0.5746 - val_precision: 0.7115 - val_recall: 0.8364 - 1s/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "509/509 - 1s - loss: 0.5768 - precision: 0.7195 - recall: 0.8154 - val_loss: 0.5637 - val_precision: 0.7156 - val_recall: 0.8181 - 1s/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "509/509 - 1s - loss: 0.5751 - precision: 0.7205 - recall: 0.8157 - val_loss: 0.5678 - val_precision: 0.7205 - val_recall: 0.8124 - 1s/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "509/509 - 1s - loss: 0.5754 - precision: 0.7217 - recall: 0.8167 - val_loss: 0.5615 - val_precision: 0.7153 - val_recall: 0.8213 - 1s/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "509/509 - 1s - loss: 0.5740 - precision: 0.7215 - recall: 0.8159 - val_loss: 0.5661 - val_precision: 0.7203 - val_recall: 0.8124 - 1s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "509/509 - 1s - loss: 0.5716 - precision: 0.7202 - recall: 0.8174 - val_loss: 0.5621 - val_precision: 0.7102 - val_recall: 0.8371 - 1s/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "509/509 - 1s - loss: 0.5706 - precision: 0.7211 - recall: 0.8182 - val_loss: 0.5662 - val_precision: 0.7248 - val_recall: 0.8063 - 1s/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "509/509 - 1s - loss: 0.5705 - precision: 0.7212 - recall: 0.8158 - val_loss: 0.5629 - val_precision: 0.7104 - val_recall: 0.8389 - 1s/epoch - 3ms/step\n",
            "Epoch 82/100\n",
            "509/509 - 2s - loss: 0.5694 - precision: 0.7209 - recall: 0.8176 - val_loss: 0.5597 - val_precision: 0.7252 - val_recall: 0.8031 - 2s/epoch - 3ms/step\n",
            "Epoch 83/100\n",
            "509/509 - 2s - loss: 0.5682 - precision: 0.7225 - recall: 0.8192 - val_loss: 0.5595 - val_precision: 0.7162 - val_recall: 0.8213 - 2s/epoch - 3ms/step\n",
            "Epoch 84/100\n",
            "509/509 - 1s - loss: 0.5681 - precision: 0.7201 - recall: 0.8173 - val_loss: 0.5641 - val_precision: 0.7129 - val_recall: 0.8314 - 1s/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "509/509 - 1s - loss: 0.5702 - precision: 0.7200 - recall: 0.8162 - val_loss: 0.5561 - val_precision: 0.7144 - val_recall: 0.8249 - 1s/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "509/509 - 1s - loss: 0.5689 - precision: 0.7208 - recall: 0.8164 - val_loss: 0.5623 - val_precision: 0.7153 - val_recall: 0.8203 - 1s/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "509/509 - 1s - loss: 0.5683 - precision: 0.7200 - recall: 0.8191 - val_loss: 0.5593 - val_precision: 0.7259 - val_recall: 0.7991 - 1s/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "509/509 - 1s - loss: 0.5683 - precision: 0.7212 - recall: 0.8158 - val_loss: 0.5610 - val_precision: 0.7101 - val_recall: 0.8375 - 1s/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "509/509 - 1s - loss: 0.5669 - precision: 0.7208 - recall: 0.8158 - val_loss: 0.5561 - val_precision: 0.7183 - val_recall: 0.8153 - 1s/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "509/509 - 1s - loss: 0.5644 - precision: 0.7202 - recall: 0.8149 - val_loss: 0.5589 - val_precision: 0.7155 - val_recall: 0.8160 - 1s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "509/509 - 1s - loss: 0.5641 - precision: 0.7205 - recall: 0.8180 - val_loss: 0.5545 - val_precision: 0.7164 - val_recall: 0.8160 - 1s/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "509/509 - 1s - loss: 0.5639 - precision: 0.7232 - recall: 0.8174 - val_loss: 0.5586 - val_precision: 0.7086 - val_recall: 0.8410 - 1s/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "509/509 - 1s - loss: 0.5666 - precision: 0.7206 - recall: 0.8165 - val_loss: 0.5597 - val_precision: 0.7211 - val_recall: 0.8120 - 1s/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "509/509 - 2s - loss: 0.5645 - precision: 0.7212 - recall: 0.8140 - val_loss: 0.5527 - val_precision: 0.7156 - val_recall: 0.8181 - 2s/epoch - 4ms/step\n",
            "Epoch 95/100\n",
            "509/509 - 2s - loss: 0.5631 - precision: 0.7224 - recall: 0.8167 - val_loss: 0.5606 - val_precision: 0.7049 - val_recall: 0.8525 - 2s/epoch - 3ms/step\n",
            "Epoch 96/100\n",
            "509/509 - 1s - loss: 0.5609 - precision: 0.7205 - recall: 0.8154 - val_loss: 0.5504 - val_precision: 0.7154 - val_recall: 0.8163 - 1s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "509/509 - 1s - loss: 0.5610 - precision: 0.7217 - recall: 0.8178 - val_loss: 0.5574 - val_precision: 0.7056 - val_recall: 0.8503 - 1s/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "509/509 - 1s - loss: 0.5609 - precision: 0.7223 - recall: 0.8169 - val_loss: 0.5512 - val_precision: 0.7161 - val_recall: 0.8174 - 1s/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "509/509 - 1s - loss: 0.5597 - precision: 0.7215 - recall: 0.8175 - val_loss: 0.5520 - val_precision: 0.7144 - val_recall: 0.8285 - 1s/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "509/509 - 1s - loss: 0.5601 - precision: 0.7205 - recall: 0.8173 - val_loss: 0.5537 - val_precision: 0.7097 - val_recall: 0.8367 - 1s/epoch - 2ms/step\n",
            "442/442 [==============================] - 1s 2ms/step - loss: 0.5518 - precision: 0.7220 - recall: 0.8313\n",
            "binary cross-entropy loss :  0.551797091960907  precision:  0.7219785451889038  recal:  0.8312526345252991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result\n",
        "As we saw, different models are not showing significant result improvements. So we may need to make some changes to our data"
      ],
      "metadata": {
        "id": "dAVWhISP03jL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The correlation matrix and its usage\n",
        "Correlation matrix simply explains the relationship between columns of a dataset. The correlation coefficient ranges between -1 and 1. A correlation coefficient of 1 indicates a perfect positive correlation, meaning that the two variables increase or decrease together in a linear relationship. A correlation coefficient of -1 indicates a perfect negative correlation, meaning that the two variables move in opposite directions in a linear relationship. A correlation coefficient close to 0 suggests no linear relationship between the variables.\n",
        "\n",
        "This matrix can be helpful when finding an optimal subset of features."
      ],
      "metadata": {
        "id": "xLjc4tO9DSgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_correlations(correlation_matrix):\n",
        "  return sorted(correlation_matrix.items(), key=lambda x:abs(x[1]))"
      ],
      "metadata": {
        "id": "-8WPCmWe4cF7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correlations(dataset):\n",
        "  columns = dataset.keys()\n",
        "  correlation = dataset[columns].corr()\n",
        "  return correlation[\"Diabetes_binary\"]"
      ],
      "metadata": {
        "id": "nAo1fc4AMXdy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_map = sort_correlations(get_correlations(raw_dataset))"
      ],
      "metadata": {
        "id": "sxLDZpig1bFM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd1e-mUK2-IJ",
        "outputId": "1ee82bc0-13f6-471a-ccf4-947e63c0a857"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('AnyHealthcare', 0.02319074853112824),\n",
              " ('NoDocbcCost', 0.040976573266643494),\n",
              " ('Sex', 0.044412858371260695),\n",
              " ('Fruits', -0.05407655628666651),\n",
              " ('Veggies', -0.07929314561269872),\n",
              " ('Smoker', 0.08599896420800192),\n",
              " ('MentHlth', 0.08702877147509416),\n",
              " ('HvyAlcoholConsump', -0.09485313995926549),\n",
              " ('CholCheck', 0.11538161710270915),\n",
              " ('Stroke', 0.12542678468516733),\n",
              " ('PhysActivity', -0.15866560486405157),\n",
              " ('Education', -0.17048063498806143),\n",
              " ('HeartDiseaseorAttack', 0.21152340436022687),\n",
              " ('PhysHlth', 0.21308101903810317),\n",
              " ('Income', -0.2244487149638171),\n",
              " ('DiffWalk', 0.272646006159808),\n",
              " ('Age', 0.27873806628188813),\n",
              " ('HighChol', 0.28921280708865016),\n",
              " ('BMI', 0.29337274476103575),\n",
              " ('HighBP', 0.3815155489073117),\n",
              " ('GenHlth', 0.4076115984949182),\n",
              " ('Diabetes_binary', 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keep_features = map(lambda b: b[0], filter(lambda a: abs(a[1]) > 0.25, correlation_map))"
      ],
      "metadata": {
        "id": "mgYZgcME11WI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_dataset = raw_dataset[list(keep_features)]"
      ],
      "metadata": {
        "id": "uGKhE_F72VFN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(modified_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ1ed4oV2rZH",
        "outputId": "66d2aadb-8954-4652-f5c6-d65f0d957891"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 38 epochs took 1 seconds\n",
            "[[0.0702186  0.08500237 0.29923042 0.03782045 0.39012718 0.30067847]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection result: Logistic regression\n",
        "By reducing the feature count using the correlation matrix and only keeping faetures that have more meaningful relationship with the target featurse, Logistic regression not only converged faster, it also kept its accuracy."
      ],
      "metadata": {
        "id": "-5gt8cFd4ZRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(modified_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMTIIVdU5Grg",
        "outputId": "01d21523-2590-4699-85e0-7288e522a214"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3619           0.0245            6.90s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         2           1.3400           0.0224            7.39s\n",
            "         3           1.3200           0.0203            7.24s\n",
            "         4           1.3008           0.0178            7.18s\n",
            "         5           1.2841           0.0163            7.52s\n",
            "         6           1.2685           0.0151            7.54s\n",
            "         7           1.2545           0.0136            7.64s\n",
            "         8           1.2417           0.0127            7.70s\n",
            "         9           1.2303           0.0117            7.57s\n",
            "        10           1.2191           0.0112            7.60s\n",
            "        11           1.2090           0.0101            7.60s\n",
            "        12           1.1982           0.0100            7.52s\n",
            "        13           1.1890           0.0094            7.45s\n",
            "        14           1.1793           0.0088            7.42s\n",
            "        15           1.1707           0.0078            7.44s\n",
            "        16           1.1641           0.0067            7.54s\n",
            "        17           1.1571           0.0076            7.47s\n",
            "        18           1.1502           0.0062            7.39s\n",
            "        19           1.1433           0.0058            7.29s\n",
            "        20           1.1379           0.0066            7.25s\n",
            "        21           1.1312           0.0059            7.18s\n",
            "        22           1.1278           0.0050            7.11s\n",
            "        23           1.1231           0.0050            7.04s\n",
            "        24           1.1165           0.0051            6.98s\n",
            "        25           1.1116           0.0041            6.90s\n",
            "        26           1.1086           0.0043            6.90s\n",
            "        27           1.1027           0.0044            6.83s\n",
            "        28           1.0970           0.0040            6.78s\n",
            "        29           1.0944           0.0038            6.69s\n",
            "        30           1.0900           0.0036            6.63s\n",
            "        31           1.0874           0.0029            6.55s\n",
            "        32           1.0857           0.0036            6.51s\n",
            "        33           1.0808           0.0026            6.44s\n",
            "        34           1.0781           0.0026            6.37s\n",
            "        35           1.0782           0.0031            6.31s\n",
            "        36           1.0720           0.0021            6.25s\n",
            "        37           1.0696           0.0023            6.21s\n",
            "        38           1.0695           0.0021            6.16s\n",
            "        39           1.0648           0.0022            6.09s\n",
            "        40           1.0644           0.0019            6.04s\n",
            "        41           1.0604           0.0020            5.97s\n",
            "        42           1.0601           0.0017            5.91s\n",
            "        43           1.0566           0.0014            5.85s\n",
            "        44           1.0571           0.0015            5.79s\n",
            "        45           1.0577           0.0017            5.72s\n",
            "        46           1.0524           0.0013            5.67s\n",
            "        47           1.0527           0.0014            5.60s\n",
            "        48           1.0483           0.0014            5.53s\n",
            "        49           1.0504           0.0012            5.47s\n",
            "        50           1.0480           0.0011            5.41s\n",
            "        51           1.0470           0.0012            5.35s\n",
            "        52           1.0462           0.0011            5.29s\n",
            "        53           1.0420           0.0010            5.23s\n",
            "        54           1.0439           0.0010            5.17s\n",
            "        55           1.0393           0.0007            5.13s\n",
            "        56           1.0415           0.0008            5.06s\n",
            "        57           1.0402           0.0007            5.01s\n",
            "        58           1.0404           0.0008            4.96s\n",
            "        59           1.0355           0.0007            4.91s\n",
            "        60           1.0376           0.0007            4.86s\n",
            "        61           1.0370           0.0007            4.79s\n",
            "        62           1.0347           0.0006            4.73s\n",
            "        63           1.0350           0.0006            4.67s\n",
            "        64           1.0356           0.0006            4.62s\n",
            "        65           1.0330           0.0005            4.56s\n",
            "        66           1.0351           0.0005            4.50s\n",
            "        67           1.0345           0.0005            4.45s\n",
            "        68           1.0309           0.0004            4.38s\n",
            "        69           1.0330           0.0005            4.33s\n",
            "        70           1.0284           0.0004            4.27s\n",
            "        71           1.0312           0.0004            4.21s\n",
            "        72           1.0286           0.0004            4.16s\n",
            "        73           1.0294           0.0004            4.09s\n",
            "        74           1.0274           0.0003            4.01s\n",
            "        75           1.0304           0.0003            3.94s\n",
            "        76           1.0274           0.0002            3.87s\n",
            "        77           1.0306           0.0004            3.80s\n",
            "        78           1.0249           0.0003            3.73s\n",
            "        79           1.0261           0.0003            3.66s\n",
            "        80           1.0262           0.0003            3.59s\n",
            "        81           1.0256           0.0003            3.52s\n",
            "        82           1.0238           0.0002            3.46s\n",
            "        83           1.0270           0.0002            3.39s\n",
            "        84           1.0269           0.0003            3.33s\n",
            "        85           1.0256           0.0001            3.26s\n",
            "        86           1.0239           0.0002            3.20s\n",
            "        87           1.0283           0.0002            3.13s\n",
            "        88           1.0240           0.0002            3.07s\n",
            "        89           1.0250           0.0002            3.01s\n",
            "        90           1.0180           0.0001            2.95s\n",
            "        91           1.0255           0.0003            2.89s\n",
            "        92           1.0225           0.0002            2.83s\n",
            "        93           1.0221           0.0001            2.77s\n",
            "        94           1.0214           0.0001            2.71s\n",
            "        95           1.0207           0.0002            2.66s\n",
            "        96           1.0190           0.0001            2.60s\n",
            "        97           1.0218           0.0001            2.55s\n",
            "        98           1.0213           0.0001            2.49s\n",
            "        99           1.0222           0.0001            2.44s\n",
            "       100           1.0190           0.0001            2.38s\n",
            "       101           1.0176           0.0001            2.33s\n",
            "       102           1.0208           0.0000            2.28s\n",
            "       103           1.0218           0.0001            2.22s\n",
            "       104           1.0241           0.0001            2.17s\n",
            "       105           1.0176           0.0000            2.11s\n",
            "       106           1.0163          -0.0000            2.06s\n",
            "       107           1.0198           0.0000            2.01s\n",
            "       108           1.0230           0.0001            1.96s\n",
            "       109           1.0202           0.0000            1.90s\n",
            "       110           1.0191           0.0000            1.85s\n",
            "       111           1.0228           0.0000            1.80s\n",
            "       112           1.0212           0.0000            1.75s\n",
            "       113           1.0165           0.0000            1.70s\n",
            "       114           1.0194           0.0000            1.65s\n",
            "       115           1.0215           0.0001            1.60s\n",
            "       116           1.0212           0.0000            1.55s\n",
            "       117           1.0174          -0.0000            1.50s\n",
            "       118           1.0188          -0.0000            1.45s\n",
            "       119           1.0187           0.0000            1.40s\n",
            "       120           1.0172           0.0000            1.36s\n",
            "       121           1.0195           0.0000            1.31s\n",
            "       122           1.0155          -0.0000            1.26s\n",
            "       123           1.0172           0.0000            1.21s\n",
            "       124           1.0196           0.0000            1.16s\n",
            "       125           1.0133          -0.0000            1.12s\n",
            "       126           1.0162          -0.0000            1.07s\n",
            "       127           1.0184           0.0000            1.02s\n",
            "       128           1.0189          -0.0000            0.98s\n",
            "       129           1.0192           0.0000            0.93s\n",
            "       130           1.0197           0.0000            0.89s\n",
            "       131           1.0183          -0.0000            0.84s\n",
            "       132           1.0172          -0.0000            0.79s\n",
            "       133           1.0165          -0.0000            0.75s\n",
            "       134           1.0183          -0.0000            0.70s\n",
            "       135           1.0182          -0.0000            0.66s\n",
            "       136           1.0159          -0.0000            0.61s\n",
            "       137           1.0182           0.0000            0.57s\n",
            "       138           1.0194           0.0000            0.52s\n",
            "       139           1.0150          -0.0001            0.48s\n",
            "       140           1.0194           0.0000            0.43s\n",
            "       141           1.0140          -0.0000            0.39s\n",
            "       142           1.0169          -0.0000            0.35s\n",
            "       143           1.0190          -0.0000            0.30s\n",
            "       144           1.0157          -0.0000            0.26s\n",
            "       145           1.0176          -0.0000            0.21s\n",
            "       146           1.0167          -0.0000            0.17s\n",
            "       147           1.0149          -0.0000            0.13s\n",
            "       148           1.0178          -0.0000            0.09s\n",
            "       149           1.0178          -0.0000            0.04s\n",
            "       150           1.0166          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.70      0.73      7010\n",
            "         1.0       0.73      0.78      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection result: GradientBoost\n",
        "GradientBoost was also capable of keeping its performance after feature selection. It is worthy of noting that tuning hyperparameters had a mild effect on this model but it was negligible."
      ],
      "metadata": {
        "id": "lLWu9esz6ZYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model(modified_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK9KPK-l6Y_S",
        "outputId": "3f28e42b-d9c4-4c60-c8e6-b7e69921a812"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "509/509 - 2s - loss: 2.9741 - precision_1: 0.5214 - recall_1: 0.4159 - val_loss: 1.4772 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 2s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "509/509 - 1s - loss: 1.3527 - precision_1: 0.5436 - recall_1: 0.4692 - val_loss: 1.2555 - val_precision_1: 0.5331 - val_recall_1: 0.9868 - 1s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "509/509 - 1s - loss: 1.1727 - precision_1: 0.5754 - recall_1: 0.6129 - val_loss: 1.0844 - val_precision_1: 0.7782 - val_recall_1: 0.5414 - 1s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "509/509 - 1s - loss: 1.0471 - precision_1: 0.6267 - recall_1: 0.6636 - val_loss: 0.9666 - val_precision_1: 0.7491 - val_recall_1: 0.6950 - 1s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "509/509 - 1s - loss: 0.9486 - precision_1: 0.6753 - recall_1: 0.6951 - val_loss: 0.8899 - val_precision_1: 0.6659 - val_recall_1: 0.8861 - 1s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "509/509 - 1s - loss: 0.8885 - precision_1: 0.6906 - recall_1: 0.7119 - val_loss: 0.8377 - val_precision_1: 0.7251 - val_recall_1: 0.7573 - 1s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "509/509 - 2s - loss: 0.8485 - precision_1: 0.6958 - recall_1: 0.7153 - val_loss: 0.7944 - val_precision_1: 0.7130 - val_recall_1: 0.7809 - 2s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "509/509 - 2s - loss: 0.8156 - precision_1: 0.7025 - recall_1: 0.7179 - val_loss: 0.7759 - val_precision_1: 0.7266 - val_recall_1: 0.7651 - 2s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "509/509 - 1s - loss: 0.7938 - precision_1: 0.7030 - recall_1: 0.7199 - val_loss: 0.7504 - val_precision_1: 0.7157 - val_recall_1: 0.7877 - 1s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "509/509 - 1s - loss: 0.7728 - precision_1: 0.7054 - recall_1: 0.7227 - val_loss: 0.7367 - val_precision_1: 0.7146 - val_recall_1: 0.7827 - 1s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "509/509 - 1s - loss: 0.7597 - precision_1: 0.7083 - recall_1: 0.7282 - val_loss: 0.7214 - val_precision_1: 0.7314 - val_recall_1: 0.7537 - 1s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "509/509 - 1s - loss: 0.7444 - precision_1: 0.7055 - recall_1: 0.7288 - val_loss: 0.7106 - val_precision_1: 0.7101 - val_recall_1: 0.7988 - 1s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "509/509 - 1s - loss: 0.7329 - precision_1: 0.7067 - recall_1: 0.7327 - val_loss: 0.7003 - val_precision_1: 0.7370 - val_recall_1: 0.7354 - 1s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "509/509 - 1s - loss: 0.7254 - precision_1: 0.7080 - recall_1: 0.7319 - val_loss: 0.6930 - val_precision_1: 0.7185 - val_recall_1: 0.7841 - 1s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "509/509 - 1s - loss: 0.7169 - precision_1: 0.7089 - recall_1: 0.7356 - val_loss: 0.6831 - val_precision_1: 0.7251 - val_recall_1: 0.7705 - 1s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "509/509 - 1s - loss: 0.7055 - precision_1: 0.7105 - recall_1: 0.7357 - val_loss: 0.6802 - val_precision_1: 0.7325 - val_recall_1: 0.7490 - 1s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "509/509 - 1s - loss: 0.7004 - precision_1: 0.7086 - recall_1: 0.7368 - val_loss: 0.6690 - val_precision_1: 0.7066 - val_recall_1: 0.8124 - 1s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "509/509 - 2s - loss: 0.6943 - precision_1: 0.7109 - recall_1: 0.7371 - val_loss: 0.6645 - val_precision_1: 0.7256 - val_recall_1: 0.7744 - 2s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "509/509 - 2s - loss: 0.6864 - precision_1: 0.7103 - recall_1: 0.7392 - val_loss: 0.6577 - val_precision_1: 0.7289 - val_recall_1: 0.7587 - 2s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "509/509 - 1s - loss: 0.6796 - precision_1: 0.7107 - recall_1: 0.7382 - val_loss: 0.6529 - val_precision_1: 0.7009 - val_recall_1: 0.8231 - 1s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "509/509 - 1s - loss: 0.6753 - precision_1: 0.7097 - recall_1: 0.7400 - val_loss: 0.6509 - val_precision_1: 0.7289 - val_recall_1: 0.7644 - 1s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "509/509 - 1s - loss: 0.6715 - precision_1: 0.7089 - recall_1: 0.7451 - val_loss: 0.6460 - val_precision_1: 0.7251 - val_recall_1: 0.7744 - 1s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "509/509 - 1s - loss: 0.6670 - precision_1: 0.7132 - recall_1: 0.7437 - val_loss: 0.6414 - val_precision_1: 0.7127 - val_recall_1: 0.8038 - 1s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "509/509 - 1s - loss: 0.6635 - precision_1: 0.7110 - recall_1: 0.7510 - val_loss: 0.6396 - val_precision_1: 0.7277 - val_recall_1: 0.7626 - 1s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "509/509 - 1s - loss: 0.6626 - precision_1: 0.7109 - recall_1: 0.7480 - val_loss: 0.6391 - val_precision_1: 0.6967 - val_recall_1: 0.8371 - 1s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "509/509 - 1s - loss: 0.6584 - precision_1: 0.7104 - recall_1: 0.7466 - val_loss: 0.6332 - val_precision_1: 0.7339 - val_recall_1: 0.7515 - 1s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "509/509 - 1s - loss: 0.6537 - precision_1: 0.7104 - recall_1: 0.7434 - val_loss: 0.6300 - val_precision_1: 0.7206 - val_recall_1: 0.7848 - 1s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "509/509 - 1s - loss: 0.6486 - precision_1: 0.7104 - recall_1: 0.7504 - val_loss: 0.6241 - val_precision_1: 0.7027 - val_recall_1: 0.8260 - 1s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "509/509 - 2s - loss: 0.6465 - precision_1: 0.7124 - recall_1: 0.7498 - val_loss: 0.6220 - val_precision_1: 0.7120 - val_recall_1: 0.8020 - 2s/epoch - 3ms/step\n",
            "Epoch 30/100\n",
            "509/509 - 2s - loss: 0.6426 - precision_1: 0.7115 - recall_1: 0.7509 - val_loss: 0.6176 - val_precision_1: 0.7304 - val_recall_1: 0.7662 - 2s/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "509/509 - 2s - loss: 0.6379 - precision_1: 0.7134 - recall_1: 0.7539 - val_loss: 0.6150 - val_precision_1: 0.7091 - val_recall_1: 0.8110 - 2s/epoch - 3ms/step\n",
            "Epoch 32/100\n",
            "509/509 - 1s - loss: 0.6372 - precision_1: 0.7099 - recall_1: 0.7525 - val_loss: 0.6138 - val_precision_1: 0.7247 - val_recall_1: 0.7834 - 1s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "509/509 - 1s - loss: 0.6342 - precision_1: 0.7128 - recall_1: 0.7503 - val_loss: 0.6129 - val_precision_1: 0.7257 - val_recall_1: 0.7748 - 1s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "509/509 - 1s - loss: 0.6317 - precision_1: 0.7116 - recall_1: 0.7557 - val_loss: 0.6109 - val_precision_1: 0.7291 - val_recall_1: 0.7680 - 1s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "509/509 - 1s - loss: 0.6324 - precision_1: 0.7115 - recall_1: 0.7539 - val_loss: 0.6097 - val_precision_1: 0.7270 - val_recall_1: 0.7683 - 1s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "509/509 - 1s - loss: 0.6276 - precision_1: 0.7127 - recall_1: 0.7540 - val_loss: 0.6063 - val_precision_1: 0.6990 - val_recall_1: 0.8324 - 1s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "509/509 - 1s - loss: 0.6279 - precision_1: 0.7132 - recall_1: 0.7547 - val_loss: 0.6037 - val_precision_1: 0.7261 - val_recall_1: 0.7784 - 1s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "509/509 - 1s - loss: 0.6278 - precision_1: 0.7125 - recall_1: 0.7546 - val_loss: 0.6112 - val_precision_1: 0.6917 - val_recall_1: 0.8460 - 1s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "509/509 - 1s - loss: 0.6256 - precision_1: 0.7142 - recall_1: 0.7565 - val_loss: 0.6003 - val_precision_1: 0.7196 - val_recall_1: 0.7902 - 1s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "509/509 - 1s - loss: 0.6222 - precision_1: 0.7115 - recall_1: 0.7570 - val_loss: 0.6102 - val_precision_1: 0.7443 - val_recall_1: 0.7193 - 1s/epoch - 3ms/step\n",
            "Epoch 41/100\n",
            "509/509 - 2s - loss: 0.6200 - precision_1: 0.7132 - recall_1: 0.7569 - val_loss: 0.6012 - val_precision_1: 0.7300 - val_recall_1: 0.7608 - 2s/epoch - 3ms/step\n",
            "Epoch 42/100\n",
            "509/509 - 2s - loss: 0.6190 - precision_1: 0.7129 - recall_1: 0.7579 - val_loss: 0.5976 - val_precision_1: 0.7192 - val_recall_1: 0.7934 - 2s/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "509/509 - 1s - loss: 0.6203 - precision_1: 0.7121 - recall_1: 0.7555 - val_loss: 0.5963 - val_precision_1: 0.7235 - val_recall_1: 0.7852 - 1s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "509/509 - 1s - loss: 0.6152 - precision_1: 0.7161 - recall_1: 0.7607 - val_loss: 0.5970 - val_precision_1: 0.7261 - val_recall_1: 0.7737 - 1s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "509/509 - 1s - loss: 0.6159 - precision_1: 0.7122 - recall_1: 0.7598 - val_loss: 0.5945 - val_precision_1: 0.7275 - val_recall_1: 0.7694 - 1s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "509/509 - 1s - loss: 0.6140 - precision_1: 0.7151 - recall_1: 0.7636 - val_loss: 0.5936 - val_precision_1: 0.7222 - val_recall_1: 0.7895 - 1s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "509/509 - 1s - loss: 0.6135 - precision_1: 0.7138 - recall_1: 0.7630 - val_loss: 0.5961 - val_precision_1: 0.7290 - val_recall_1: 0.7619 - 1s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "509/509 - 1s - loss: 0.6124 - precision_1: 0.7138 - recall_1: 0.7631 - val_loss: 0.5915 - val_precision_1: 0.7118 - val_recall_1: 0.8067 - 1s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "509/509 - 1s - loss: 0.6105 - precision_1: 0.7137 - recall_1: 0.7610 - val_loss: 0.5908 - val_precision_1: 0.7212 - val_recall_1: 0.7909 - 1s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "509/509 - 1s - loss: 0.6098 - precision_1: 0.7152 - recall_1: 0.7638 - val_loss: 0.5917 - val_precision_1: 0.7008 - val_recall_1: 0.8242 - 1s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "509/509 - 1s - loss: 0.6074 - precision_1: 0.7138 - recall_1: 0.7679 - val_loss: 0.5882 - val_precision_1: 0.7273 - val_recall_1: 0.7762 - 1s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "509/509 - 2s - loss: 0.6070 - precision_1: 0.7132 - recall_1: 0.7641 - val_loss: 0.5894 - val_precision_1: 0.7052 - val_recall_1: 0.8238 - 2s/epoch - 3ms/step\n",
            "Epoch 53/100\n",
            "509/509 - 2s - loss: 0.6069 - precision_1: 0.7143 - recall_1: 0.7670 - val_loss: 0.5866 - val_precision_1: 0.7082 - val_recall_1: 0.8167 - 2s/epoch - 3ms/step\n",
            "Epoch 54/100\n",
            "509/509 - 2s - loss: 0.6056 - precision_1: 0.7146 - recall_1: 0.7635 - val_loss: 0.5870 - val_precision_1: 0.7259 - val_recall_1: 0.7737 - 2s/epoch - 3ms/step\n",
            "Epoch 55/100\n",
            "509/509 - 1s - loss: 0.6025 - precision_1: 0.7149 - recall_1: 0.7670 - val_loss: 0.5838 - val_precision_1: 0.7169 - val_recall_1: 0.7999 - 1s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "509/509 - 1s - loss: 0.6028 - precision_1: 0.7149 - recall_1: 0.7706 - val_loss: 0.5866 - val_precision_1: 0.7359 - val_recall_1: 0.7512 - 1s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "509/509 - 1s - loss: 0.6018 - precision_1: 0.7138 - recall_1: 0.7700 - val_loss: 0.5797 - val_precision_1: 0.7168 - val_recall_1: 0.8027 - 1s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "509/509 - 1s - loss: 0.6003 - precision_1: 0.7127 - recall_1: 0.7674 - val_loss: 0.5809 - val_precision_1: 0.7133 - val_recall_1: 0.8045 - 1s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "509/509 - 1s - loss: 0.5971 - precision_1: 0.7158 - recall_1: 0.7677 - val_loss: 0.5806 - val_precision_1: 0.7070 - val_recall_1: 0.8188 - 1s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "509/509 - 1s - loss: 0.5966 - precision_1: 0.7169 - recall_1: 0.7692 - val_loss: 0.5788 - val_precision_1: 0.7025 - val_recall_1: 0.8260 - 1s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "509/509 - 1s - loss: 0.5972 - precision_1: 0.7164 - recall_1: 0.7667 - val_loss: 0.5793 - val_precision_1: 0.7196 - val_recall_1: 0.7902 - 1s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "509/509 - 1s - loss: 0.5960 - precision_1: 0.7160 - recall_1: 0.7681 - val_loss: 0.5765 - val_precision_1: 0.7123 - val_recall_1: 0.8074 - 1s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "509/509 - 1s - loss: 0.5958 - precision_1: 0.7163 - recall_1: 0.7690 - val_loss: 0.5767 - val_precision_1: 0.7074 - val_recall_1: 0.8206 - 1s/epoch - 3ms/step\n",
            "Epoch 64/100\n",
            "509/509 - 2s - loss: 0.5934 - precision_1: 0.7173 - recall_1: 0.7692 - val_loss: 0.5745 - val_precision_1: 0.7332 - val_recall_1: 0.7587 - 2s/epoch - 4ms/step\n",
            "Epoch 65/100\n",
            "509/509 - 2s - loss: 0.5945 - precision_1: 0.7169 - recall_1: 0.7667 - val_loss: 0.5768 - val_precision_1: 0.7288 - val_recall_1: 0.7648 - 2s/epoch - 4ms/step\n",
            "Epoch 66/100\n",
            "509/509 - 1s - loss: 0.5927 - precision_1: 0.7152 - recall_1: 0.7730 - val_loss: 0.5745 - val_precision_1: 0.7356 - val_recall_1: 0.7519 - 1s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "509/509 - 1s - loss: 0.5913 - precision_1: 0.7183 - recall_1: 0.7701 - val_loss: 0.5744 - val_precision_1: 0.7261 - val_recall_1: 0.7773 - 1s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "509/509 - 1s - loss: 0.5916 - precision_1: 0.7176 - recall_1: 0.7684 - val_loss: 0.5700 - val_precision_1: 0.7110 - val_recall_1: 0.8131 - 1s/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "509/509 - 1s - loss: 0.5876 - precision_1: 0.7172 - recall_1: 0.7726 - val_loss: 0.5719 - val_precision_1: 0.7167 - val_recall_1: 0.8034 - 1s/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "509/509 - 1s - loss: 0.5870 - precision_1: 0.7185 - recall_1: 0.7692 - val_loss: 0.5692 - val_precision_1: 0.7207 - val_recall_1: 0.7880 - 1s/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "509/509 - 1s - loss: 0.5869 - precision_1: 0.7174 - recall_1: 0.7679 - val_loss: 0.5691 - val_precision_1: 0.7223 - val_recall_1: 0.7859 - 1s/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "509/509 - 1s - loss: 0.5854 - precision_1: 0.7193 - recall_1: 0.7697 - val_loss: 0.5710 - val_precision_1: 0.7471 - val_recall_1: 0.7275 - 1s/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "509/509 - 1s - loss: 0.5868 - precision_1: 0.7186 - recall_1: 0.7701 - val_loss: 0.5711 - val_precision_1: 0.7348 - val_recall_1: 0.7501 - 1s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "509/509 - 1s - loss: 0.5843 - precision_1: 0.7183 - recall_1: 0.7685 - val_loss: 0.5649 - val_precision_1: 0.7111 - val_recall_1: 0.8124 - 1s/epoch - 3ms/step\n",
            "Epoch 75/100\n",
            "509/509 - 2s - loss: 0.5833 - precision_1: 0.7190 - recall_1: 0.7709 - val_loss: 0.5687 - val_precision_1: 0.7259 - val_recall_1: 0.7823 - 2s/epoch - 4ms/step\n",
            "Epoch 76/100\n",
            "509/509 - 2s - loss: 0.5832 - precision_1: 0.7160 - recall_1: 0.7694 - val_loss: 0.5653 - val_precision_1: 0.7279 - val_recall_1: 0.7769 - 2s/epoch - 3ms/step\n",
            "Epoch 77/100\n",
            "509/509 - 1s - loss: 0.5823 - precision_1: 0.7207 - recall_1: 0.7695 - val_loss: 0.5679 - val_precision_1: 0.7304 - val_recall_1: 0.7691 - 1s/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "509/509 - 1s - loss: 0.5823 - precision_1: 0.7185 - recall_1: 0.7716 - val_loss: 0.5643 - val_precision_1: 0.7260 - val_recall_1: 0.7809 - 1s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "509/509 - 1s - loss: 0.5809 - precision_1: 0.7183 - recall_1: 0.7723 - val_loss: 0.5675 - val_precision_1: 0.7326 - val_recall_1: 0.7630 - 1s/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "509/509 - 1s - loss: 0.5817 - precision_1: 0.7172 - recall_1: 0.7728 - val_loss: 0.5654 - val_precision_1: 0.7352 - val_recall_1: 0.7494 - 1s/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "509/509 - 1s - loss: 0.5799 - precision_1: 0.7179 - recall_1: 0.7726 - val_loss: 0.5651 - val_precision_1: 0.7167 - val_recall_1: 0.8006 - 1s/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "509/509 - 1s - loss: 0.5794 - precision_1: 0.7181 - recall_1: 0.7711 - val_loss: 0.5616 - val_precision_1: 0.7364 - val_recall_1: 0.7533 - 1s/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "509/509 - 1s - loss: 0.5789 - precision_1: 0.7182 - recall_1: 0.7727 - val_loss: 0.5651 - val_precision_1: 0.7262 - val_recall_1: 0.7777 - 1s/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "509/509 - 1s - loss: 0.5771 - precision_1: 0.7187 - recall_1: 0.7729 - val_loss: 0.5586 - val_precision_1: 0.7174 - val_recall_1: 0.7963 - 1s/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "509/509 - 1s - loss: 0.5770 - precision_1: 0.7183 - recall_1: 0.7765 - val_loss: 0.5660 - val_precision_1: 0.7459 - val_recall_1: 0.7250 - 1s/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "509/509 - 2s - loss: 0.5763 - precision_1: 0.7179 - recall_1: 0.7686 - val_loss: 0.5591 - val_precision_1: 0.7241 - val_recall_1: 0.7855 - 2s/epoch - 4ms/step\n",
            "Epoch 87/100\n",
            "509/509 - 2s - loss: 0.5751 - precision_1: 0.7169 - recall_1: 0.7753 - val_loss: 0.5667 - val_precision_1: 0.7457 - val_recall_1: 0.7265 - 2s/epoch - 4ms/step\n",
            "Epoch 88/100\n",
            "509/509 - 1s - loss: 0.5773 - precision_1: 0.7183 - recall_1: 0.7705 - val_loss: 0.5651 - val_precision_1: 0.7566 - val_recall_1: 0.7078 - 1s/epoch - 3ms/step\n",
            "Epoch 89/100\n",
            "509/509 - 1s - loss: 0.5765 - precision_1: 0.7190 - recall_1: 0.7697 - val_loss: 0.5596 - val_precision_1: 0.7279 - val_recall_1: 0.7769 - 1s/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "509/509 - 1s - loss: 0.5750 - precision_1: 0.7191 - recall_1: 0.7747 - val_loss: 0.5664 - val_precision_1: 0.7591 - val_recall_1: 0.7028 - 1s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "509/509 - 1s - loss: 0.5745 - precision_1: 0.7193 - recall_1: 0.7749 - val_loss: 0.5654 - val_precision_1: 0.7460 - val_recall_1: 0.7257 - 1s/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "509/509 - 1s - loss: 0.5750 - precision_1: 0.7193 - recall_1: 0.7744 - val_loss: 0.5605 - val_precision_1: 0.7324 - val_recall_1: 0.7615 - 1s/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "509/509 - 1s - loss: 0.5748 - precision_1: 0.7189 - recall_1: 0.7731 - val_loss: 0.5606 - val_precision_1: 0.7297 - val_recall_1: 0.7644 - 1s/epoch - 2ms/step\n",
            "Epoch 94/100\n",
            "509/509 - 1s - loss: 0.5742 - precision_1: 0.7179 - recall_1: 0.7704 - val_loss: 0.5577 - val_precision_1: 0.7217 - val_recall_1: 0.7884 - 1s/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "509/509 - 1s - loss: 0.5745 - precision_1: 0.7175 - recall_1: 0.7742 - val_loss: 0.5607 - val_precision_1: 0.7338 - val_recall_1: 0.7590 - 1s/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "509/509 - 1s - loss: 0.5726 - precision_1: 0.7193 - recall_1: 0.7751 - val_loss: 0.5573 - val_precision_1: 0.7295 - val_recall_1: 0.7676 - 1s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "509/509 - 2s - loss: 0.5723 - precision_1: 0.7197 - recall_1: 0.7748 - val_loss: 0.5583 - val_precision_1: 0.7302 - val_recall_1: 0.7644 - 2s/epoch - 3ms/step\n",
            "Epoch 98/100\n",
            "509/509 - 2s - loss: 0.5717 - precision_1: 0.7187 - recall_1: 0.7775 - val_loss: 0.5545 - val_precision_1: 0.7251 - val_recall_1: 0.7802 - 2s/epoch - 4ms/step\n",
            "Epoch 99/100\n",
            "509/509 - 2s - loss: 0.5714 - precision_1: 0.7205 - recall_1: 0.7760 - val_loss: 0.5618 - val_precision_1: 0.7534 - val_recall_1: 0.7143 - 2s/epoch - 3ms/step\n",
            "Epoch 100/100\n",
            "509/509 - 1s - loss: 0.5721 - precision_1: 0.7173 - recall_1: 0.7742 - val_loss: 0.5542 - val_precision_1: 0.7227 - val_recall_1: 0.7884 - 1s/epoch - 2ms/step\n",
            "442/442 [==============================] - 1s 2ms/step - loss: 0.5552 - precision_1: 0.7303 - recall_1: 0.7753\n",
            "binary cross-entropy loss :  0.555183470249176  precision:  0.7303118109703064  recal:  0.7752840518951416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection result: DNN\n",
        "DNN also proved to be consistant after feature selection. It even had a mild improvement(which is again, negligible)."
      ],
      "metadata": {
        "id": "tLuJEkwX8gUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection overall result\n",
        "So to conclude, we were able to predict our target feature with an acceptable accuracy even after selecting a subsest of our features. The following is the list of remaining features which proved to be decisive: DiffWalk, Age, HighChol, BMI, HighBP, GenHlth\n"
      ],
      "metadata": {
        "id": "HDGtLOsC88QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rename functions to snake_case"
      ],
      "metadata": {
        "id": "4DSyQcxh8f66"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_name(count):\n",
        "  return [\"f{c}\".format(c=i) for i in range(count)]"
      ],
      "metadata": {
        "id": "nO_T_h6MCctd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "component_count = 15\n",
        "\n",
        "all_features = list(raw_dataset.keys())\n",
        "\n",
        "all_features.remove(\"Diabetes_binary\")\n",
        "\n",
        "pca = PCA(n_components = component_count)\n",
        "\n",
        "pca_columns = pca.fit_transform(standardized_dataset[all_features])"
      ],
      "metadata": {
        "id": "I81sh3xB-I2m"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_dataset = pd.DataFrame()\n",
        "pca_dataset[\"Diabetes_binary\"] = standardized_dataset[\"Diabetes_binary\"]\n",
        "pca_dataset[get_feature_name(component_count)] =  pca_columns"
      ],
      "metadata": {
        "id": "7CTxHp3tAP8D"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(pca_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKYBh6qNB3FH",
        "outputId": "464d9b56-0082-440a-f83c-a65423701d94"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 34 epochs took 1 seconds\n",
            "[[ 0.32456242 -0.22470491  0.03210159 -0.06513454  0.10273124  0.20779652\n",
            "   0.02765655  0.01666475 -0.03813325  0.0327819   0.05444129 -0.01668668\n",
            "   0.01500069  0.02064489  0.00038825]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(pca_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2dG5eNRDbkg",
        "outputId": "6351638b-f8c7-4095-b040-85651829d18d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3612           0.0251            1.22m\n",
            "         2           1.3384           0.0226            1.29m\n",
            "         3           1.3179           0.0210            1.28m\n",
            "         4           1.2981           0.0182            1.28m\n",
            "         5           1.2813           0.0174            1.18m\n",
            "         6           1.2650           0.0155            1.12m\n",
            "         7           1.2502           0.0142            1.07m\n",
            "         8           1.2371           0.0133            1.03m\n",
            "         9           1.2247           0.0123            1.00m\n",
            "        10           1.2139           0.0115           58.44s\n",
            "        11           1.2016           0.0104           57.19s\n",
            "        12           1.1933           0.0100           56.75s\n",
            "        13           1.1833           0.0092           55.64s\n",
            "        14           1.1737           0.0083           54.65s\n",
            "        15           1.1655           0.0075           53.76s\n",
            "        16           1.1581           0.0072           52.91s\n",
            "        17           1.1513           0.0070           52.10s\n",
            "        18           1.1460           0.0060           51.42s\n",
            "        19           1.1388           0.0058           50.67s\n",
            "        20           1.1326           0.0053           50.82s\n",
            "        21           1.1279           0.0052           51.08s\n",
            "        22           1.1234           0.0047           51.27s\n",
            "        23           1.1213           0.0044           51.40s\n",
            "        24           1.1150           0.0042           51.53s\n",
            "        25           1.1104           0.0039           51.57s\n",
            "        26           1.1068           0.0037           51.57s\n",
            "        27           1.1020           0.0033           51.51s\n",
            "        28           1.0976           0.0030           51.53s\n",
            "        29           1.0942           0.0030           51.00s\n",
            "        30           1.0936           0.0028           50.31s\n",
            "        31           1.0902           0.0025           49.63s\n",
            "        32           1.0884           0.0025           48.96s\n",
            "        33           1.0830           0.0021           48.27s\n",
            "        34           1.0814           0.0022           47.61s\n",
            "        35           1.0830           0.0022           46.98s\n",
            "        36           1.0771           0.0018           46.52s\n",
            "        37           1.0752           0.0018           45.88s\n",
            "        38           1.0756           0.0018           45.28s\n",
            "        39           1.0726           0.0014           44.71s\n",
            "        40           1.0707           0.0016           44.18s\n",
            "        41           1.0680           0.0014           43.64s\n",
            "        42           1.0682           0.0013           43.12s\n",
            "        43           1.0645           0.0012           42.56s\n",
            "        44           1.0648           0.0011           42.18s\n",
            "        45           1.0654           0.0012           41.65s\n",
            "        46           1.0610           0.0009           41.11s\n",
            "        47           1.0615           0.0010           40.60s\n",
            "        48           1.0580           0.0009           40.11s\n",
            "        49           1.0588           0.0010           39.60s\n",
            "        50           1.0595           0.0008           39.11s\n",
            "        51           1.0557           0.0009           38.62s\n",
            "        52           1.0564           0.0008           38.22s\n",
            "        53           1.0521           0.0007           37.75s\n",
            "        54           1.0538           0.0007           37.30s\n",
            "        55           1.0500           0.0007           36.85s\n",
            "        56           1.0523           0.0007           36.39s\n",
            "        57           1.0504           0.0005           36.02s\n",
            "        58           1.0502           0.0005           35.85s\n",
            "        59           1.0464           0.0006           35.62s\n",
            "        60           1.0491           0.0006           35.43s\n",
            "        61           1.0487           0.0006           35.21s\n",
            "        62           1.0456           0.0006           34.97s\n",
            "        63           1.0473           0.0005           34.72s\n",
            "        64           1.0474           0.0005           34.45s\n",
            "        65           1.0451           0.0005           34.18s\n",
            "        66           1.0468           0.0004           33.79s\n",
            "        67           1.0450           0.0005           33.34s\n",
            "        68           1.0414           0.0005           32.96s\n",
            "        69           1.0432           0.0004           32.50s\n",
            "        70           1.0400           0.0003           32.04s\n",
            "        71           1.0419           0.0004           31.57s\n",
            "        72           1.0399           0.0003           31.11s\n",
            "        73           1.0403           0.0004           30.65s\n",
            "        74           1.0388           0.0003           30.20s\n",
            "        75           1.0396           0.0004           29.75s\n",
            "        76           1.0371           0.0004           29.35s\n",
            "        77           1.0412           0.0003           28.92s\n",
            "        78           1.0353           0.0003           28.48s\n",
            "        79           1.0358           0.0002           28.08s\n",
            "        80           1.0359           0.0002           27.64s\n",
            "        81           1.0352           0.0004           27.22s\n",
            "        82           1.0355           0.0003           26.79s\n",
            "        83           1.0368           0.0003           26.36s\n",
            "        84           1.0370           0.0003           25.98s\n",
            "        85           1.0342           0.0002           25.54s\n",
            "        86           1.0341           0.0002           25.11s\n",
            "        87           1.0366           0.0002           24.69s\n",
            "        88           1.0341           0.0002           24.26s\n",
            "        89           1.0349           0.0001           23.84s\n",
            "        90           1.0274           0.0001           23.42s\n",
            "        91           1.0358           0.0003           23.01s\n",
            "        92           1.0322           0.0002           22.64s\n",
            "        93           1.0317           0.0001           22.23s\n",
            "        94           1.0313           0.0001           21.86s\n",
            "        95           1.0299           0.0002           21.55s\n",
            "        96           1.0282           0.0002           21.23s\n",
            "        97           1.0297           0.0001           20.90s\n",
            "        98           1.0302           0.0001           20.56s\n",
            "        99           1.0314           0.0002           20.22s\n",
            "       100           1.0265           0.0002           19.87s\n",
            "       101           1.0259           0.0001           19.54s\n",
            "       102           1.0284           0.0001           19.19s\n",
            "       103           1.0289           0.0001           18.77s\n",
            "       104           1.0310           0.0002           18.34s\n",
            "       105           1.0259           0.0001           17.92s\n",
            "       106           1.0233           0.0001           17.49s\n",
            "       107           1.0263           0.0001           17.07s\n",
            "       108           1.0314           0.0001           16.65s\n",
            "       109           1.0274           0.0000           16.24s\n",
            "       110           1.0264           0.0001           15.82s\n",
            "       111           1.0290           0.0001           15.41s\n",
            "       112           1.0286           0.0001           15.00s\n",
            "       113           1.0232           0.0001           14.59s\n",
            "       114           1.0245           0.0001           14.18s\n",
            "       115           1.0278           0.0001           13.77s\n",
            "       116           1.0265           0.0001           13.37s\n",
            "       117           1.0232           0.0000           12.96s\n",
            "       118           1.0245           0.0001           12.56s\n",
            "       119           1.0250           0.0000           12.15s\n",
            "       120           1.0236           0.0001           11.75s\n",
            "       121           1.0244           0.0001           11.34s\n",
            "       122           1.0198           0.0001           10.94s\n",
            "       123           1.0225           0.0001           10.54s\n",
            "       124           1.0255           0.0000           10.14s\n",
            "       125           1.0188           0.0001            9.74s\n",
            "       126           1.0215           0.0000            9.34s\n",
            "       127           1.0219           0.0000            8.94s\n",
            "       128           1.0238          -0.0000            8.55s\n",
            "       129           1.0228           0.0001            8.15s\n",
            "       130           1.0236           0.0001            7.76s\n",
            "       131           1.0221           0.0001            7.37s\n",
            "       132           1.0206          -0.0000            7.00s\n",
            "       133           1.0194           0.0000            6.63s\n",
            "       134           1.0211           0.0001            6.25s\n",
            "       135           1.0206           0.0000            5.87s\n",
            "       136           1.0189           0.0001            5.49s\n",
            "       137           1.0178          -0.0000            5.11s\n",
            "       138           1.0204           0.0000            4.73s\n",
            "       139           1.0197           0.0000            4.35s\n",
            "       140           1.0216           0.0001            3.96s\n",
            "       141           1.0169          -0.0001            3.56s\n",
            "       142           1.0202           0.0000            3.16s\n",
            "       143           1.0196           0.0001            2.76s\n",
            "       144           1.0180           0.0000            2.37s\n",
            "       145           1.0195           0.0000            1.97s\n",
            "       146           1.0178           0.0000            1.58s\n",
            "       147           1.0183           0.0000            1.18s\n",
            "       148           1.0190          -0.0000            0.79s\n",
            "       149           1.0188           0.0000            0.39s\n",
            "       150           1.0181          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.70      0.73      7010\n",
            "         1.0       0.73      0.79      0.76      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.75      0.74      0.74     14139\n",
            "weighted avg       0.75      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model(pca_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKDs0BosD2bD",
        "outputId": "b59ccbcd-187a-4dbd-b205-21c72865a58e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "509/509 - 4s - loss: 2.3829 - precision_2: 0.6952 - recall_2: 0.6226 - val_loss: 1.4191 - val_precision_2: 0.7522 - val_recall_2: 0.7150 - 4s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "509/509 - 1s - loss: 1.3206 - precision_2: 0.7415 - recall_2: 0.6770 - val_loss: 1.2090 - val_precision_2: 0.7260 - val_recall_2: 0.7701 - 1s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "509/509 - 1s - loss: 1.1298 - precision_2: 0.7393 - recall_2: 0.6891 - val_loss: 1.0408 - val_precision_2: 0.7337 - val_recall_2: 0.7594 - 1s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "509/509 - 2s - loss: 1.0268 - precision_2: 0.7412 - recall_2: 0.7000 - val_loss: 1.0087 - val_precision_2: 0.7623 - val_recall_2: 0.6763 - 2s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "509/509 - 2s - loss: 0.9632 - precision_2: 0.7365 - recall_2: 0.7061 - val_loss: 0.9069 - val_precision_2: 0.7198 - val_recall_2: 0.7927 - 2s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "509/509 - 1s - loss: 0.9081 - precision_2: 0.7326 - recall_2: 0.7198 - val_loss: 0.8936 - val_precision_2: 0.7149 - val_recall_2: 0.8027 - 1s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "509/509 - 1s - loss: 0.8693 - precision_2: 0.7307 - recall_2: 0.7400 - val_loss: 0.8189 - val_precision_2: 0.7352 - val_recall_2: 0.7673 - 1s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "509/509 - 1s - loss: 0.8367 - precision_2: 0.7280 - recall_2: 0.7503 - val_loss: 0.8326 - val_precision_2: 0.7002 - val_recall_2: 0.8378 - 1s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "509/509 - 1s - loss: 0.8124 - precision_2: 0.7243 - recall_2: 0.7682 - val_loss: 0.7852 - val_precision_2: 0.7161 - val_recall_2: 0.8092 - 1s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "509/509 - 1s - loss: 0.7919 - precision_2: 0.7220 - recall_2: 0.7780 - val_loss: 0.7783 - val_precision_2: 0.7270 - val_recall_2: 0.7855 - 1s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "509/509 - 1s - loss: 0.7733 - precision_2: 0.7196 - recall_2: 0.7841 - val_loss: 0.7583 - val_precision_2: 0.7430 - val_recall_2: 0.7504 - 1s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "509/509 - 1s - loss: 0.7568 - precision_2: 0.7191 - recall_2: 0.7883 - val_loss: 0.7464 - val_precision_2: 0.7018 - val_recall_2: 0.8375 - 1s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "509/509 - 1s - loss: 0.7471 - precision_2: 0.7176 - recall_2: 0.7937 - val_loss: 0.7351 - val_precision_2: 0.7070 - val_recall_2: 0.8285 - 1s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "509/509 - 1s - loss: 0.7399 - precision_2: 0.7186 - recall_2: 0.7979 - val_loss: 0.7276 - val_precision_2: 0.7112 - val_recall_2: 0.8210 - 1s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "509/509 - 2s - loss: 0.7370 - precision_2: 0.7162 - recall_2: 0.7997 - val_loss: 0.7311 - val_precision_2: 0.7076 - val_recall_2: 0.8274 - 2s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "509/509 - 2s - loss: 0.7299 - precision_2: 0.7153 - recall_2: 0.8021 - val_loss: 0.7107 - val_precision_2: 0.7126 - val_recall_2: 0.8185 - 2s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "509/509 - 2s - loss: 0.7195 - precision_2: 0.7145 - recall_2: 0.8008 - val_loss: 0.7041 - val_precision_2: 0.7058 - val_recall_2: 0.8314 - 2s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "509/509 - 1s - loss: 0.7100 - precision_2: 0.7170 - recall_2: 0.8038 - val_loss: 0.7006 - val_precision_2: 0.7195 - val_recall_2: 0.8074 - 1s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "509/509 - 1s - loss: 0.7015 - precision_2: 0.7157 - recall_2: 0.8046 - val_loss: 0.6875 - val_precision_2: 0.7087 - val_recall_2: 0.8292 - 1s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "509/509 - 1s - loss: 0.6972 - precision_2: 0.7149 - recall_2: 0.8051 - val_loss: 0.6849 - val_precision_2: 0.7073 - val_recall_2: 0.8306 - 1s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "509/509 - 1s - loss: 0.6920 - precision_2: 0.7148 - recall_2: 0.8057 - val_loss: 0.6810 - val_precision_2: 0.7059 - val_recall_2: 0.8346 - 1s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "509/509 - 1s - loss: 0.6866 - precision_2: 0.7150 - recall_2: 0.8071 - val_loss: 0.6730 - val_precision_2: 0.7097 - val_recall_2: 0.8264 - 1s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "509/509 - 1s - loss: 0.6814 - precision_2: 0.7151 - recall_2: 0.8065 - val_loss: 0.6678 - val_precision_2: 0.7071 - val_recall_2: 0.8314 - 1s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "509/509 - 1s - loss: 0.6779 - precision_2: 0.7133 - recall_2: 0.8048 - val_loss: 0.6704 - val_precision_2: 0.7264 - val_recall_2: 0.7984 - 1s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "509/509 - 1s - loss: 0.6751 - precision_2: 0.7153 - recall_2: 0.8087 - val_loss: 0.6681 - val_precision_2: 0.7183 - val_recall_2: 0.8124 - 1s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "509/509 - 1s - loss: 0.6726 - precision_2: 0.7157 - recall_2: 0.8079 - val_loss: 0.6604 - val_precision_2: 0.7179 - val_recall_2: 0.8120 - 1s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "509/509 - 2s - loss: 0.6669 - precision_2: 0.7137 - recall_2: 0.8089 - val_loss: 0.6543 - val_precision_2: 0.7130 - val_recall_2: 0.8203 - 2s/epoch - 3ms/step\n",
            "Epoch 28/100\n",
            "509/509 - 2s - loss: 0.6626 - precision_2: 0.7137 - recall_2: 0.8052 - val_loss: 0.6510 - val_precision_2: 0.7066 - val_recall_2: 0.8321 - 2s/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "509/509 - 1s - loss: 0.6561 - precision_2: 0.7156 - recall_2: 0.8081 - val_loss: 0.6434 - val_precision_2: 0.7135 - val_recall_2: 0.8203 - 1s/epoch - 3ms/step\n",
            "Epoch 30/100\n",
            "509/509 - 1s - loss: 0.6515 - precision_2: 0.7163 - recall_2: 0.8063 - val_loss: 0.6387 - val_precision_2: 0.7122 - val_recall_2: 0.8231 - 1s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "509/509 - 1s - loss: 0.6489 - precision_2: 0.7157 - recall_2: 0.8101 - val_loss: 0.6363 - val_precision_2: 0.7093 - val_recall_2: 0.8264 - 1s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "509/509 - 1s - loss: 0.6454 - precision_2: 0.7165 - recall_2: 0.8041 - val_loss: 0.6383 - val_precision_2: 0.7067 - val_recall_2: 0.8367 - 1s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "509/509 - 1s - loss: 0.6425 - precision_2: 0.7148 - recall_2: 0.8060 - val_loss: 0.6286 - val_precision_2: 0.7076 - val_recall_2: 0.8317 - 1s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "509/509 - 1s - loss: 0.6387 - precision_2: 0.7156 - recall_2: 0.8094 - val_loss: 0.6339 - val_precision_2: 0.7178 - val_recall_2: 0.8131 - 1s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "509/509 - 1s - loss: 0.6361 - precision_2: 0.7156 - recall_2: 0.8095 - val_loss: 0.6170 - val_precision_2: 0.7160 - val_recall_2: 0.8188 - 1s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "509/509 - 1s - loss: 0.6338 - precision_2: 0.7148 - recall_2: 0.8070 - val_loss: 0.6266 - val_precision_2: 0.7122 - val_recall_2: 0.8213 - 1s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "509/509 - 1s - loss: 0.6300 - precision_2: 0.7151 - recall_2: 0.8068 - val_loss: 0.6105 - val_precision_2: 0.7105 - val_recall_2: 0.8260 - 1s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "509/509 - 2s - loss: 0.6285 - precision_2: 0.7152 - recall_2: 0.8091 - val_loss: 0.6258 - val_precision_2: 0.7160 - val_recall_2: 0.8178 - 2s/epoch - 4ms/step\n",
            "Epoch 39/100\n",
            "509/509 - 2s - loss: 0.6274 - precision_2: 0.7164 - recall_2: 0.8065 - val_loss: 0.6078 - val_precision_2: 0.7168 - val_recall_2: 0.8163 - 2s/epoch - 4ms/step\n",
            "Epoch 40/100\n",
            "509/509 - 1s - loss: 0.6257 - precision_2: 0.7162 - recall_2: 0.8071 - val_loss: 0.6227 - val_precision_2: 0.7122 - val_recall_2: 0.8224 - 1s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "509/509 - 1s - loss: 0.6223 - precision_2: 0.7163 - recall_2: 0.8073 - val_loss: 0.6047 - val_precision_2: 0.7112 - val_recall_2: 0.8242 - 1s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "509/509 - 1s - loss: 0.6196 - precision_2: 0.7161 - recall_2: 0.8065 - val_loss: 0.6151 - val_precision_2: 0.7077 - val_recall_2: 0.8314 - 1s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "509/509 - 1s - loss: 0.6156 - precision_2: 0.7168 - recall_2: 0.8089 - val_loss: 0.5969 - val_precision_2: 0.7133 - val_recall_2: 0.8221 - 1s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "509/509 - 1s - loss: 0.6137 - precision_2: 0.7169 - recall_2: 0.8085 - val_loss: 0.6139 - val_precision_2: 0.7144 - val_recall_2: 0.8203 - 1s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "509/509 - 1s - loss: 0.6145 - precision_2: 0.7168 - recall_2: 0.8066 - val_loss: 0.5931 - val_precision_2: 0.7138 - val_recall_2: 0.8181 - 1s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "509/509 - 1s - loss: 0.6119 - precision_2: 0.7175 - recall_2: 0.8068 - val_loss: 0.6103 - val_precision_2: 0.7149 - val_recall_2: 0.8170 - 1s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "509/509 - 1s - loss: 0.6090 - precision_2: 0.7174 - recall_2: 0.8058 - val_loss: 0.5905 - val_precision_2: 0.7208 - val_recall_2: 0.8042 - 1s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "509/509 - 1s - loss: 0.6069 - precision_2: 0.7159 - recall_2: 0.8079 - val_loss: 0.6056 - val_precision_2: 0.7150 - val_recall_2: 0.8174 - 1s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "509/509 - 2s - loss: 0.6039 - precision_2: 0.7158 - recall_2: 0.8085 - val_loss: 0.5848 - val_precision_2: 0.7193 - val_recall_2: 0.8092 - 2s/epoch - 4ms/step\n",
            "Epoch 50/100\n",
            "509/509 - 2s - loss: 0.6024 - precision_2: 0.7185 - recall_2: 0.8056 - val_loss: 0.6058 - val_precision_2: 0.7078 - val_recall_2: 0.8324 - 2s/epoch - 4ms/step\n",
            "Epoch 51/100\n",
            "509/509 - 1s - loss: 0.6022 - precision_2: 0.7179 - recall_2: 0.8075 - val_loss: 0.5866 - val_precision_2: 0.7113 - val_recall_2: 0.8267 - 1s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "509/509 - 1s - loss: 0.6032 - precision_2: 0.7178 - recall_2: 0.8061 - val_loss: 0.5987 - val_precision_2: 0.7178 - val_recall_2: 0.8142 - 1s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "509/509 - 1s - loss: 0.6004 - precision_2: 0.7185 - recall_2: 0.8060 - val_loss: 0.5831 - val_precision_2: 0.7146 - val_recall_2: 0.8203 - 1s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "509/509 - 1s - loss: 0.5998 - precision_2: 0.7167 - recall_2: 0.8049 - val_loss: 0.6018 - val_precision_2: 0.7154 - val_recall_2: 0.8188 - 1s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "509/509 - 1s - loss: 0.6008 - precision_2: 0.7178 - recall_2: 0.8065 - val_loss: 0.5873 - val_precision_2: 0.7088 - val_recall_2: 0.8321 - 1s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "509/509 - 1s - loss: 0.5994 - precision_2: 0.7181 - recall_2: 0.8061 - val_loss: 0.5965 - val_precision_2: 0.7225 - val_recall_2: 0.8045 - 1s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "509/509 - 1s - loss: 0.5989 - precision_2: 0.7184 - recall_2: 0.8057 - val_loss: 0.5819 - val_precision_2: 0.7138 - val_recall_2: 0.8242 - 1s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "509/509 - 1s - loss: 0.5968 - precision_2: 0.7190 - recall_2: 0.8065 - val_loss: 0.5949 - val_precision_2: 0.7111 - val_recall_2: 0.8267 - 1s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "509/509 - 1s - loss: 0.5959 - precision_2: 0.7190 - recall_2: 0.8035 - val_loss: 0.5782 - val_precision_2: 0.7110 - val_recall_2: 0.8264 - 1s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "509/509 - 1s - loss: 0.5956 - precision_2: 0.7193 - recall_2: 0.8043 - val_loss: 0.5939 - val_precision_2: 0.7136 - val_recall_2: 0.8260 - 1s/epoch - 3ms/step\n",
            "Epoch 61/100\n",
            "509/509 - 2s - loss: 0.5935 - precision_2: 0.7171 - recall_2: 0.8072 - val_loss: 0.5744 - val_precision_2: 0.7208 - val_recall_2: 0.8034 - 2s/epoch - 4ms/step\n",
            "Epoch 62/100\n",
            "509/509 - 2s - loss: 0.5916 - precision_2: 0.7175 - recall_2: 0.8060 - val_loss: 0.5886 - val_precision_2: 0.7149 - val_recall_2: 0.8178 - 2s/epoch - 3ms/step\n",
            "Epoch 63/100\n",
            "509/509 - 1s - loss: 0.5899 - precision_2: 0.7181 - recall_2: 0.8039 - val_loss: 0.5714 - val_precision_2: 0.7154 - val_recall_2: 0.8174 - 1s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "509/509 - 1s - loss: 0.5887 - precision_2: 0.7188 - recall_2: 0.8079 - val_loss: 0.5870 - val_precision_2: 0.7126 - val_recall_2: 0.8238 - 1s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "509/509 - 1s - loss: 0.5872 - precision_2: 0.7184 - recall_2: 0.8066 - val_loss: 0.5730 - val_precision_2: 0.7093 - val_recall_2: 0.8292 - 1s/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "509/509 - 1s - loss: 0.5859 - precision_2: 0.7196 - recall_2: 0.8064 - val_loss: 0.5851 - val_precision_2: 0.7099 - val_recall_2: 0.8264 - 1s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "509/509 - 1s - loss: 0.5866 - precision_2: 0.7189 - recall_2: 0.8054 - val_loss: 0.5696 - val_precision_2: 0.7181 - val_recall_2: 0.8117 - 1s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "509/509 - 1s - loss: 0.5840 - precision_2: 0.7186 - recall_2: 0.8057 - val_loss: 0.5800 - val_precision_2: 0.7108 - val_recall_2: 0.8264 - 1s/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "509/509 - 1s - loss: 0.5851 - precision_2: 0.7202 - recall_2: 0.8062 - val_loss: 0.5693 - val_precision_2: 0.7106 - val_recall_2: 0.8292 - 1s/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "509/509 - 1s - loss: 0.5857 - precision_2: 0.7184 - recall_2: 0.8082 - val_loss: 0.5840 - val_precision_2: 0.7131 - val_recall_2: 0.8206 - 1s/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "509/509 - 2s - loss: 0.5847 - precision_2: 0.7190 - recall_2: 0.8063 - val_loss: 0.5683 - val_precision_2: 0.7107 - val_recall_2: 0.8278 - 2s/epoch - 3ms/step\n",
            "Epoch 72/100\n",
            "509/509 - 2s - loss: 0.5833 - precision_2: 0.7191 - recall_2: 0.8062 - val_loss: 0.5807 - val_precision_2: 0.7142 - val_recall_2: 0.8213 - 2s/epoch - 4ms/step\n",
            "Epoch 73/100\n",
            "509/509 - 2s - loss: 0.5823 - precision_2: 0.7191 - recall_2: 0.8063 - val_loss: 0.5664 - val_precision_2: 0.7151 - val_recall_2: 0.8153 - 2s/epoch - 3ms/step\n",
            "Epoch 74/100\n",
            "509/509 - 1s - loss: 0.5806 - precision_2: 0.7177 - recall_2: 0.8074 - val_loss: 0.5772 - val_precision_2: 0.7140 - val_recall_2: 0.8231 - 1s/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "509/509 - 1s - loss: 0.5803 - precision_2: 0.7180 - recall_2: 0.8073 - val_loss: 0.5636 - val_precision_2: 0.7135 - val_recall_2: 0.8221 - 1s/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "509/509 - 1s - loss: 0.5797 - precision_2: 0.7191 - recall_2: 0.8059 - val_loss: 0.5784 - val_precision_2: 0.7107 - val_recall_2: 0.8303 - 1s/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "509/509 - 1s - loss: 0.5793 - precision_2: 0.7186 - recall_2: 0.8070 - val_loss: 0.5607 - val_precision_2: 0.7147 - val_recall_2: 0.8235 - 1s/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "509/509 - 1s - loss: 0.5770 - precision_2: 0.7193 - recall_2: 0.8074 - val_loss: 0.5732 - val_precision_2: 0.7182 - val_recall_2: 0.8113 - 1s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "509/509 - 1s - loss: 0.5754 - precision_2: 0.7187 - recall_2: 0.8096 - val_loss: 0.5589 - val_precision_2: 0.7118 - val_recall_2: 0.8249 - 1s/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "509/509 - 1s - loss: 0.5752 - precision_2: 0.7192 - recall_2: 0.8076 - val_loss: 0.5761 - val_precision_2: 0.7104 - val_recall_2: 0.8342 - 1s/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "509/509 - 1s - loss: 0.5762 - precision_2: 0.7188 - recall_2: 0.8056 - val_loss: 0.5576 - val_precision_2: 0.7223 - val_recall_2: 0.8027 - 1s/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "509/509 - 2s - loss: 0.5756 - precision_2: 0.7188 - recall_2: 0.8083 - val_loss: 0.5749 - val_precision_2: 0.7154 - val_recall_2: 0.8192 - 2s/epoch - 3ms/step\n",
            "Epoch 83/100\n",
            "509/509 - 2s - loss: 0.5775 - precision_2: 0.7196 - recall_2: 0.8070 - val_loss: 0.5615 - val_precision_2: 0.7159 - val_recall_2: 0.8174 - 2s/epoch - 4ms/step\n",
            "Epoch 84/100\n",
            "509/509 - 1s - loss: 0.5785 - precision_2: 0.7195 - recall_2: 0.8080 - val_loss: 0.5761 - val_precision_2: 0.7182 - val_recall_2: 0.8160 - 1s/epoch - 3ms/step\n",
            "Epoch 85/100\n",
            "509/509 - 1s - loss: 0.5768 - precision_2: 0.7190 - recall_2: 0.8069 - val_loss: 0.5615 - val_precision_2: 0.7239 - val_recall_2: 0.8006 - 1s/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "509/509 - 1s - loss: 0.5756 - precision_2: 0.7184 - recall_2: 0.8076 - val_loss: 0.5740 - val_precision_2: 0.7111 - val_recall_2: 0.8285 - 1s/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "509/509 - 1s - loss: 0.5756 - precision_2: 0.7205 - recall_2: 0.8070 - val_loss: 0.5582 - val_precision_2: 0.7159 - val_recall_2: 0.8192 - 1s/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "509/509 - 1s - loss: 0.5742 - precision_2: 0.7198 - recall_2: 0.8077 - val_loss: 0.5710 - val_precision_2: 0.7124 - val_recall_2: 0.8238 - 1s/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "509/509 - 1s - loss: 0.5729 - precision_2: 0.7211 - recall_2: 0.8053 - val_loss: 0.5573 - val_precision_2: 0.7235 - val_recall_2: 0.8002 - 1s/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "509/509 - 1s - loss: 0.5728 - precision_2: 0.7197 - recall_2: 0.8062 - val_loss: 0.5692 - val_precision_2: 0.7143 - val_recall_2: 0.8210 - 1s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "509/509 - 1s - loss: 0.5727 - precision_2: 0.7192 - recall_2: 0.8082 - val_loss: 0.5593 - val_precision_2: 0.7095 - val_recall_2: 0.8317 - 1s/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "509/509 - 1s - loss: 0.5717 - precision_2: 0.7195 - recall_2: 0.8077 - val_loss: 0.5672 - val_precision_2: 0.7153 - val_recall_2: 0.8185 - 1s/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "509/509 - 2s - loss: 0.5711 - precision_2: 0.7194 - recall_2: 0.8084 - val_loss: 0.5559 - val_precision_2: 0.7138 - val_recall_2: 0.8224 - 2s/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "509/509 - 2s - loss: 0.5712 - precision_2: 0.7206 - recall_2: 0.8062 - val_loss: 0.5710 - val_precision_2: 0.7088 - val_recall_2: 0.8375 - 2s/epoch - 4ms/step\n",
            "Epoch 95/100\n",
            "509/509 - 2s - loss: 0.5704 - precision_2: 0.7206 - recall_2: 0.8084 - val_loss: 0.5549 - val_precision_2: 0.7146 - val_recall_2: 0.8203 - 2s/epoch - 3ms/step\n",
            "Epoch 96/100\n",
            "509/509 - 1s - loss: 0.5703 - precision_2: 0.7194 - recall_2: 0.8065 - val_loss: 0.5667 - val_precision_2: 0.7147 - val_recall_2: 0.8217 - 1s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "509/509 - 1s - loss: 0.5690 - precision_2: 0.7188 - recall_2: 0.8079 - val_loss: 0.5559 - val_precision_2: 0.7096 - val_recall_2: 0.8321 - 1s/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "509/509 - 1s - loss: 0.5683 - precision_2: 0.7197 - recall_2: 0.8092 - val_loss: 0.5661 - val_precision_2: 0.7125 - val_recall_2: 0.8278 - 1s/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "509/509 - 1s - loss: 0.5686 - precision_2: 0.7188 - recall_2: 0.8103 - val_loss: 0.5532 - val_precision_2: 0.7159 - val_recall_2: 0.8149 - 1s/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "509/509 - 1s - loss: 0.5688 - precision_2: 0.7196 - recall_2: 0.8077 - val_loss: 0.5652 - val_precision_2: 0.7154 - val_recall_2: 0.8206 - 1s/epoch - 2ms/step\n",
            "442/442 [==============================] - 1s 2ms/step - loss: 0.5654 - precision_2: 0.7232 - recall_2: 0.8019\n",
            "binary cross-entropy loss :  0.5653501749038696  precision:  0.7232131361961365  recal:  0.8019357323646545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature reduction: PCA\n",
        "Principle component analysis is a technique which is used to decrease feature count such that their information is preserved, only in lower dimensions. It mainly aims to keep the variance of the original data in fewer columns. Result however, did not show any significant improvement, suggesting that important relations between features and target are already captured.  \n"
      ],
      "metadata": {
        "id": "P2SPP1tvHTyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Final result\n",
        "for the purpose of our research, we will use the following weights obtained from the Logistic regression model to apply on users' input.\n",
        "\n",
        "### feature selected model:\n",
        "$$[0.0702186,  0.08500237, 0.29923042, 0.03782045, 0.39012718, 0.30067847]$$\n",
        "\n",
        "### original model:\n",
        "$$[ 0.73642276,  0.5859419,   1.35431797,  0.07424616, -0.00842411,  0.17406674\n",
        "   0.23032686, -0.02977515, -0.03619828, -0.08796805, -0.72938801,  0.05957254,\n",
        "   0.01305635,  0.58596997, -0.00528506, -0.00804519,  0.13615712,  0.26243553,\n",
        "   0.15226778, -0.02630124, -0.0588881 ]$$"
      ],
      "metadata": {
        "id": "G_ytRJFYKK2t"
      }
    }
  ]
}