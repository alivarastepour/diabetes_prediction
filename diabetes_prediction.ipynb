{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoqV2DRj34Fq1FsrF7Kk82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alivarastepour/diabetes_prediction/blob/master/diabetes_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purpose of this notebook\n",
        "This notebook aims to build a model that determines whether a person is prone to diabetes or not. Additionally, it seeks to identify a subset of features (risk factors) that can accurately predict the risk of diabetes. The weights of the optimal solution will be utilized in another project, where they will be applied to users' inputs in real time.\n",
        "\n",
        "## Dataset\n",
        "This notebook makes use of a subset of a larger dataset which aimed to collect uniform, state-specific data on preventive health practices and risk behaviors that are associated with chronic diseases, injuries, and preventable infectious diseases in the adult population. The subset used in this notebook can be accessed [here](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_binary_5050split_health_indicators_BRFSS2015.csv)."
      ],
      "metadata": {
        "id": "q2IpptVI_N1M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EtkfdEZs9h7u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LeakyReLU,Dropout\n",
        "from keras.optimizers import Adagrad, RMSprop, Adam\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.initializers import he_normal\n",
        "from keras.activations import selu\n",
        "from keras.metrics import Precision, Recall\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/drive')\n",
        "DATASET_ADDRESS = '/drive/MyDrive/diabetes_info.csv'\n",
        "raw_dataset = pd.read_csv(DATASET_ADDRESS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF0Coc72--j8",
        "outputId": "520a052d-4e35-4b71-b54b-d8255c553ba1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5AdQ7V9_LDr",
        "outputId": "362afff5-7b89-47a5-f388-1e7a3a79faa5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 70692 entries, 0 to 70691\n",
            "Data columns (total 22 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   Diabetes_binary       70692 non-null  float64\n",
            " 1   HighBP                70692 non-null  float64\n",
            " 2   HighChol              70692 non-null  float64\n",
            " 3   CholCheck             70692 non-null  float64\n",
            " 4   BMI                   70692 non-null  float64\n",
            " 5   Smoker                70692 non-null  float64\n",
            " 6   Stroke                70692 non-null  float64\n",
            " 7   HeartDiseaseorAttack  70692 non-null  float64\n",
            " 8   PhysActivity          70692 non-null  float64\n",
            " 9   Fruits                70692 non-null  float64\n",
            " 10  Veggies               70692 non-null  float64\n",
            " 11  HvyAlcoholConsump     70692 non-null  float64\n",
            " 12  AnyHealthcare         70692 non-null  float64\n",
            " 13  NoDocbcCost           70692 non-null  float64\n",
            " 14  GenHlth               70692 non-null  float64\n",
            " 15  MentHlth              70692 non-null  float64\n",
            " 16  PhysHlth              70692 non-null  float64\n",
            " 17  DiffWalk              70692 non-null  float64\n",
            " 18  Sex                   70692 non-null  float64\n",
            " 19  Age                   70692 non-null  float64\n",
            " 20  Education             70692 non-null  float64\n",
            " 21  Income                70692 non-null  float64\n",
            "dtypes: float64(22)\n",
            "memory usage: 11.9 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = raw_dataset.drop(columns=['Income']) # Income unit used in this research is USD which is not scalable to Rials. So we ignore it."
      ],
      "metadata": {
        "id": "Bs6tMOknBSo9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = raw_dataset[\"Diabetes_binary\"]\n",
        "x = raw_dataset.drop(columns=[\"Diabetes_binary\"])"
      ],
      "metadata": {
        "id": "Di2eBqgLFkFT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=9)"
      ],
      "metadata": {
        "id": "SSYmWVclFr_V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection\n",
        "While our data may appear relatively clean, this does not guarantee optimal performance. Therefore, we must leverage a range of machine learning models to assess their effectiveness and identify potential modifications to the original data that can enhance the performance of our models."
      ],
      "metadata": {
        "id": "8k-7qJWsJsml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First model: Gradient boost classifier\n",
        "Boosting algorithms have been widely recognized as effective choices for handling tabular data. Among them, gradient boosting stands out as a prominent technique that leverages decision trees to create a powerful ensemble model. Nonetheless, to ensure its optimal performance, careful consideration should be given to hyperparameter tuning."
      ],
      "metadata": {
        "id": "xpB6qQ79KbGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(dataset):\n",
        "  y = dataset[\"Diabetes_binary\"]\n",
        "  x = dataset.drop(columns=[\"Diabetes_binary\"])\n",
        "  return train_test_split(x, y, test_size=0.20, random_state=9)"
      ],
      "metadata": {
        "id": "2bZnlwkq8_PX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_boost_classifier_model(dataset, learning_rate=0.05, n_estimators=150, subsample=0.8):\n",
        "  x_train, x_test, y_train, y_test = get_data(dataset)\n",
        "  reg = GradientBoostingClassifier(random_state=90,\n",
        "                                loss='deviance',\n",
        "                                learning_rate=learning_rate,\n",
        "                                n_estimators=n_estimators,\n",
        "                                subsample=subsample,\n",
        "                                criterion='friedman_mse',\n",
        "                                verbose=2,\n",
        "                                )\n",
        "  reg.fit(x_train, y_train)\n",
        "  y_pred = reg.predict(x_test)\n",
        "  report = classification_report(y_test, y_pred)\n",
        "  print(report)"
      ],
      "metadata": {
        "id": "WGLlf7wgCSDR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(raw_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJHCJ4TfCzce",
        "outputId": "7fbd029e-12ac-4af8-c802-f716086045dc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3619           0.0245            8.04s\n",
            "         2           1.3400           0.0224            7.90s\n",
            "         3           1.3200           0.0203            7.82s\n",
            "         4           1.3008           0.0178            7.93s\n",
            "         5           1.2841           0.0163            7.84s\n",
            "         6           1.2685           0.0151            7.79s\n",
            "         7           1.2545           0.0136            7.71s\n",
            "         8           1.2417           0.0127            7.77s\n",
            "         9           1.2303           0.0117            7.68s\n",
            "        10           1.2191           0.0112            7.65s\n",
            "        11           1.2090           0.0101            7.71s\n",
            "        12           1.1982           0.0100            7.63s\n",
            "        13           1.1890           0.0094            7.55s\n",
            "        14           1.1793           0.0088            7.48s\n",
            "        15           1.1707           0.0078            7.42s\n",
            "        16           1.1641           0.0067            7.37s\n",
            "        17           1.1571           0.0076            7.37s\n",
            "        18           1.1502           0.0062            7.31s\n",
            "        19           1.1433           0.0058            7.25s\n",
            "        20           1.1379           0.0066            7.21s\n",
            "        21           1.1312           0.0059            7.14s\n",
            "        22           1.1278           0.0050            7.07s\n",
            "        23           1.1231           0.0050            7.01s\n",
            "        24           1.1165           0.0051            6.96s\n",
            "        25           1.1117           0.0040            6.89s\n",
            "        26           1.1087           0.0043            6.83s\n",
            "        27           1.1027           0.0045            6.76s\n",
            "        28           1.0968           0.0040            6.72s\n",
            "        29           1.0944           0.0034            6.66s\n",
            "        30           1.0899           0.0036            6.63s\n",
            "        31           1.0874           0.0029            6.56s\n",
            "        32           1.0859           0.0034            6.51s\n",
            "        33           1.0807           0.0030            6.46s\n",
            "        34           1.0780           0.0027            6.39s\n",
            "        35           1.0783           0.0027            6.33s\n",
            "        36           1.0720           0.0022            6.28s\n",
            "        37           1.0695           0.0025            6.22s\n",
            "        38           1.0690           0.0026            6.17s\n",
            "        39           1.0643           0.0021            6.11s\n",
            "        40           1.0638           0.0020            6.05s\n",
            "        41           1.0599           0.0021            5.99s\n",
            "        42           1.0596           0.0020            5.93s\n",
            "        43           1.0558           0.0017            5.87s\n",
            "        44           1.0562           0.0016            5.82s\n",
            "        45           1.0564           0.0018            5.76s\n",
            "        46           1.0509           0.0017            5.69s\n",
            "        47           1.0510           0.0016            5.64s\n",
            "        48           1.0466           0.0015            5.58s\n",
            "        49           1.0481           0.0013            5.54s\n",
            "        50           1.0459           0.0013            5.49s\n",
            "        51           1.0449           0.0012            5.43s\n",
            "        52           1.0439           0.0012            5.38s\n",
            "        53           1.0396           0.0012            5.32s\n",
            "        54           1.0412           0.0013            5.26s\n",
            "        55           1.0364           0.0008            5.20s\n",
            "        56           1.0387           0.0009            5.15s\n",
            "        57           1.0372           0.0009            5.09s\n",
            "        58           1.0372           0.0011            5.03s\n",
            "        59           1.0320           0.0008            4.97s\n",
            "        60           1.0339           0.0009            4.92s\n",
            "        61           1.0332           0.0008            4.86s\n",
            "        62           1.0304           0.0010            4.81s\n",
            "        63           1.0309           0.0009            4.75s\n",
            "        64           1.0313           0.0009            4.70s\n",
            "        65           1.0284           0.0007            4.64s\n",
            "        66           1.0303           0.0007            4.59s\n",
            "        67           1.0293           0.0007            4.54s\n",
            "        68           1.0260           0.0006            4.49s\n",
            "        69           1.0276           0.0006            4.44s\n",
            "        70           1.0231           0.0006            4.38s\n",
            "        71           1.0259           0.0006            4.32s\n",
            "        72           1.0230           0.0005            4.27s\n",
            "        73           1.0236           0.0005            4.21s\n",
            "        74           1.0214           0.0004            4.15s\n",
            "        75           1.0240           0.0006            4.10s\n",
            "        76           1.0205           0.0004            4.04s\n",
            "        77           1.0237           0.0004            3.99s\n",
            "        78           1.0187           0.0004            3.93s\n",
            "        79           1.0194           0.0003            3.87s\n",
            "        80           1.0192           0.0004            3.82s\n",
            "        81           1.0185           0.0005            3.76s\n",
            "        82           1.0164           0.0004            3.71s\n",
            "        83           1.0197           0.0004            3.65s\n",
            "        84           1.0190           0.0003            3.60s\n",
            "        85           1.0175           0.0005            3.54s\n",
            "        86           1.0162           0.0003            3.49s\n",
            "        87           1.0197           0.0004            3.44s\n",
            "        88           1.0161           0.0003            3.39s\n",
            "        89           1.0169           0.0002            3.33s\n",
            "        90           1.0100           0.0002            3.28s\n",
            "        91           1.0170           0.0003            3.22s\n",
            "        92           1.0138           0.0002            3.16s\n",
            "        93           1.0131           0.0002            3.11s\n",
            "        94           1.0125           0.0002            3.06s\n",
            "        95           1.0117           0.0003            3.00s\n",
            "        96           1.0103           0.0002            2.95s\n",
            "        97           1.0120           0.0003            2.89s\n",
            "        98           1.0123           0.0002            2.84s\n",
            "        99           1.0126           0.0001            2.78s\n",
            "       100           1.0090           0.0002            2.73s\n",
            "       101           1.0077           0.0001            2.67s\n",
            "       102           1.0112           0.0001            2.62s\n",
            "       103           1.0108           0.0002            2.56s\n",
            "       104           1.0133           0.0003            2.51s\n",
            "       105           1.0075           0.0002            2.45s\n",
            "       106           1.0057           0.0002            2.40s\n",
            "       107           1.0097           0.0001            2.34s\n",
            "       108           1.0123           0.0001            2.29s\n",
            "       109           1.0088           0.0001            2.23s\n",
            "       110           1.0077           0.0001            2.18s\n",
            "       111           1.0115           0.0001            2.12s\n",
            "       112           1.0102           0.0001            2.07s\n",
            "       113           1.0054           0.0001            2.02s\n",
            "       114           1.0082           0.0001            1.96s\n",
            "       115           1.0096           0.0002            1.90s\n",
            "       116           1.0094           0.0002            1.85s\n",
            "       117           1.0058           0.0001            1.79s\n",
            "       118           1.0073           0.0001            1.74s\n",
            "       119           1.0065           0.0001            1.68s\n",
            "       120           1.0054           0.0001            1.63s\n",
            "       121           1.0069           0.0002            1.58s\n",
            "       122           1.0032           0.0001            1.52s\n",
            "       123           1.0049           0.0001            1.47s\n",
            "       124           1.0079           0.0001            1.41s\n",
            "       125           1.0008           0.0001            1.36s\n",
            "       126           1.0040           0.0001            1.31s\n",
            "       127           1.0051           0.0000            1.25s\n",
            "       128           1.0058           0.0001            1.20s\n",
            "       129           1.0049           0.0000            1.14s\n",
            "       130           1.0060           0.0001            1.09s\n",
            "       131           1.0048           0.0001            1.03s\n",
            "       132           1.0032           0.0001            0.98s\n",
            "       133           1.0025          -0.0000            0.92s\n",
            "       134           1.0038           0.0000            0.87s\n",
            "       135           1.0041           0.0001            0.81s\n",
            "       136           1.0022          -0.0000            0.76s\n",
            "       137           1.0038           0.0001            0.71s\n",
            "       138           1.0046          -0.0000            0.65s\n",
            "       139           1.0014           0.0001            0.60s\n",
            "       140           1.0050           0.0000            0.54s\n",
            "       141           0.9991           0.0000            0.49s\n",
            "       142           1.0022           0.0000            0.43s\n",
            "       143           1.0043          -0.0000            0.38s\n",
            "       144           1.0015           0.0001            0.33s\n",
            "       145           1.0029           0.0000            0.27s\n",
            "       146           1.0021           0.0001            0.22s\n",
            "       147           1.0001           0.0000            0.16s\n",
            "       148           1.0022           0.0001            0.11s\n",
            "       149           1.0026          -0.0000            0.05s\n",
            "       150           1.0019          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.74      7010\n",
            "         1.0       0.73      0.79      0.76      7129\n",
            "\n",
            "    accuracy                           0.75     14139\n",
            "   macro avg       0.75      0.75      0.75     14139\n",
            "weighted avg       0.75      0.75      0.75     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The deviance loss\n",
        "Deviance loss is a commonly used loss function in binary classification problems. With a glance at its formula, we can easily unserstand why:\n",
        "\n",
        "$$\n",
        "L(y, p) = \\left(y \\log(p) + (1 - y) \\log(1 - p)\\right)\n",
        "$$\n",
        "\n",
        "where y is true class and p is statistical probability.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uZs3QLvqMiLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F-1 score\n",
        "F-1 score uses precision(ratio of true possitives to true possitves and false possitives) and recall(ratio of true possitives to true possitves and false negatives) scores to prvoide a balance between them:\n",
        "\n",
        "$$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n"
      ],
      "metadata": {
        "id": "EQZOaVr-tgUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = cross_val_score(reg, x_train, y_train, cv=5, scoring='f1_macro')\n",
        "# print(\"cross validation scores(F-1) where k=5: \", scores)"
      ],
      "metadata": {
        "id": "DXcnTyapIGf-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = cross_val_score(reg, x_train, y_train, cv=10, scoring='f1_macro')\n",
        "# print(\"cross validation scores(F-1) where k=10: \", scores)"
      ],
      "metadata": {
        "id": "qMnr_XTivGkY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial evaluataion result\n",
        "As demonstrated above, whether employing Gradient Boosting with or without cross-validation, the F1 score hovers around 0.75. While this performance is acceptable, there is room for improvement."
      ],
      "metadata": {
        "id": "k3bKt0KavyOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second model: Logistic regression\n",
        "While Logistic Regression is typically considered a more linear model compared to ensemble methods, it remains a highly prevalent choice in classification problems. It offers several distinct advantages, such as strong interpretability, feature importance insights, and the ability to not only make binary classifications but also provide class probabilities. This probabilistic aspect can prove particularly valuable in certain situations.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nee1jLoqwq3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_model(dataset):\n",
        "  x_train, x_test, y_train, y_test = get_data(dataset)\n",
        "  log_reg = LogisticRegression(random_state=32, solver='sag', multi_class='multinomial', verbose=2, max_iter=500)\n",
        "  log_reg.fit(x_train, y_train)\n",
        "  y_pred_log_reg = log_reg.predict(x_test)\n",
        "  report_log_reg = classification_report(y_test, y_pred_log_reg)\n",
        "  print(log_reg.coef_)\n",
        "  print(report_log_reg)"
      ],
      "metadata": {
        "id": "Wf0DjjTz8ZgA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(raw_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_vmMs0o8ohm",
        "outputId": "947c3732-074e-400e-b201-79150007936e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 183 epochs took 8 seconds\n",
            "[[ 0.37257845  0.28995716  0.66909954  0.0368971  -0.00249678  0.09599408\n",
            "   0.11823623 -0.01565236 -0.01979998 -0.05413245 -0.37604701  0.00626531\n",
            "   0.02791119  0.30086121 -0.00214749 -0.00390029  0.08630184  0.11340987\n",
            "   0.07794295 -0.03393694]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation result\n",
        "Logistic regression exhibited slightly lower performance compared to Gradient Boosting, indicating that additional data preprocessing may be necessary to enhance model outcomes."
      ],
      "metadata": {
        "id": "4ogNdchF1lL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for class imbalance"
      ],
      "metadata": {
        "id": "Z8cK7DTC4WB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.bincount(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hRAk6_d3_f-",
        "outputId": "6941c912-ec1d-4768-e5f8-3d1e0146971d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([35346, 35346])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardizing features\n",
        "In this section we standardize featuers that their domain may mislead oue models."
      ],
      "metadata": {
        "id": "xruScgS86yxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_standardize = list(x.keys())"
      ],
      "metadata": {
        "id": "yhunvldR63yy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "standarized_features = scaler.fit_transform(raw_dataset[columns_to_standardize])\n",
        "standardized_dataset = pd.DataFrame()\n",
        "standardized_dataset[\"Diabetes_binary\"] = raw_dataset[\"Diabetes_binary\"]\n",
        "standardized_dataset[columns_to_standardize] = standarized_features"
      ],
      "metadata": {
        "id": "BwgHyv_V7aZ9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(standardized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAo1SN_EC-WW",
        "outputId": "719a9f20-624e-43b4-8955-802c26c4ec70"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3619           0.0245            7.87s\n",
            "         2           1.3400           0.0224            7.74s\n",
            "         3           1.3200           0.0203            7.65s\n",
            "         4           1.3008           0.0178            7.83s\n",
            "         5           1.2841           0.0163            7.74s\n",
            "         6           1.2685           0.0151            7.71s\n",
            "         7           1.2545           0.0136            7.67s\n",
            "         8           1.2417           0.0127            7.67s\n",
            "         9           1.2303           0.0117            7.60s\n",
            "        10           1.2191           0.0112            7.52s\n",
            "        11           1.2090           0.0101            7.46s\n",
            "        12           1.1982           0.0100            7.49s\n",
            "        13           1.1890           0.0094            7.42s\n",
            "        14           1.1793           0.0088            7.38s\n",
            "        15           1.1707           0.0078            7.32s\n",
            "        16           1.1641           0.0067            7.28s\n",
            "        17           1.1571           0.0076            7.22s\n",
            "        18           1.1502           0.0062            7.16s\n",
            "        19           1.1433           0.0058            7.10s\n",
            "        20           1.1379           0.0066            7.06s\n",
            "        21           1.1312           0.0059            7.00s\n",
            "        22           1.1278           0.0050            6.94s\n",
            "        23           1.1231           0.0050            6.90s\n",
            "        24           1.1165           0.0051            6.86s\n",
            "        25           1.1117           0.0040            6.80s\n",
            "        26           1.1087           0.0043            6.73s\n",
            "        27           1.1027           0.0045            6.67s\n",
            "        28           1.0968           0.0040            6.63s\n",
            "        29           1.0944           0.0034            6.57s\n",
            "        30           1.0899           0.0036            6.58s\n",
            "        31           1.0874           0.0029            6.52s\n",
            "        32           1.0859           0.0034            6.48s\n",
            "        33           1.0807           0.0030            6.42s\n",
            "        34           1.0780           0.0027            6.35s\n",
            "        35           1.0783           0.0027            6.29s\n",
            "        36           1.0720           0.0022            6.24s\n",
            "        37           1.0695           0.0025            6.18s\n",
            "        38           1.0690           0.0026            6.13s\n",
            "        39           1.0643           0.0021            6.07s\n",
            "        40           1.0638           0.0020            6.02s\n",
            "        41           1.0599           0.0021            6.01s\n",
            "        42           1.0596           0.0020            6.04s\n",
            "        43           1.0558           0.0017            6.06s\n",
            "        44           1.0562           0.0016            6.05s\n",
            "        45           1.0564           0.0018            6.07s\n",
            "        46           1.0509           0.0017            6.13s\n",
            "        47           1.0510           0.0016            6.15s\n",
            "        48           1.0466           0.0015            6.14s\n",
            "        49           1.0481           0.0013            6.17s\n",
            "        50           1.0459           0.0013            6.17s\n",
            "        51           1.0449           0.0012            6.18s\n",
            "        52           1.0439           0.0012            6.30s\n",
            "        53           1.0396           0.0012            6.62s\n",
            "        54           1.0412           0.0013            6.75s\n",
            "        55           1.0364           0.0008            6.77s\n",
            "        56           1.0387           0.0009            6.83s\n",
            "        57           1.0372           0.0009            6.92s\n",
            "        58           1.0372           0.0011            6.90s\n",
            "        59           1.0320           0.0008            6.83s\n",
            "        60           1.0339           0.0009            6.77s\n",
            "        61           1.0332           0.0008            6.70s\n",
            "        62           1.0304           0.0010            6.67s\n",
            "        63           1.0309           0.0009            6.66s\n",
            "        64           1.0313           0.0009            6.74s\n",
            "        65           1.0284           0.0007            6.67s\n",
            "        66           1.0303           0.0007            6.62s\n",
            "        67           1.0293           0.0007            6.55s\n",
            "        68           1.0260           0.0006            6.48s\n",
            "        69           1.0276           0.0006            6.40s\n",
            "        70           1.0231           0.0006            6.32s\n",
            "        71           1.0259           0.0006            6.34s\n",
            "        72           1.0230           0.0005            6.41s\n",
            "        73           1.0236           0.0005            6.34s\n",
            "        74           1.0214           0.0004            6.36s\n",
            "        75           1.0240           0.0006            6.51s\n",
            "        76           1.0205           0.0004            6.57s\n",
            "        77           1.0237           0.0004            6.59s\n",
            "        78           1.0187           0.0004            6.54s\n",
            "        79           1.0194           0.0003            6.56s\n",
            "        80           1.0192           0.0004            6.85s\n",
            "        81           1.0185           0.0005            7.09s\n",
            "        82           1.0164           0.0004            7.09s\n",
            "        83           1.0197           0.0004            7.08s\n",
            "        84           1.0190           0.0003            7.06s\n",
            "        85           1.0175           0.0005            7.06s\n",
            "        86           1.0162           0.0003            7.03s\n",
            "        87           1.0197           0.0004            7.05s\n",
            "        88           1.0161           0.0003            7.00s\n",
            "        89           1.0169           0.0002            6.89s\n",
            "        90           1.0100           0.0002            6.82s\n",
            "        91           1.0170           0.0003            6.71s\n",
            "        92           1.0138           0.0002            6.58s\n",
            "        93           1.0131           0.0002            6.50s\n",
            "        94           1.0125           0.0002            6.40s\n",
            "        95           1.0117           0.0003            6.27s\n",
            "        96           1.0103           0.0002            6.17s\n",
            "        97           1.0120           0.0003            6.05s\n",
            "        98           1.0123           0.0002            5.92s\n",
            "        99           1.0126           0.0001            5.81s\n",
            "       100           1.0090           0.0002            5.68s\n",
            "       101           1.0077           0.0001            5.56s\n",
            "       102           1.0112           0.0001            5.44s\n",
            "       103           1.0108           0.0002            5.31s\n",
            "       104           1.0133           0.0003            5.18s\n",
            "       105           1.0075           0.0002            5.06s\n",
            "       106           1.0057           0.0002            4.94s\n",
            "       107           1.0097           0.0001            4.84s\n",
            "       108           1.0123           0.0001            4.76s\n",
            "       109           1.0088           0.0001            4.66s\n",
            "       110           1.0077           0.0001            4.53s\n",
            "       111           1.0115           0.0001            4.43s\n",
            "       112           1.0102           0.0001            4.31s\n",
            "       113           1.0054           0.0001            4.21s\n",
            "       114           1.0082           0.0001            4.10s\n",
            "       115           1.0096           0.0002            3.99s\n",
            "       116           1.0094           0.0002            3.87s\n",
            "       117           1.0058           0.0001            3.74s\n",
            "       118           1.0073           0.0001            3.63s\n",
            "       119           1.0065           0.0001            3.53s\n",
            "       120           1.0054           0.0001            3.44s\n",
            "       121           1.0069           0.0002            3.33s\n",
            "       122           1.0032           0.0001            3.21s\n",
            "       123           1.0049           0.0001            3.08s\n",
            "       124           1.0079           0.0001            2.97s\n",
            "       125           1.0008           0.0001            2.84s\n",
            "       126           1.0040           0.0001            2.71s\n",
            "       127           1.0051           0.0000            2.59s\n",
            "       128           1.0058           0.0001            2.47s\n",
            "       129           1.0049           0.0000            2.35s\n",
            "       130           1.0060           0.0001            2.22s\n",
            "       131           1.0048           0.0001            2.11s\n",
            "       132           1.0032           0.0001            1.99s\n",
            "       133           1.0025          -0.0000            1.87s\n",
            "       134           1.0038           0.0000            1.75s\n",
            "       135           1.0041           0.0001            1.64s\n",
            "       136           1.0022          -0.0000            1.52s\n",
            "       137           1.0038           0.0001            1.41s\n",
            "       138           1.0046          -0.0000            1.29s\n",
            "       139           1.0014           0.0001            1.18s\n",
            "       140           1.0050           0.0000            1.07s\n",
            "       141           0.9991           0.0000            0.96s\n",
            "       142           1.0022           0.0000            0.85s\n",
            "       143           1.0043          -0.0000            0.74s\n",
            "       144           1.0015           0.0001            0.63s\n",
            "       145           1.0029           0.0000            0.53s\n",
            "       146           1.0021           0.0001            0.42s\n",
            "       147           1.0001           0.0000            0.31s\n",
            "       148           1.0022           0.0001            0.21s\n",
            "       149           1.0025          -0.0000            0.10s\n",
            "       150           1.0019          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.74      7010\n",
            "         1.0       0.73      0.79      0.76      7129\n",
            "\n",
            "    accuracy                           0.75     14139\n",
            "   macro avg       0.75      0.75      0.75     14139\n",
            "weighted avg       0.75      0.75      0.75     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(standardized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxiEcthP8uY3",
        "outputId": "6b6a2acc-2e8c-4ff6-8cc0-1c377c78a8e6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 35 epochs took 1 seconds\n",
            "[[ 0.18478788  0.14481089  0.10466094  0.26256455 -0.00122275  0.02319897\n",
            "   0.04193551 -0.007137   -0.00966964 -0.02206856 -0.07615758  0.00134743\n",
            "   0.00820435  0.33510984 -0.01749871 -0.03925738  0.03750007  0.05652744\n",
            "   0.22233715 -0.03485117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.75      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing features\n",
        "Standardization helped the convergance of our model, but didn't countribute to the evaluation metrics. Now we try with normalized data."
      ],
      "metadata": {
        "id": "0MOiYC7bBXQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_normalize = list(x.keys())"
      ],
      "metadata": {
        "id": "bO4AayIo_wCw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "normalized_features = min_max_scaler.fit_transform(raw_dataset[columns_to_standardize])\n",
        "normalized_dataset = pd.DataFrame()\n",
        "normalized_dataset[\"Diabetes_binary\"] = raw_dataset[\"Diabetes_binary\"]\n",
        "normalized_dataset[columns_to_standardize] = normalized_features"
      ],
      "metadata": {
        "id": "0LIS3MbUBroL"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(normalized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGrBEi4iDFzt",
        "outputId": "47fb22d7-308d-44ae-a251-983ad132eae6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         1           1.3619           0.0245            9.25s\n",
            "         2           1.3400           0.0224            8.53s\n",
            "         3           1.3200           0.0203            8.36s\n",
            "         4           1.3008           0.0178            8.38s\n",
            "         5           1.2841           0.0163            8.32s\n",
            "         6           1.2685           0.0151            8.12s\n",
            "         7           1.2545           0.0136            8.06s\n",
            "         8           1.2417           0.0127            7.98s\n",
            "         9           1.2303           0.0117            7.90s\n",
            "        10           1.2191           0.0112            7.79s\n",
            "        11           1.2090           0.0101            7.69s\n",
            "        12           1.1982           0.0100            7.62s\n",
            "        13           1.1890           0.0094            7.55s\n",
            "        14           1.1793           0.0088            7.45s\n",
            "        15           1.1707           0.0078            7.36s\n",
            "        16           1.1641           0.0067            7.31s\n",
            "        17           1.1571           0.0076            7.26s\n",
            "        18           1.1502           0.0062            7.18s\n",
            "        19           1.1433           0.0058            7.11s\n",
            "        20           1.1379           0.0066            7.13s\n",
            "        21           1.1312           0.0059            7.07s\n",
            "        22           1.1278           0.0050            7.01s\n",
            "        23           1.1231           0.0050            6.94s\n",
            "        24           1.1165           0.0051            6.88s\n",
            "        25           1.1117           0.0040            6.83s\n",
            "        26           1.1087           0.0043            6.77s\n",
            "        27           1.1027           0.0045            6.71s\n",
            "        28           1.0968           0.0040            6.66s\n",
            "        29           1.0944           0.0034            6.60s\n",
            "        30           1.0899           0.0036            6.54s\n",
            "        31           1.0874           0.0029            6.47s\n",
            "        32           1.0859           0.0034            6.42s\n",
            "        33           1.0807           0.0030            6.36s\n",
            "        34           1.0780           0.0027            6.30s\n",
            "        35           1.0783           0.0027            6.24s\n",
            "        36           1.0720           0.0022            6.18s\n",
            "        37           1.0695           0.0025            6.13s\n",
            "        38           1.0690           0.0026            6.07s\n",
            "        39           1.0643           0.0021            6.03s\n",
            "        40           1.0638           0.0020            5.97s\n",
            "        41           1.0599           0.0021            5.93s\n",
            "        42           1.0596           0.0020            5.87s\n",
            "        43           1.0558           0.0017            5.81s\n",
            "        44           1.0562           0.0016            5.75s\n",
            "        45           1.0564           0.0018            5.70s\n",
            "        46           1.0509           0.0017            5.64s\n",
            "        47           1.0510           0.0016            5.58s\n",
            "        48           1.0466           0.0015            5.53s\n",
            "        49           1.0481           0.0013            5.47s\n",
            "        50           1.0459           0.0013            5.41s\n",
            "        51           1.0449           0.0012            5.35s\n",
            "        52           1.0439           0.0012            5.30s\n",
            "        53           1.0396           0.0012            5.26s\n",
            "        54           1.0412           0.0013            5.19s\n",
            "        55           1.0364           0.0008            5.14s\n",
            "        56           1.0387           0.0009            5.08s\n",
            "        57           1.0372           0.0009            5.03s\n",
            "        58           1.0372           0.0011            4.98s\n",
            "        59           1.0320           0.0008            4.94s\n",
            "        60           1.0339           0.0009            4.92s\n",
            "        61           1.0332           0.0008            4.90s\n",
            "        62           1.0304           0.0010            4.88s\n",
            "        63           1.0309           0.0009            4.86s\n",
            "        64           1.0313           0.0009            4.83s\n",
            "        65           1.0284           0.0007            4.80s\n",
            "        66           1.0303           0.0007            4.78s\n",
            "        67           1.0293           0.0007            4.75s\n",
            "        68           1.0260           0.0006            4.72s\n",
            "        69           1.0276           0.0006            4.70s\n",
            "        70           1.0231           0.0006            4.66s\n",
            "        71           1.0259           0.0006            4.63s\n",
            "        72           1.0230           0.0005            4.61s\n",
            "        73           1.0236           0.0005            4.58s\n",
            "        74           1.0214           0.0004            4.54s\n",
            "        75           1.0240           0.0006            4.50s\n",
            "        76           1.0205           0.0004            4.46s\n",
            "        77           1.0237           0.0004            4.42s\n",
            "        78           1.0187           0.0004            4.38s\n",
            "        79           1.0194           0.0003            4.33s\n",
            "        80           1.0192           0.0004            4.29s\n",
            "        81           1.0185           0.0005            4.25s\n",
            "        82           1.0164           0.0004            4.20s\n",
            "        83           1.0197           0.0004            4.15s\n",
            "        84           1.0190           0.0003            4.11s\n",
            "        85           1.0175           0.0005            4.06s\n",
            "        86           1.0162           0.0003            4.01s\n",
            "        87           1.0197           0.0004            3.97s\n",
            "        88           1.0161           0.0003            3.91s\n",
            "        89           1.0169           0.0002            3.86s\n",
            "        90           1.0100           0.0002            3.81s\n",
            "        91           1.0170           0.0003            3.76s\n",
            "        92           1.0138           0.0002            3.70s\n",
            "        93           1.0131           0.0002            3.65s\n",
            "        94           1.0125           0.0002            3.60s\n",
            "        95           1.0117           0.0003            3.54s\n",
            "        96           1.0103           0.0002            3.49s\n",
            "        97           1.0120           0.0003            3.44s\n",
            "        98           1.0123           0.0002            3.38s\n",
            "        99           1.0126           0.0001            3.33s\n",
            "       100           1.0090           0.0002            3.27s\n",
            "       101           1.0077           0.0001            3.21s\n",
            "       102           1.0112           0.0001            3.15s\n",
            "       103           1.0108           0.0002            3.09s\n",
            "       104           1.0133           0.0003            3.03s\n",
            "       105           1.0075           0.0002            2.97s\n",
            "       106           1.0057           0.0002            2.91s\n",
            "       107           1.0097           0.0001            2.85s\n",
            "       108           1.0123           0.0001            2.79s\n",
            "       109           1.0088           0.0001            2.73s\n",
            "       110           1.0077           0.0001            2.67s\n",
            "       111           1.0115           0.0001            2.60s\n",
            "       112           1.0102           0.0001            2.53s\n",
            "       113           1.0054           0.0001            2.45s\n",
            "       114           1.0082           0.0001            2.38s\n",
            "       115           1.0096           0.0002            2.31s\n",
            "       116           1.0094           0.0002            2.24s\n",
            "       117           1.0058           0.0001            2.17s\n",
            "       118           1.0073           0.0001            2.10s\n",
            "       119           1.0065           0.0001            2.03s\n",
            "       120           1.0054           0.0001            1.97s\n",
            "       121           1.0069           0.0002            1.90s\n",
            "       122           1.0032           0.0001            1.83s\n",
            "       123           1.0049           0.0001            1.76s\n",
            "       124           1.0079           0.0001            1.69s\n",
            "       125           1.0008           0.0001            1.62s\n",
            "       126           1.0040           0.0001            1.56s\n",
            "       127           1.0051           0.0000            1.49s\n",
            "       128           1.0058           0.0001            1.42s\n",
            "       129           1.0049           0.0000            1.36s\n",
            "       130           1.0060           0.0001            1.29s\n",
            "       131           1.0048           0.0001            1.22s\n",
            "       132           1.0032           0.0001            1.16s\n",
            "       133           1.0025          -0.0000            1.09s\n",
            "       134           1.0038           0.0000            1.03s\n",
            "       135           1.0041           0.0001            0.96s\n",
            "       136           1.0022          -0.0000            0.89s\n",
            "       137           1.0038           0.0001            0.83s\n",
            "       138           1.0046          -0.0000            0.76s\n",
            "       139           1.0014           0.0001            0.70s\n",
            "       140           1.0050           0.0000            0.64s\n",
            "       141           0.9991           0.0000            0.57s\n",
            "       142           1.0022           0.0000            0.51s\n",
            "       143           1.0043          -0.0000            0.44s\n",
            "       144           1.0015           0.0001            0.38s\n",
            "       145           1.0029           0.0000            0.32s\n",
            "       146           1.0021           0.0001            0.25s\n",
            "       147           1.0001           0.0000            0.19s\n",
            "       148           1.0022           0.0001            0.13s\n",
            "       149           1.0026          -0.0000            0.06s\n",
            "       150           1.0019          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.74      7010\n",
            "         1.0       0.73      0.79      0.76      7129\n",
            "\n",
            "    accuracy                           0.75     14139\n",
            "   macro avg       0.75      0.75      0.75     14139\n",
            "weighted avg       0.75      0.75      0.75     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(normalized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQZQbxroB6V3",
        "outputId": "174934af-d67d-40f0-b904-cc3c6fc07081"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 47 epochs took 2 seconds\n",
            "[[ 3.73490615e-01  2.90048149e-01  6.70926918e-01  3.13847638e+00\n",
            "  -2.39854135e-03  9.56291752e-02  1.18219842e-01 -1.60231524e-02\n",
            "  -1.99615981e-02 -5.40853570e-02 -3.76418229e-01  6.72929731e-03\n",
            "   2.79537211e-02  1.20272703e+00 -6.45089519e-02 -1.16871052e-01\n",
            "   8.72398803e-02  1.13443496e-01  9.31422712e-01 -1.69375700e-01]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.75      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What happened?\n",
        "It turned out that algorithms like Logitstic regression and Gradientboost are robust to data scale due to a number of factors like their loss functions, use of decision trees and regularization factors, etc. So we have to find another way to reach our goal.\n",
        "\n",
        "# Next model: DNN\n",
        "neural networks are the master of finding complex relations between featurse. In addition to that, they can be combined with various functionalities to improve model's behavoir even further, e.g. optimizers, regularization factors, etc."
      ],
      "metadata": {
        "id": "UN0ILPX_Bh9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dnn_model(dataset):\n",
        "  x_train, x_test, y_train, y_test = get_data(dataset)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, input_dim=x_train.shape[1], activation=LeakyReLU(alpha=0.1), kernel_initializer=he_normal()))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(32, activation='relu', kernel_regularizer=l1(0.1)))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  adam = Adagrad(learning_rate=0.1)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[Precision(), Recall()])\n",
        "  model.fit(x_train, y_train, epochs=100, verbose=2, validation_split=0.1, batch_size=100,)\n",
        "  res = model.evaluate(x_test, y_test)\n",
        "  print(\"binary cross-entropy loss : \", res[0], \" precision: \", res[1], \" recal: \", res[2])"
      ],
      "metadata": {
        "id": "FcQCSaw-665S"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model(standardized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPxwqurHQ7eu",
        "outputId": "9dd1ad23-a1fd-460a-9b15-2affdde34a9e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "509/509 - 2s - loss: 2.3990 - precision_1: 0.6433 - recall_1: 0.5997 - val_loss: 1.4802 - val_precision_1: 0.7731 - val_recall_1: 0.6284 - 2s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "509/509 - 1s - loss: 1.3142 - precision_1: 0.7220 - recall_1: 0.7479 - val_loss: 1.1847 - val_precision_1: 0.7292 - val_recall_1: 0.7798 - 1s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "509/509 - 1s - loss: 1.1248 - precision_1: 0.7175 - recall_1: 0.7837 - val_loss: 1.0481 - val_precision_1: 0.7037 - val_recall_1: 0.8435 - 1s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "509/509 - 1s - loss: 1.0250 - precision_1: 0.7157 - recall_1: 0.7965 - val_loss: 0.9885 - val_precision_1: 0.6973 - val_recall_1: 0.8593 - 1s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "509/509 - 1s - loss: 0.9607 - precision_1: 0.7148 - recall_1: 0.8081 - val_loss: 0.9263 - val_precision_1: 0.7064 - val_recall_1: 0.8392 - 1s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "509/509 - 2s - loss: 0.9196 - precision_1: 0.7128 - recall_1: 0.8163 - val_loss: 0.8906 - val_precision_1: 0.7027 - val_recall_1: 0.8471 - 2s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "509/509 - 2s - loss: 0.8852 - precision_1: 0.7153 - recall_1: 0.8170 - val_loss: 0.8582 - val_precision_1: 0.7174 - val_recall_1: 0.8117 - 2s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "509/509 - 1s - loss: 0.8534 - precision_1: 0.7144 - recall_1: 0.8139 - val_loss: 0.8361 - val_precision_1: 0.7007 - val_recall_1: 0.8532 - 1s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "509/509 - 1s - loss: 0.8287 - precision_1: 0.7133 - recall_1: 0.8189 - val_loss: 0.8142 - val_precision_1: 0.7077 - val_recall_1: 0.8400 - 1s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "509/509 - 1s - loss: 0.8101 - precision_1: 0.7155 - recall_1: 0.8142 - val_loss: 0.7989 - val_precision_1: 0.7306 - val_recall_1: 0.7769 - 1s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "509/509 - 1s - loss: 0.7981 - precision_1: 0.7128 - recall_1: 0.8213 - val_loss: 0.7828 - val_precision_1: 0.7247 - val_recall_1: 0.7938 - 1s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "509/509 - 1s - loss: 0.7838 - precision_1: 0.7153 - recall_1: 0.8180 - val_loss: 0.7730 - val_precision_1: 0.7055 - val_recall_1: 0.8421 - 1s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "509/509 - 1s - loss: 0.7703 - precision_1: 0.7133 - recall_1: 0.8195 - val_loss: 0.7541 - val_precision_1: 0.7104 - val_recall_1: 0.8360 - 1s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "509/509 - 1s - loss: 0.7585 - precision_1: 0.7126 - recall_1: 0.8252 - val_loss: 0.7484 - val_precision_1: 0.7020 - val_recall_1: 0.8493 - 1s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "509/509 - 1s - loss: 0.7484 - precision_1: 0.7099 - recall_1: 0.8286 - val_loss: 0.7340 - val_precision_1: 0.7067 - val_recall_1: 0.8410 - 1s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "509/509 - 1s - loss: 0.7417 - precision_1: 0.7136 - recall_1: 0.8260 - val_loss: 0.7384 - val_precision_1: 0.7039 - val_recall_1: 0.8478 - 1s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "509/509 - 2s - loss: 0.7357 - precision_1: 0.7115 - recall_1: 0.8276 - val_loss: 0.7267 - val_precision_1: 0.7021 - val_recall_1: 0.8521 - 2s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "509/509 - 2s - loss: 0.7335 - precision_1: 0.7088 - recall_1: 0.8346 - val_loss: 0.7292 - val_precision_1: 0.7069 - val_recall_1: 0.8435 - 2s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "509/509 - 1s - loss: 0.7246 - precision_1: 0.7133 - recall_1: 0.8284 - val_loss: 0.7037 - val_precision_1: 0.6950 - val_recall_1: 0.8657 - 1s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "509/509 - 1s - loss: 0.7168 - precision_1: 0.7082 - recall_1: 0.8353 - val_loss: 0.7097 - val_precision_1: 0.7020 - val_recall_1: 0.8561 - 1s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "509/509 - 1s - loss: 0.7097 - precision_1: 0.7081 - recall_1: 0.8380 - val_loss: 0.6938 - val_precision_1: 0.7076 - val_recall_1: 0.8378 - 1s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "509/509 - 1s - loss: 0.7025 - precision_1: 0.7101 - recall_1: 0.8356 - val_loss: 0.6962 - val_precision_1: 0.7160 - val_recall_1: 0.8206 - 1s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "509/509 - 1s - loss: 0.6958 - precision_1: 0.7075 - recall_1: 0.8370 - val_loss: 0.6845 - val_precision_1: 0.6979 - val_recall_1: 0.8604 - 1s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "509/509 - 1s - loss: 0.6917 - precision_1: 0.7087 - recall_1: 0.8343 - val_loss: 0.6873 - val_precision_1: 0.6891 - val_recall_1: 0.8754 - 1s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "509/509 - 1s - loss: 0.6861 - precision_1: 0.7097 - recall_1: 0.8359 - val_loss: 0.6742 - val_precision_1: 0.6922 - val_recall_1: 0.8704 - 1s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "509/509 - 1s - loss: 0.6819 - precision_1: 0.7075 - recall_1: 0.8394 - val_loss: 0.6728 - val_precision_1: 0.7024 - val_recall_1: 0.8493 - 1s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "509/509 - 1s - loss: 0.6781 - precision_1: 0.7078 - recall_1: 0.8396 - val_loss: 0.6647 - val_precision_1: 0.7085 - val_recall_1: 0.8353 - 1s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "509/509 - 1s - loss: 0.6719 - precision_1: 0.7074 - recall_1: 0.8376 - val_loss: 0.6668 - val_precision_1: 0.6983 - val_recall_1: 0.8593 - 1s/epoch - 3ms/step\n",
            "Epoch 29/100\n",
            "509/509 - 2s - loss: 0.6690 - precision_1: 0.7070 - recall_1: 0.8388 - val_loss: 0.6584 - val_precision_1: 0.6971 - val_recall_1: 0.8618 - 2s/epoch - 3ms/step\n",
            "Epoch 30/100\n",
            "509/509 - 2s - loss: 0.6641 - precision_1: 0.7058 - recall_1: 0.8390 - val_loss: 0.6619 - val_precision_1: 0.6916 - val_recall_1: 0.8704 - 2s/epoch - 3ms/step\n",
            "Epoch 31/100\n",
            "509/509 - 1s - loss: 0.6595 - precision_1: 0.7096 - recall_1: 0.8395 - val_loss: 0.6466 - val_precision_1: 0.7009 - val_recall_1: 0.8539 - 1s/epoch - 3ms/step\n",
            "Epoch 32/100\n",
            "509/509 - 1s - loss: 0.6566 - precision_1: 0.7094 - recall_1: 0.8370 - val_loss: 0.6578 - val_precision_1: 0.7008 - val_recall_1: 0.8561 - 1s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "509/509 - 1s - loss: 0.6568 - precision_1: 0.7068 - recall_1: 0.8405 - val_loss: 0.6454 - val_precision_1: 0.6974 - val_recall_1: 0.8596 - 1s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "509/509 - 1s - loss: 0.6548 - precision_1: 0.7067 - recall_1: 0.8407 - val_loss: 0.6536 - val_precision_1: 0.6983 - val_recall_1: 0.8604 - 1s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "509/509 - 1s - loss: 0.6512 - precision_1: 0.7061 - recall_1: 0.8437 - val_loss: 0.6394 - val_precision_1: 0.7023 - val_recall_1: 0.8507 - 1s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "509/509 - 1s - loss: 0.6480 - precision_1: 0.7065 - recall_1: 0.8435 - val_loss: 0.6433 - val_precision_1: 0.6989 - val_recall_1: 0.8593 - 1s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "509/509 - 1s - loss: 0.6441 - precision_1: 0.7038 - recall_1: 0.8456 - val_loss: 0.6348 - val_precision_1: 0.6922 - val_recall_1: 0.8686 - 1s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "509/509 - 1s - loss: 0.6411 - precision_1: 0.7075 - recall_1: 0.8431 - val_loss: 0.6355 - val_precision_1: 0.6952 - val_recall_1: 0.8622 - 1s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "509/509 - 1s - loss: 0.6400 - precision_1: 0.7061 - recall_1: 0.8438 - val_loss: 0.6304 - val_precision_1: 0.7026 - val_recall_1: 0.8511 - 1s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "509/509 - 1s - loss: 0.6381 - precision_1: 0.7049 - recall_1: 0.8425 - val_loss: 0.6325 - val_precision_1: 0.7038 - val_recall_1: 0.8482 - 1s/epoch - 3ms/step\n",
            "Epoch 41/100\n",
            "509/509 - 2s - loss: 0.6377 - precision_1: 0.7094 - recall_1: 0.8403 - val_loss: 0.6334 - val_precision_1: 0.6969 - val_recall_1: 0.8611 - 2s/epoch - 3ms/step\n",
            "Epoch 42/100\n",
            "509/509 - 2s - loss: 0.6372 - precision_1: 0.7059 - recall_1: 0.8450 - val_loss: 0.6281 - val_precision_1: 0.7022 - val_recall_1: 0.8485 - 2s/epoch - 3ms/step\n",
            "Epoch 43/100\n",
            "509/509 - 1s - loss: 0.6353 - precision_1: 0.7089 - recall_1: 0.8429 - val_loss: 0.6339 - val_precision_1: 0.7032 - val_recall_1: 0.8468 - 1s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "509/509 - 1s - loss: 0.6367 - precision_1: 0.7075 - recall_1: 0.8397 - val_loss: 0.6250 - val_precision_1: 0.7079 - val_recall_1: 0.8382 - 1s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "509/509 - 1s - loss: 0.6340 - precision_1: 0.7059 - recall_1: 0.8456 - val_loss: 0.6344 - val_precision_1: 0.7103 - val_recall_1: 0.8357 - 1s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "509/509 - 1s - loss: 0.6311 - precision_1: 0.7071 - recall_1: 0.8389 - val_loss: 0.6168 - val_precision_1: 0.6995 - val_recall_1: 0.8568 - 1s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "509/509 - 1s - loss: 0.6268 - precision_1: 0.7072 - recall_1: 0.8446 - val_loss: 0.6258 - val_precision_1: 0.7042 - val_recall_1: 0.8439 - 1s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "509/509 - 1s - loss: 0.6238 - precision_1: 0.7057 - recall_1: 0.8452 - val_loss: 0.6106 - val_precision_1: 0.6971 - val_recall_1: 0.8604 - 1s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "509/509 - 1s - loss: 0.6235 - precision_1: 0.7019 - recall_1: 0.8525 - val_loss: 0.6272 - val_precision_1: 0.7016 - val_recall_1: 0.8503 - 1s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "509/509 - 1s - loss: 0.6228 - precision_1: 0.7071 - recall_1: 0.8431 - val_loss: 0.6085 - val_precision_1: 0.6997 - val_recall_1: 0.8568 - 1s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "509/509 - 1s - loss: 0.6195 - precision_1: 0.7031 - recall_1: 0.8492 - val_loss: 0.6187 - val_precision_1: 0.7011 - val_recall_1: 0.8525 - 1s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "509/509 - 1s - loss: 0.6194 - precision_1: 0.7077 - recall_1: 0.8412 - val_loss: 0.6086 - val_precision_1: 0.6982 - val_recall_1: 0.8589 - 1s/epoch - 3ms/step\n",
            "Epoch 53/100\n",
            "509/509 - 2s - loss: 0.6164 - precision_1: 0.7070 - recall_1: 0.8451 - val_loss: 0.6137 - val_precision_1: 0.7063 - val_recall_1: 0.8414 - 2s/epoch - 3ms/step\n",
            "Epoch 54/100\n",
            "509/509 - 2s - loss: 0.6188 - precision_1: 0.7067 - recall_1: 0.8440 - val_loss: 0.6078 - val_precision_1: 0.6872 - val_recall_1: 0.8826 - 2s/epoch - 3ms/step\n",
            "Epoch 55/100\n",
            "509/509 - 1s - loss: 0.6219 - precision_1: 0.7059 - recall_1: 0.8455 - val_loss: 0.6222 - val_precision_1: 0.6956 - val_recall_1: 0.8639 - 1s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "509/509 - 1s - loss: 0.6209 - precision_1: 0.7054 - recall_1: 0.8472 - val_loss: 0.6078 - val_precision_1: 0.7056 - val_recall_1: 0.8410 - 1s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "509/509 - 1s - loss: 0.6193 - precision_1: 0.7085 - recall_1: 0.8419 - val_loss: 0.6204 - val_precision_1: 0.6872 - val_recall_1: 0.8818 - 1s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "509/509 - 1s - loss: 0.6164 - precision_1: 0.7067 - recall_1: 0.8433 - val_loss: 0.6059 - val_precision_1: 0.6894 - val_recall_1: 0.8750 - 1s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "509/509 - 1s - loss: 0.6142 - precision_1: 0.7061 - recall_1: 0.8455 - val_loss: 0.6148 - val_precision_1: 0.7044 - val_recall_1: 0.8446 - 1s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "509/509 - 1s - loss: 0.6125 - precision_1: 0.7057 - recall_1: 0.8410 - val_loss: 0.6030 - val_precision_1: 0.6958 - val_recall_1: 0.8622 - 1s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "509/509 - 1s - loss: 0.6143 - precision_1: 0.7085 - recall_1: 0.8376 - val_loss: 0.6119 - val_precision_1: 0.6896 - val_recall_1: 0.8765 - 1s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "509/509 - 1s - loss: 0.6124 - precision_1: 0.7074 - recall_1: 0.8437 - val_loss: 0.6026 - val_precision_1: 0.6928 - val_recall_1: 0.8736 - 1s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "509/509 - 1s - loss: 0.6095 - precision_1: 0.7069 - recall_1: 0.8440 - val_loss: 0.6103 - val_precision_1: 0.6923 - val_recall_1: 0.8725 - 1s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "509/509 - 2s - loss: 0.6083 - precision_1: 0.7097 - recall_1: 0.8392 - val_loss: 0.5973 - val_precision_1: 0.6985 - val_recall_1: 0.8611 - 2s/epoch - 3ms/step\n",
            "Epoch 65/100\n",
            "509/509 - 2s - loss: 0.6072 - precision_1: 0.7062 - recall_1: 0.8409 - val_loss: 0.6063 - val_precision_1: 0.6928 - val_recall_1: 0.8704 - 2s/epoch - 3ms/step\n",
            "Epoch 66/100\n",
            "509/509 - 2s - loss: 0.6048 - precision_1: 0.7116 - recall_1: 0.8345 - val_loss: 0.5927 - val_precision_1: 0.6940 - val_recall_1: 0.8672 - 2s/epoch - 4ms/step\n",
            "Epoch 67/100\n",
            "509/509 - 2s - loss: 0.6034 - precision_1: 0.7121 - recall_1: 0.8354 - val_loss: 0.6032 - val_precision_1: 0.6989 - val_recall_1: 0.8600 - 2s/epoch - 3ms/step\n",
            "Epoch 68/100\n",
            "509/509 - 2s - loss: 0.6001 - precision_1: 0.7132 - recall_1: 0.8325 - val_loss: 0.5864 - val_precision_1: 0.7035 - val_recall_1: 0.8485 - 2s/epoch - 3ms/step\n",
            "Epoch 69/100\n",
            "509/509 - 1s - loss: 0.6029 - precision_1: 0.7121 - recall_1: 0.8325 - val_loss: 0.6034 - val_precision_1: 0.7066 - val_recall_1: 0.8371 - 1s/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "509/509 - 1s - loss: 0.6038 - precision_1: 0.7123 - recall_1: 0.8327 - val_loss: 0.5942 - val_precision_1: 0.7006 - val_recall_1: 0.8586 - 1s/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "509/509 - 1s - loss: 0.6011 - precision_1: 0.7125 - recall_1: 0.8350 - val_loss: 0.5987 - val_precision_1: 0.7243 - val_recall_1: 0.7984 - 1s/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "509/509 - 1s - loss: 0.5995 - precision_1: 0.7140 - recall_1: 0.8284 - val_loss: 0.5908 - val_precision_1: 0.7091 - val_recall_1: 0.8324 - 1s/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "509/509 - 1s - loss: 0.5987 - precision_1: 0.7141 - recall_1: 0.8258 - val_loss: 0.5984 - val_precision_1: 0.6979 - val_recall_1: 0.8625 - 1s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "509/509 - 1s - loss: 0.5982 - precision_1: 0.7178 - recall_1: 0.8225 - val_loss: 0.5918 - val_precision_1: 0.7030 - val_recall_1: 0.8493 - 1s/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "509/509 - 2s - loss: 0.5971 - precision_1: 0.7146 - recall_1: 0.8267 - val_loss: 0.5948 - val_precision_1: 0.7006 - val_recall_1: 0.8579 - 2s/epoch - 3ms/step\n",
            "Epoch 76/100\n",
            "509/509 - 2s - loss: 0.5959 - precision_1: 0.7135 - recall_1: 0.8247 - val_loss: 0.5874 - val_precision_1: 0.7007 - val_recall_1: 0.8568 - 2s/epoch - 3ms/step\n",
            "Epoch 77/100\n",
            "509/509 - 1s - loss: 0.5943 - precision_1: 0.7155 - recall_1: 0.8205 - val_loss: 0.5917 - val_precision_1: 0.7065 - val_recall_1: 0.8385 - 1s/epoch - 3ms/step\n",
            "Epoch 78/100\n",
            "509/509 - 1s - loss: 0.5927 - precision_1: 0.7190 - recall_1: 0.8158 - val_loss: 0.5856 - val_precision_1: 0.7036 - val_recall_1: 0.8464 - 1s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "509/509 - 1s - loss: 0.5907 - precision_1: 0.7196 - recall_1: 0.8121 - val_loss: 0.5856 - val_precision_1: 0.7107 - val_recall_1: 0.8314 - 1s/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "509/509 - 1s - loss: 0.5887 - precision_1: 0.7199 - recall_1: 0.8154 - val_loss: 0.5830 - val_precision_1: 0.6986 - val_recall_1: 0.8614 - 1s/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "509/509 - 1s - loss: 0.5872 - precision_1: 0.7187 - recall_1: 0.8198 - val_loss: 0.5832 - val_precision_1: 0.7295 - val_recall_1: 0.7848 - 1s/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "509/509 - 1s - loss: 0.5856 - precision_1: 0.7181 - recall_1: 0.8163 - val_loss: 0.5804 - val_precision_1: 0.6925 - val_recall_1: 0.8725 - 1s/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "509/509 - 1s - loss: 0.5832 - precision_1: 0.7188 - recall_1: 0.8188 - val_loss: 0.5828 - val_precision_1: 0.7182 - val_recall_1: 0.8120 - 1s/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "509/509 - 1s - loss: 0.5823 - precision_1: 0.7167 - recall_1: 0.8202 - val_loss: 0.5744 - val_precision_1: 0.6986 - val_recall_1: 0.8596 - 1s/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "509/509 - 1s - loss: 0.5801 - precision_1: 0.7202 - recall_1: 0.8179 - val_loss: 0.5770 - val_precision_1: 0.7087 - val_recall_1: 0.8346 - 1s/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "509/509 - 1s - loss: 0.5798 - precision_1: 0.7201 - recall_1: 0.8136 - val_loss: 0.5731 - val_precision_1: 0.6915 - val_recall_1: 0.8733 - 1s/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "509/509 - 2s - loss: 0.5784 - precision_1: 0.7200 - recall_1: 0.8177 - val_loss: 0.5834 - val_precision_1: 0.6970 - val_recall_1: 0.8632 - 2s/epoch - 3ms/step\n",
            "Epoch 88/100\n",
            "509/509 - 2s - loss: 0.5790 - precision_1: 0.7202 - recall_1: 0.8154 - val_loss: 0.5709 - val_precision_1: 0.7158 - val_recall_1: 0.8195 - 2s/epoch - 3ms/step\n",
            "Epoch 89/100\n",
            "509/509 - 1s - loss: 0.5783 - precision_1: 0.7218 - recall_1: 0.8105 - val_loss: 0.5767 - val_precision_1: 0.7128 - val_recall_1: 0.8256 - 1s/epoch - 3ms/step\n",
            "Epoch 90/100\n",
            "509/509 - 1s - loss: 0.5770 - precision_1: 0.7214 - recall_1: 0.8122 - val_loss: 0.5704 - val_precision_1: 0.7005 - val_recall_1: 0.8575 - 1s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "509/509 - 1s - loss: 0.5762 - precision_1: 0.7178 - recall_1: 0.8170 - val_loss: 0.5750 - val_precision_1: 0.6972 - val_recall_1: 0.8607 - 1s/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "509/509 - 1s - loss: 0.5780 - precision_1: 0.7197 - recall_1: 0.8156 - val_loss: 0.5711 - val_precision_1: 0.7057 - val_recall_1: 0.8439 - 1s/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "509/509 - 2s - loss: 0.5784 - precision_1: 0.7211 - recall_1: 0.8143 - val_loss: 0.5742 - val_precision_1: 0.7031 - val_recall_1: 0.8489 - 2s/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "509/509 - 1s - loss: 0.5768 - precision_1: 0.7205 - recall_1: 0.8167 - val_loss: 0.5712 - val_precision_1: 0.7114 - val_recall_1: 0.8314 - 1s/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "509/509 - 1s - loss: 0.5758 - precision_1: 0.7191 - recall_1: 0.8165 - val_loss: 0.5700 - val_precision_1: 0.7295 - val_recall_1: 0.7859 - 1s/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "509/509 - 1s - loss: 0.5738 - precision_1: 0.7222 - recall_1: 0.8127 - val_loss: 0.5680 - val_precision_1: 0.7097 - val_recall_1: 0.8314 - 1s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "509/509 - 1s - loss: 0.5723 - precision_1: 0.7223 - recall_1: 0.8122 - val_loss: 0.5680 - val_precision_1: 0.7192 - val_recall_1: 0.8124 - 1s/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "509/509 - 1s - loss: 0.5716 - precision_1: 0.7222 - recall_1: 0.8099 - val_loss: 0.5693 - val_precision_1: 0.7207 - val_recall_1: 0.8049 - 1s/epoch - 3ms/step\n",
            "Epoch 99/100\n",
            "509/509 - 2s - loss: 0.5729 - precision_1: 0.7217 - recall_1: 0.8121 - val_loss: 0.5658 - val_precision_1: 0.7113 - val_recall_1: 0.8317 - 2s/epoch - 3ms/step\n",
            "Epoch 100/100\n",
            "509/509 - 2s - loss: 0.5703 - precision_1: 0.7230 - recall_1: 0.8100 - val_loss: 0.5660 - val_precision_1: 0.7089 - val_recall_1: 0.8353 - 2s/epoch - 3ms/step\n",
            "442/442 [==============================] - 1s 2ms/step - loss: 0.5639 - precision_1: 0.7207 - recall_1: 0.8242\n",
            "binary cross-entropy loss :  0.5639325976371765  precision:  0.7207162976264954  recal:  0.8242390155792236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result\n",
        "As we saw, different models are not showing significant result improvements. So we may need to make some changes to our data"
      ],
      "metadata": {
        "id": "dAVWhISP03jL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The correlation matrix and its usage\n",
        "Correlation matrix simply explains the relationship between columns of a dataset. The correlation coefficient ranges between -1 and 1. A correlation coefficient of 1 indicates a perfect positive correlation, meaning that the two variables increase or decrease together in a linear relationship. A correlation coefficient of -1 indicates a perfect negative correlation, meaning that the two variables move in opposite directions in a linear relationship. A correlation coefficient close to 0 suggests no linear relationship between the variables.\n",
        "\n",
        "This matrix can be helpful when finding an optimal subset of features."
      ],
      "metadata": {
        "id": "xLjc4tO9DSgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_correlations(correlation_matrix):\n",
        "  return sorted(correlation_matrix.items(), key=lambda x:abs(x[1]))"
      ],
      "metadata": {
        "id": "-8WPCmWe4cF7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correlations(dataset):\n",
        "  columns = dataset.keys()\n",
        "  correlation = dataset[columns].corr()\n",
        "  return correlation[\"Diabetes_binary\"]"
      ],
      "metadata": {
        "id": "nAo1fc4AMXdy"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_map = sort_correlations(get_correlations(raw_dataset))"
      ],
      "metadata": {
        "id": "sxLDZpig1bFM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd1e-mUK2-IJ",
        "outputId": "c8ae5abd-e607-4f78-c7e5-d29c1acafc09"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('AnyHealthcare', 0.02319074853112824),\n",
              " ('NoDocbcCost', 0.040976573266643494),\n",
              " ('Sex', 0.044412858371260695),\n",
              " ('Fruits', -0.05407655628666651),\n",
              " ('Veggies', -0.07929314561269872),\n",
              " ('Smoker', 0.08599896420800192),\n",
              " ('MentHlth', 0.08702877147509416),\n",
              " ('HvyAlcoholConsump', -0.09485313995926549),\n",
              " ('CholCheck', 0.11538161710270915),\n",
              " ('Stroke', 0.12542678468516733),\n",
              " ('PhysActivity', -0.15866560486405157),\n",
              " ('Education', -0.17048063498806143),\n",
              " ('HeartDiseaseorAttack', 0.21152340436022687),\n",
              " ('PhysHlth', 0.21308101903810317),\n",
              " ('DiffWalk', 0.272646006159808),\n",
              " ('Age', 0.27873806628188813),\n",
              " ('HighChol', 0.28921280708865016),\n",
              " ('BMI', 0.29337274476103575),\n",
              " ('HighBP', 0.3815155489073117),\n",
              " ('GenHlth', 0.4076115984949182),\n",
              " ('Diabetes_binary', 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keep_features = map(lambda b: b[0], filter(lambda a: abs(a[1]) > 0.25, correlation_map))"
      ],
      "metadata": {
        "id": "mgYZgcME11WI"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_dataset = raw_dataset[list(keep_features)]"
      ],
      "metadata": {
        "id": "uGKhE_F72VFN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(modified_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ1ed4oV2rZH",
        "outputId": "32e3d1a4-ab8e-435a-f8d3-74164d4b6b60"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 38 epochs took 1 seconds\n",
            "[[0.0702186  0.08500237 0.29923042 0.03782045 0.39012718 0.30067847]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection result: Logistic regression\n",
        "By reducing the feature count using the correlation matrix and only keeping faetures that have more meaningful relationship with the target featurse, Logistic regression not only converged faster, it also kept its accuracy."
      ],
      "metadata": {
        "id": "-5gt8cFd4ZRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(modified_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMTIIVdU5Grg",
        "outputId": "53238e3b-8f3e-43a3-c468-235dc982d4e1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3619           0.0245            5.40s\n",
            "         2           1.3400           0.0224            4.98s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         3           1.3200           0.0203            4.92s\n",
            "         4           1.3008           0.0178            4.82s\n",
            "         5           1.2841           0.0163            4.73s\n",
            "         6           1.2685           0.0151            4.68s\n",
            "         7           1.2545           0.0136            4.68s\n",
            "         8           1.2417           0.0127            4.63s\n",
            "         9           1.2303           0.0117            4.58s\n",
            "        10           1.2191           0.0112            4.55s\n",
            "        11           1.2090           0.0101            4.60s\n",
            "        12           1.1982           0.0100            4.54s\n",
            "        13           1.1890           0.0094            4.55s\n",
            "        14           1.1793           0.0088            4.54s\n",
            "        15           1.1707           0.0078            4.49s\n",
            "        16           1.1641           0.0067            4.45s\n",
            "        17           1.1571           0.0076            4.42s\n",
            "        18           1.1502           0.0062            4.37s\n",
            "        19           1.1433           0.0058            4.32s\n",
            "        20           1.1379           0.0066            4.29s\n",
            "        21           1.1312           0.0059            4.25s\n",
            "        22           1.1278           0.0050            4.20s\n",
            "        23           1.1231           0.0050            4.18s\n",
            "        24           1.1165           0.0051            4.14s\n",
            "        25           1.1116           0.0041            4.12s\n",
            "        26           1.1086           0.0043            4.09s\n",
            "        27           1.1027           0.0044            4.05s\n",
            "        28           1.0970           0.0040            4.02s\n",
            "        29           1.0944           0.0038            3.98s\n",
            "        30           1.0900           0.0036            3.94s\n",
            "        31           1.0874           0.0029            3.91s\n",
            "        32           1.0857           0.0036            3.87s\n",
            "        33           1.0808           0.0026            3.83s\n",
            "        34           1.0781           0.0026            3.79s\n",
            "        35           1.0782           0.0031            3.75s\n",
            "        36           1.0720           0.0021            3.71s\n",
            "        37           1.0696           0.0023            3.68s\n",
            "        38           1.0695           0.0021            3.65s\n",
            "        39           1.0648           0.0022            3.61s\n",
            "        40           1.0644           0.0019            3.57s\n",
            "        41           1.0604           0.0020            3.55s\n",
            "        42           1.0601           0.0017            3.52s\n",
            "        43           1.0566           0.0014            3.48s\n",
            "        44           1.0571           0.0015            3.45s\n",
            "        45           1.0577           0.0017            3.43s\n",
            "        46           1.0524           0.0013            3.40s\n",
            "        47           1.0527           0.0014            3.36s\n",
            "        48           1.0483           0.0014            3.33s\n",
            "        49           1.0504           0.0012            3.30s\n",
            "        50           1.0480           0.0011            3.27s\n",
            "        51           1.0470           0.0012            3.24s\n",
            "        52           1.0462           0.0011            3.20s\n",
            "        53           1.0420           0.0010            3.16s\n",
            "        54           1.0439           0.0010            3.13s\n",
            "        55           1.0393           0.0007            3.11s\n",
            "        56           1.0415           0.0008            3.08s\n",
            "        57           1.0402           0.0007            3.04s\n",
            "        58           1.0404           0.0008            3.01s\n",
            "        59           1.0355           0.0007            2.98s\n",
            "        60           1.0376           0.0007            2.94s\n",
            "        61           1.0370           0.0007            2.91s\n",
            "        62           1.0347           0.0006            2.88s\n",
            "        63           1.0350           0.0006            2.84s\n",
            "        64           1.0356           0.0006            2.81s\n",
            "        65           1.0330           0.0005            2.78s\n",
            "        66           1.0351           0.0005            2.74s\n",
            "        67           1.0345           0.0005            2.71s\n",
            "        68           1.0309           0.0004            2.67s\n",
            "        69           1.0330           0.0005            2.64s\n",
            "        70           1.0284           0.0004            2.61s\n",
            "        71           1.0312           0.0004            2.58s\n",
            "        72           1.0286           0.0004            2.54s\n",
            "        73           1.0294           0.0004            2.51s\n",
            "        74           1.0274           0.0003            2.48s\n",
            "        75           1.0304           0.0003            2.45s\n",
            "        76           1.0274           0.0002            2.42s\n",
            "        77           1.0306           0.0004            2.39s\n",
            "        78           1.0249           0.0003            2.36s\n",
            "        79           1.0261           0.0003            2.32s\n",
            "        80           1.0262           0.0003            2.29s\n",
            "        81           1.0256           0.0003            2.25s\n",
            "        82           1.0238           0.0002            2.22s\n",
            "        83           1.0270           0.0002            2.19s\n",
            "        84           1.0269           0.0003            2.15s\n",
            "        85           1.0256           0.0001            2.12s\n",
            "        86           1.0239           0.0002            2.09s\n",
            "        87           1.0283           0.0002            2.05s\n",
            "        88           1.0240           0.0002            2.02s\n",
            "        89           1.0250           0.0002            1.98s\n",
            "        90           1.0180           0.0001            1.95s\n",
            "        91           1.0255           0.0003            1.92s\n",
            "        92           1.0225           0.0002            1.89s\n",
            "        93           1.0221           0.0001            1.86s\n",
            "        94           1.0214           0.0001            1.82s\n",
            "        95           1.0207           0.0002            1.79s\n",
            "        96           1.0190           0.0001            1.76s\n",
            "        97           1.0218           0.0001            1.73s\n",
            "        98           1.0213           0.0001            1.70s\n",
            "        99           1.0222           0.0001            1.66s\n",
            "       100           1.0190           0.0001            1.63s\n",
            "       101           1.0176           0.0001            1.59s\n",
            "       102           1.0208           0.0000            1.56s\n",
            "       103           1.0218           0.0001            1.53s\n",
            "       104           1.0241           0.0001            1.50s\n",
            "       105           1.0176           0.0000            1.46s\n",
            "       106           1.0163          -0.0000            1.43s\n",
            "       107           1.0198           0.0000            1.40s\n",
            "       108           1.0230           0.0001            1.37s\n",
            "       109           1.0202           0.0000            1.33s\n",
            "       110           1.0191           0.0000            1.30s\n",
            "       111           1.0228           0.0000            1.28s\n",
            "       112           1.0212           0.0000            1.25s\n",
            "       113           1.0165           0.0000            1.22s\n",
            "       114           1.0194           0.0000            1.19s\n",
            "       115           1.0215           0.0001            1.16s\n",
            "       116           1.0212           0.0000            1.13s\n",
            "       117           1.0174          -0.0000            1.10s\n",
            "       118           1.0188          -0.0000            1.07s\n",
            "       119           1.0187           0.0000            1.04s\n",
            "       120           1.0172           0.0000            1.01s\n",
            "       121           1.0195           0.0000            0.98s\n",
            "       122           1.0155          -0.0000            0.95s\n",
            "       123           1.0172           0.0000            0.92s\n",
            "       124           1.0196           0.0000            0.89s\n",
            "       125           1.0133          -0.0000            0.86s\n",
            "       126           1.0162          -0.0000            0.83s\n",
            "       127           1.0184           0.0000            0.80s\n",
            "       128           1.0189          -0.0000            0.76s\n",
            "       129           1.0192           0.0000            0.73s\n",
            "       130           1.0197           0.0000            0.70s\n",
            "       131           1.0183          -0.0000            0.67s\n",
            "       132           1.0172          -0.0000            0.63s\n",
            "       133           1.0165          -0.0000            0.60s\n",
            "       134           1.0183          -0.0000            0.57s\n",
            "       135           1.0182          -0.0000            0.53s\n",
            "       136           1.0159          -0.0000            0.50s\n",
            "       137           1.0182           0.0000            0.46s\n",
            "       138           1.0194           0.0000            0.43s\n",
            "       139           1.0150          -0.0001            0.39s\n",
            "       140           1.0194           0.0000            0.36s\n",
            "       141           1.0140          -0.0000            0.32s\n",
            "       142           1.0169          -0.0000            0.29s\n",
            "       143           1.0190          -0.0000            0.25s\n",
            "       144           1.0157          -0.0000            0.22s\n",
            "       145           1.0176          -0.0000            0.18s\n",
            "       146           1.0167          -0.0000            0.15s\n",
            "       147           1.0149          -0.0000            0.11s\n",
            "       148           1.0178          -0.0000            0.07s\n",
            "       149           1.0178          -0.0000            0.04s\n",
            "       150           1.0166          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.70      0.73      7010\n",
            "         1.0       0.73      0.78      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection result: GradientBoost\n",
        "GradientBoost was also capable of keeping its performance after feature selection. It is worthy of noting that tuning hyperparameters had a mild effect on this model but it was negligible."
      ],
      "metadata": {
        "id": "lLWu9esz6ZYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model(modified_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK9KPK-l6Y_S",
        "outputId": "69e694c4-bb54-42e4-b8dc-f864aa1b190b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "509/509 - 3s - loss: 3.0580 - precision_2: 0.5315 - recall_2: 0.4983 - val_loss: 1.4246 - val_precision_2: 0.5755 - val_recall_2: 0.9180 - 3s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "509/509 - 1s - loss: 1.3094 - precision_2: 0.5808 - recall_2: 0.6524 - val_loss: 1.2035 - val_precision_2: 0.7547 - val_recall_2: 0.5861 - 1s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "509/509 - 1s - loss: 1.1172 - precision_2: 0.6209 - recall_2: 0.6806 - val_loss: 1.0288 - val_precision_2: 0.7063 - val_recall_2: 0.7809 - 1s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "509/509 - 1s - loss: 1.0026 - precision_2: 0.6494 - recall_2: 0.6794 - val_loss: 0.9491 - val_precision_2: 0.7062 - val_recall_2: 0.8056 - 1s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "509/509 - 1s - loss: 0.9300 - precision_2: 0.6736 - recall_2: 0.6876 - val_loss: 0.8605 - val_precision_2: 0.6928 - val_recall_2: 0.8407 - 1s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "509/509 - 1s - loss: 0.8742 - precision_2: 0.6897 - recall_2: 0.6908 - val_loss: 0.8314 - val_precision_2: 0.7063 - val_recall_2: 0.8231 - 1s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "509/509 - 1s - loss: 0.8335 - precision_2: 0.7003 - recall_2: 0.6934 - val_loss: 0.7817 - val_precision_2: 0.6991 - val_recall_2: 0.8360 - 1s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "509/509 - 1s - loss: 0.8069 - precision_2: 0.7045 - recall_2: 0.6978 - val_loss: 0.7777 - val_precision_2: 0.6855 - val_recall_2: 0.8654 - 1s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "509/509 - 1s - loss: 0.7836 - precision_2: 0.7047 - recall_2: 0.7019 - val_loss: 0.7347 - val_precision_2: 0.7151 - val_recall_2: 0.7898 - 1s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "509/509 - 1s - loss: 0.7694 - precision_2: 0.7110 - recall_2: 0.6981 - val_loss: 0.7352 - val_precision_2: 0.7333 - val_recall_2: 0.7451 - 1s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "509/509 - 2s - loss: 0.7527 - precision_2: 0.7109 - recall_2: 0.7017 - val_loss: 0.7094 - val_precision_2: 0.7139 - val_recall_2: 0.7952 - 2s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "509/509 - 2s - loss: 0.7420 - precision_2: 0.7139 - recall_2: 0.7068 - val_loss: 0.7154 - val_precision_2: 0.7426 - val_recall_2: 0.7282 - 2s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "509/509 - 1s - loss: 0.7321 - precision_2: 0.7148 - recall_2: 0.7084 - val_loss: 0.6922 - val_precision_2: 0.7440 - val_recall_2: 0.7254 - 1s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "509/509 - 1s - loss: 0.7201 - precision_2: 0.7128 - recall_2: 0.7091 - val_loss: 0.6913 - val_precision_2: 0.7271 - val_recall_2: 0.7698 - 1s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "509/509 - 1s - loss: 0.7147 - precision_2: 0.7143 - recall_2: 0.7155 - val_loss: 0.6790 - val_precision_2: 0.7480 - val_recall_2: 0.7164 - 1s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "509/509 - 1s - loss: 0.7071 - precision_2: 0.7140 - recall_2: 0.7131 - val_loss: 0.6753 - val_precision_2: 0.7302 - val_recall_2: 0.7540 - 1s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "509/509 - 1s - loss: 0.6998 - precision_2: 0.7156 - recall_2: 0.7126 - val_loss: 0.6663 - val_precision_2: 0.7058 - val_recall_2: 0.8142 - 1s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "509/509 - 1s - loss: 0.6933 - precision_2: 0.7176 - recall_2: 0.7176 - val_loss: 0.6739 - val_precision_2: 0.7524 - val_recall_2: 0.6942 - 1s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "509/509 - 1s - loss: 0.6924 - precision_2: 0.7126 - recall_2: 0.7151 - val_loss: 0.6542 - val_precision_2: 0.7331 - val_recall_2: 0.7494 - 1s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "509/509 - 1s - loss: 0.6854 - precision_2: 0.7131 - recall_2: 0.7168 - val_loss: 0.6617 - val_precision_2: 0.6979 - val_recall_2: 0.8314 - 1s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "509/509 - 1s - loss: 0.6776 - precision_2: 0.7141 - recall_2: 0.7200 - val_loss: 0.6421 - val_precision_2: 0.7241 - val_recall_2: 0.7705 - 1s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "509/509 - 1s - loss: 0.6718 - precision_2: 0.7176 - recall_2: 0.7236 - val_loss: 0.6482 - val_precision_2: 0.7181 - val_recall_2: 0.7845 - 1s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "509/509 - 2s - loss: 0.6689 - precision_2: 0.7145 - recall_2: 0.7194 - val_loss: 0.6353 - val_precision_2: 0.7189 - val_recall_2: 0.7820 - 2s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "509/509 - 2s - loss: 0.6654 - precision_2: 0.7188 - recall_2: 0.7224 - val_loss: 0.6549 - val_precision_2: 0.6826 - val_recall_2: 0.8614 - 2s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "509/509 - 1s - loss: 0.6624 - precision_2: 0.7159 - recall_2: 0.7259 - val_loss: 0.6279 - val_precision_2: 0.7250 - val_recall_2: 0.7712 - 1s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "509/509 - 1s - loss: 0.6588 - precision_2: 0.7152 - recall_2: 0.7191 - val_loss: 0.6347 - val_precision_2: 0.7249 - val_recall_2: 0.7716 - 1s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "509/509 - 1s - loss: 0.6557 - precision_2: 0.7177 - recall_2: 0.7231 - val_loss: 0.6244 - val_precision_2: 0.7358 - val_recall_2: 0.7458 - 1s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "509/509 - 1s - loss: 0.6511 - precision_2: 0.7171 - recall_2: 0.7233 - val_loss: 0.6296 - val_precision_2: 0.7084 - val_recall_2: 0.8081 - 1s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "509/509 - 1s - loss: 0.6475 - precision_2: 0.7177 - recall_2: 0.7250 - val_loss: 0.6160 - val_precision_2: 0.7247 - val_recall_2: 0.7680 - 1s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "509/509 - 1s - loss: 0.6444 - precision_2: 0.7173 - recall_2: 0.7262 - val_loss: 0.6223 - val_precision_2: 0.7100 - val_recall_2: 0.8056 - 1s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "509/509 - 1s - loss: 0.6413 - precision_2: 0.7182 - recall_2: 0.7271 - val_loss: 0.6109 - val_precision_2: 0.7282 - val_recall_2: 0.7608 - 1s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "509/509 - 1s - loss: 0.6374 - precision_2: 0.7181 - recall_2: 0.7296 - val_loss: 0.6181 - val_precision_2: 0.7086 - val_recall_2: 0.8038 - 1s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "509/509 - 1s - loss: 0.6373 - precision_2: 0.7164 - recall_2: 0.7259 - val_loss: 0.6056 - val_precision_2: 0.7141 - val_recall_2: 0.7916 - 1s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "509/509 - 1s - loss: 0.6339 - precision_2: 0.7186 - recall_2: 0.7312 - val_loss: 0.6149 - val_precision_2: 0.7054 - val_recall_2: 0.8163 - 1s/epoch - 3ms/step\n",
            "Epoch 35/100\n",
            "509/509 - 2s - loss: 0.6304 - precision_2: 0.7183 - recall_2: 0.7297 - val_loss: 0.6048 - val_precision_2: 0.7462 - val_recall_2: 0.7200 - 2s/epoch - 3ms/step\n",
            "Epoch 36/100\n",
            "509/509 - 2s - loss: 0.6302 - precision_2: 0.7173 - recall_2: 0.7290 - val_loss: 0.6117 - val_precision_2: 0.7430 - val_recall_2: 0.7265 - 2s/epoch - 3ms/step\n",
            "Epoch 37/100\n",
            "509/509 - 1s - loss: 0.6303 - precision_2: 0.7174 - recall_2: 0.7272 - val_loss: 0.5991 - val_precision_2: 0.7269 - val_recall_2: 0.7615 - 1s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "509/509 - 1s - loss: 0.6276 - precision_2: 0.7167 - recall_2: 0.7271 - val_loss: 0.6050 - val_precision_2: 0.7172 - val_recall_2: 0.7891 - 1s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "509/509 - 1s - loss: 0.6269 - precision_2: 0.7171 - recall_2: 0.7262 - val_loss: 0.5966 - val_precision_2: 0.7144 - val_recall_2: 0.7963 - 1s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "509/509 - 1s - loss: 0.6237 - precision_2: 0.7174 - recall_2: 0.7324 - val_loss: 0.6035 - val_precision_2: 0.7045 - val_recall_2: 0.8238 - 1s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "509/509 - 1s - loss: 0.6220 - precision_2: 0.7173 - recall_2: 0.7301 - val_loss: 0.5941 - val_precision_2: 0.7095 - val_recall_2: 0.8070 - 1s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "509/509 - 1s - loss: 0.6185 - precision_2: 0.7169 - recall_2: 0.7354 - val_loss: 0.5984 - val_precision_2: 0.7177 - val_recall_2: 0.7891 - 1s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "509/509 - 1s - loss: 0.6174 - precision_2: 0.7185 - recall_2: 0.7352 - val_loss: 0.5886 - val_precision_2: 0.7176 - val_recall_2: 0.7888 - 1s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "509/509 - 1s - loss: 0.6155 - precision_2: 0.7186 - recall_2: 0.7328 - val_loss: 0.5952 - val_precision_2: 0.7072 - val_recall_2: 0.8070 - 1s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "509/509 - 1s - loss: 0.6149 - precision_2: 0.7178 - recall_2: 0.7341 - val_loss: 0.5855 - val_precision_2: 0.7205 - val_recall_2: 0.7863 - 1s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "509/509 - 2s - loss: 0.6132 - precision_2: 0.7169 - recall_2: 0.7326 - val_loss: 0.5931 - val_precision_2: 0.7345 - val_recall_2: 0.7487 - 2s/epoch - 3ms/step\n",
            "Epoch 47/100\n",
            "509/509 - 2s - loss: 0.6085 - precision_2: 0.7206 - recall_2: 0.7358 - val_loss: 0.5822 - val_precision_2: 0.7140 - val_recall_2: 0.7963 - 2s/epoch - 3ms/step\n",
            "Epoch 48/100\n",
            "509/509 - 2s - loss: 0.6104 - precision_2: 0.7172 - recall_2: 0.7320 - val_loss: 0.5872 - val_precision_2: 0.7151 - val_recall_2: 0.7963 - 2s/epoch - 3ms/step\n",
            "Epoch 49/100\n",
            "509/509 - 1s - loss: 0.6058 - precision_2: 0.7188 - recall_2: 0.7362 - val_loss: 0.5802 - val_precision_2: 0.7082 - val_recall_2: 0.8099 - 1s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "509/509 - 1s - loss: 0.6070 - precision_2: 0.7194 - recall_2: 0.7366 - val_loss: 0.5869 - val_precision_2: 0.7225 - val_recall_2: 0.7748 - 1s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "509/509 - 1s - loss: 0.6067 - precision_2: 0.7188 - recall_2: 0.7368 - val_loss: 0.5805 - val_precision_2: 0.7139 - val_recall_2: 0.8006 - 1s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "509/509 - 1s - loss: 0.6062 - precision_2: 0.7181 - recall_2: 0.7348 - val_loss: 0.5853 - val_precision_2: 0.7071 - val_recall_2: 0.8081 - 1s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "509/509 - 1s - loss: 0.6054 - precision_2: 0.7187 - recall_2: 0.7362 - val_loss: 0.5792 - val_precision_2: 0.7281 - val_recall_2: 0.7601 - 1s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "509/509 - 1s - loss: 0.6020 - precision_2: 0.7188 - recall_2: 0.7342 - val_loss: 0.5816 - val_precision_2: 0.7150 - val_recall_2: 0.7984 - 1s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "509/509 - 1s - loss: 0.6014 - precision_2: 0.7202 - recall_2: 0.7399 - val_loss: 0.5763 - val_precision_2: 0.7208 - val_recall_2: 0.7848 - 1s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "509/509 - 1s - loss: 0.6007 - precision_2: 0.7185 - recall_2: 0.7373 - val_loss: 0.5804 - val_precision_2: 0.7145 - val_recall_2: 0.7974 - 1s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "509/509 - 1s - loss: 0.5999 - precision_2: 0.7188 - recall_2: 0.7391 - val_loss: 0.5741 - val_precision_2: 0.7192 - val_recall_2: 0.7923 - 1s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "509/509 - 2s - loss: 0.5992 - precision_2: 0.7189 - recall_2: 0.7361 - val_loss: 0.5773 - val_precision_2: 0.7184 - val_recall_2: 0.7891 - 2s/epoch - 3ms/step\n",
            "Epoch 59/100\n",
            "509/509 - 2s - loss: 0.5976 - precision_2: 0.7180 - recall_2: 0.7361 - val_loss: 0.5738 - val_precision_2: 0.7033 - val_recall_2: 0.8188 - 2s/epoch - 3ms/step\n",
            "Epoch 60/100\n",
            "509/509 - 1s - loss: 0.5966 - precision_2: 0.7192 - recall_2: 0.7395 - val_loss: 0.5763 - val_precision_2: 0.7110 - val_recall_2: 0.8070 - 1s/epoch - 3ms/step\n",
            "Epoch 61/100\n",
            "509/509 - 1s - loss: 0.5946 - precision_2: 0.7192 - recall_2: 0.7418 - val_loss: 0.5698 - val_precision_2: 0.7149 - val_recall_2: 0.7981 - 1s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "509/509 - 1s - loss: 0.5949 - precision_2: 0.7196 - recall_2: 0.7395 - val_loss: 0.5735 - val_precision_2: 0.7182 - val_recall_2: 0.7884 - 1s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "509/509 - 1s - loss: 0.5944 - precision_2: 0.7202 - recall_2: 0.7393 - val_loss: 0.5677 - val_precision_2: 0.7105 - val_recall_2: 0.8074 - 1s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "509/509 - 1s - loss: 0.5919 - precision_2: 0.7199 - recall_2: 0.7413 - val_loss: 0.5740 - val_precision_2: 0.7097 - val_recall_2: 0.8106 - 1s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "509/509 - 1s - loss: 0.5920 - precision_2: 0.7192 - recall_2: 0.7404 - val_loss: 0.5668 - val_precision_2: 0.7104 - val_recall_2: 0.8052 - 1s/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "509/509 - 1s - loss: 0.5899 - precision_2: 0.7201 - recall_2: 0.7408 - val_loss: 0.5695 - val_precision_2: 0.7193 - val_recall_2: 0.7873 - 1s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "509/509 - 1s - loss: 0.5871 - precision_2: 0.7206 - recall_2: 0.7435 - val_loss: 0.5646 - val_precision_2: 0.7210 - val_recall_2: 0.7855 - 1s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "509/509 - 1s - loss: 0.5875 - precision_2: 0.7186 - recall_2: 0.7426 - val_loss: 0.5715 - val_precision_2: 0.7028 - val_recall_2: 0.8195 - 1s/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "509/509 - 1s - loss: 0.5878 - precision_2: 0.7203 - recall_2: 0.7458 - val_loss: 0.5645 - val_precision_2: 0.7293 - val_recall_2: 0.7562 - 1s/epoch - 3ms/step\n",
            "Epoch 70/100\n",
            "509/509 - 2s - loss: 0.5867 - precision_2: 0.7193 - recall_2: 0.7446 - val_loss: 0.5673 - val_precision_2: 0.7222 - val_recall_2: 0.7837 - 2s/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "509/509 - 2s - loss: 0.5870 - precision_2: 0.7214 - recall_2: 0.7429 - val_loss: 0.5621 - val_precision_2: 0.7094 - val_recall_2: 0.8110 - 2s/epoch - 3ms/step\n",
            "Epoch 72/100\n",
            "509/509 - 1s - loss: 0.5862 - precision_2: 0.7191 - recall_2: 0.7427 - val_loss: 0.5672 - val_precision_2: 0.7046 - val_recall_2: 0.8145 - 1s/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "509/509 - 1s - loss: 0.5836 - precision_2: 0.7234 - recall_2: 0.7450 - val_loss: 0.5606 - val_precision_2: 0.7279 - val_recall_2: 0.7662 - 1s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "509/509 - 1s - loss: 0.5835 - precision_2: 0.7209 - recall_2: 0.7427 - val_loss: 0.5641 - val_precision_2: 0.7188 - val_recall_2: 0.7916 - 1s/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "509/509 - 1s - loss: 0.5834 - precision_2: 0.7226 - recall_2: 0.7457 - val_loss: 0.5594 - val_precision_2: 0.7237 - val_recall_2: 0.7794 - 1s/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "509/509 - 1s - loss: 0.5809 - precision_2: 0.7208 - recall_2: 0.7496 - val_loss: 0.5623 - val_precision_2: 0.7242 - val_recall_2: 0.7784 - 1s/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "509/509 - 1s - loss: 0.5804 - precision_2: 0.7211 - recall_2: 0.7472 - val_loss: 0.5590 - val_precision_2: 0.7280 - val_recall_2: 0.7666 - 1s/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "509/509 - 1s - loss: 0.5800 - precision_2: 0.7216 - recall_2: 0.7465 - val_loss: 0.5605 - val_precision_2: 0.7209 - val_recall_2: 0.7859 - 1s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "509/509 - 1s - loss: 0.5795 - precision_2: 0.7216 - recall_2: 0.7468 - val_loss: 0.5575 - val_precision_2: 0.7189 - val_recall_2: 0.7913 - 1s/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "509/509 - 1s - loss: 0.5785 - precision_2: 0.7214 - recall_2: 0.7461 - val_loss: 0.5591 - val_precision_2: 0.7118 - val_recall_2: 0.8038 - 1s/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "509/509 - 2s - loss: 0.5781 - precision_2: 0.7212 - recall_2: 0.7491 - val_loss: 0.5569 - val_precision_2: 0.7121 - val_recall_2: 0.8031 - 2s/epoch - 3ms/step\n",
            "Epoch 82/100\n",
            "509/509 - 2s - loss: 0.5791 - precision_2: 0.7201 - recall_2: 0.7450 - val_loss: 0.5573 - val_precision_2: 0.7133 - val_recall_2: 0.8034 - 2s/epoch - 3ms/step\n",
            "Epoch 83/100\n",
            "509/509 - 2s - loss: 0.5771 - precision_2: 0.7206 - recall_2: 0.7468 - val_loss: 0.5559 - val_precision_2: 0.7192 - val_recall_2: 0.7895 - 2s/epoch - 3ms/step\n",
            "Epoch 84/100\n",
            "509/509 - 1s - loss: 0.5763 - precision_2: 0.7217 - recall_2: 0.7504 - val_loss: 0.5568 - val_precision_2: 0.7169 - val_recall_2: 0.7952 - 1s/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "509/509 - 1s - loss: 0.5765 - precision_2: 0.7182 - recall_2: 0.7452 - val_loss: 0.5554 - val_precision_2: 0.7240 - val_recall_2: 0.7787 - 1s/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "509/509 - 1s - loss: 0.5766 - precision_2: 0.7189 - recall_2: 0.7460 - val_loss: 0.5565 - val_precision_2: 0.7147 - val_recall_2: 0.8002 - 1s/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "509/509 - 1s - loss: 0.5740 - precision_2: 0.7217 - recall_2: 0.7493 - val_loss: 0.5537 - val_precision_2: 0.7203 - val_recall_2: 0.7884 - 1s/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "509/509 - 1s - loss: 0.5735 - precision_2: 0.7208 - recall_2: 0.7496 - val_loss: 0.5558 - val_precision_2: 0.7124 - val_recall_2: 0.8045 - 1s/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "509/509 - 1s - loss: 0.5740 - precision_2: 0.7205 - recall_2: 0.7502 - val_loss: 0.5528 - val_precision_2: 0.7147 - val_recall_2: 0.7974 - 1s/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "509/509 - 1s - loss: 0.5735 - precision_2: 0.7209 - recall_2: 0.7483 - val_loss: 0.5577 - val_precision_2: 0.7040 - val_recall_2: 0.8278 - 1s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "509/509 - 1s - loss: 0.5742 - precision_2: 0.7200 - recall_2: 0.7510 - val_loss: 0.5520 - val_precision_2: 0.7236 - val_recall_2: 0.7816 - 1s/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "509/509 - 1s - loss: 0.5732 - precision_2: 0.7212 - recall_2: 0.7493 - val_loss: 0.5552 - val_precision_2: 0.7275 - val_recall_2: 0.7666 - 1s/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "509/509 - 2s - loss: 0.5716 - precision_2: 0.7225 - recall_2: 0.7505 - val_loss: 0.5513 - val_precision_2: 0.7180 - val_recall_2: 0.7884 - 2s/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "509/509 - 2s - loss: 0.5711 - precision_2: 0.7218 - recall_2: 0.7534 - val_loss: 0.5540 - val_precision_2: 0.7287 - val_recall_2: 0.7637 - 2s/epoch - 3ms/step\n",
            "Epoch 95/100\n",
            "509/509 - 2s - loss: 0.5703 - precision_2: 0.7232 - recall_2: 0.7531 - val_loss: 0.5520 - val_precision_2: 0.7075 - val_recall_2: 0.8131 - 2s/epoch - 3ms/step\n",
            "Epoch 96/100\n",
            "509/509 - 1s - loss: 0.5699 - precision_2: 0.7230 - recall_2: 0.7550 - val_loss: 0.5527 - val_precision_2: 0.7108 - val_recall_2: 0.8077 - 1s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "509/509 - 1s - loss: 0.5688 - precision_2: 0.7223 - recall_2: 0.7557 - val_loss: 0.5511 - val_precision_2: 0.7271 - val_recall_2: 0.7651 - 1s/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "509/509 - 1s - loss: 0.5692 - precision_2: 0.7217 - recall_2: 0.7525 - val_loss: 0.5518 - val_precision_2: 0.7247 - val_recall_2: 0.7719 - 1s/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "509/509 - 1s - loss: 0.5691 - precision_2: 0.7219 - recall_2: 0.7498 - val_loss: 0.5519 - val_precision_2: 0.7043 - val_recall_2: 0.8246 - 1s/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "509/509 - 1s - loss: 0.5677 - precision_2: 0.7214 - recall_2: 0.7555 - val_loss: 0.5513 - val_precision_2: 0.7143 - val_recall_2: 0.7984 - 1s/epoch - 2ms/step\n",
            "442/442 [==============================] - 1s 2ms/step - loss: 0.5503 - precision_2: 0.7234 - recall_2: 0.7871\n",
            "binary cross-entropy loss :  0.5503036975860596  precision:  0.7234399318695068  recal:  0.7870669364929199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection result: DNN\n",
        "DNN also proved to be consistant after feature selection. It even had a mild improvement(which is again, negligible)."
      ],
      "metadata": {
        "id": "tLuJEkwX8gUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection overall result\n",
        "So to conclude, we were able to predict our target feature with an acceptable accuracy even after selecting a subsest of our features. The following is the list of remaining features which proved to be decisive: DiffWalk, Age, HighChol, BMI, HighBP, GenHlth\n"
      ],
      "metadata": {
        "id": "HDGtLOsC88QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rename functions to snake_case"
      ],
      "metadata": {
        "id": "4DSyQcxh8f66"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_name(count):\n",
        "  return [\"f{c}\".format(c=i) for i in range(count)]"
      ],
      "metadata": {
        "id": "nO_T_h6MCctd"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "component_count = 15\n",
        "\n",
        "all_features = list(raw_dataset.keys())\n",
        "\n",
        "all_features.remove(\"Diabetes_binary\")\n",
        "\n",
        "pca = PCA(n_components = component_count)\n",
        "\n",
        "pca_columns = pca.fit_transform(standardized_dataset[all_features])"
      ],
      "metadata": {
        "id": "I81sh3xB-I2m"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_dataset = pd.DataFrame()\n",
        "pca_dataset[\"Diabetes_binary\"] = standardized_dataset[\"Diabetes_binary\"]\n",
        "pca_dataset[get_feature_name(component_count)] =  pca_columns"
      ],
      "metadata": {
        "id": "7CTxHp3tAP8D"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model(pca_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKYBh6qNB3FH",
        "outputId": "44909ec9-f2bd-465c-c3ab-0b21bda867b3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 35 epochs took 1 seconds\n",
            "[[ 0.34523619 -0.2000573   0.0320172  -0.12004041  0.15654393  0.13973331\n",
            "  -0.01753243  0.02543494 -0.02066987  0.03206145  0.02802083  0.05011259\n",
            "  -0.00294837  0.02060696 -0.00771329]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boost_classifier_model(pca_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2dG5eNRDbkg",
        "outputId": "0081df04-a663-43c1-bc7a-dc6fa3ab4470"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         1           1.3608           0.0256           49.52s\n",
            "         2           1.3377           0.0230           49.12s\n",
            "         3           1.3170           0.0212           48.21s\n",
            "         4           1.2966           0.0187           47.60s\n",
            "         5           1.2799           0.0176           47.13s\n",
            "         6           1.2633           0.0157           46.84s\n",
            "         7           1.2483           0.0145           46.40s\n",
            "         8           1.2349           0.0135           46.00s\n",
            "         9           1.2225           0.0127           45.82s\n",
            "        10           1.2112           0.0114           45.52s\n",
            "        11           1.1991           0.0105           45.09s\n",
            "        12           1.1910           0.0099           44.77s\n",
            "        13           1.1813           0.0091           44.37s\n",
            "        14           1.1718           0.0081           44.00s\n",
            "        15           1.1634           0.0076           43.68s\n",
            "        16           1.1561           0.0073           43.32s\n",
            "        17           1.1499           0.0065           42.98s\n",
            "        18           1.1447           0.0063           42.70s\n",
            "        19           1.1379           0.0057           42.36s\n",
            "        20           1.1321           0.0050           42.01s\n",
            "        21           1.1268           0.0050           41.70s\n",
            "        22           1.1234           0.0045           41.57s\n",
            "        23           1.1211           0.0042           41.94s\n",
            "        24           1.1148           0.0039           42.41s\n",
            "        25           1.1108           0.0036           42.71s\n",
            "        26           1.1072           0.0034           43.02s\n",
            "        27           1.1033           0.0034           43.18s\n",
            "        28           1.0982           0.0028           43.31s\n",
            "        29           1.0954           0.0028           43.36s\n",
            "        30           1.0953           0.0025           43.37s\n",
            "        31           1.0916           0.0023           43.40s\n",
            "        32           1.0903           0.0024           42.91s\n",
            "        33           1.0852           0.0019           42.41s\n",
            "        34           1.0837           0.0021           41.93s\n",
            "        35           1.0853           0.0019           41.41s\n",
            "        36           1.0794           0.0015           40.93s\n",
            "        37           1.0778           0.0016           40.48s\n",
            "        38           1.0788           0.0018           40.00s\n",
            "        39           1.0758           0.0015           39.58s\n",
            "        40           1.0737           0.0015           39.13s\n",
            "        41           1.0713           0.0011           38.69s\n",
            "        42           1.0714           0.0013           38.22s\n",
            "        43           1.0680           0.0011           37.80s\n",
            "        44           1.0682           0.0011           37.37s\n",
            "        45           1.0687           0.0012           36.92s\n",
            "        46           1.0644           0.0009           36.62s\n",
            "        47           1.0649           0.0011           36.23s\n",
            "        48           1.0611           0.0010           35.82s\n",
            "        49           1.0621           0.0009           35.42s\n",
            "        50           1.0631           0.0010           35.01s\n",
            "        51           1.0596           0.0008           34.59s\n",
            "        52           1.0595           0.0009           34.20s\n",
            "        53           1.0551           0.0006           33.80s\n",
            "        54           1.0576           0.0007           33.47s\n",
            "        55           1.0530           0.0006           33.07s\n",
            "        56           1.0552           0.0007           32.69s\n",
            "        57           1.0547           0.0005           32.30s\n",
            "        58           1.0539           0.0005           31.91s\n",
            "        59           1.0499           0.0006           31.54s\n",
            "        60           1.0520           0.0007           31.16s\n",
            "        61           1.0517           0.0006           30.78s\n",
            "        62           1.0492           0.0005           30.51s\n",
            "        63           1.0508           0.0006           30.35s\n",
            "        64           1.0510           0.0004           30.19s\n",
            "        65           1.0484           0.0005           29.98s\n",
            "        66           1.0497           0.0005           29.77s\n",
            "        67           1.0485           0.0005           29.54s\n",
            "        68           1.0445           0.0004           29.33s\n",
            "        69           1.0464           0.0004           29.10s\n",
            "        70           1.0438           0.0003           28.87s\n",
            "        71           1.0455           0.0002           28.61s\n",
            "        72           1.0439           0.0004           28.21s\n",
            "        73           1.0433           0.0005           27.80s\n",
            "        74           1.0423           0.0003           27.39s\n",
            "        75           1.0430           0.0004           27.00s\n",
            "        76           1.0402           0.0002           26.61s\n",
            "        77           1.0448           0.0002           26.23s\n",
            "        78           1.0386           0.0003           25.84s\n",
            "        79           1.0393           0.0003           25.45s\n",
            "        80           1.0397           0.0002           25.06s\n",
            "        81           1.0387           0.0002           24.68s\n",
            "        82           1.0390           0.0003           24.30s\n",
            "        83           1.0402           0.0002           23.91s\n",
            "        84           1.0407           0.0002           23.53s\n",
            "        85           1.0378           0.0003           23.15s\n",
            "        86           1.0370           0.0003           22.77s\n",
            "        87           1.0400           0.0002           22.40s\n",
            "        88           1.0374           0.0004           22.03s\n",
            "        89           1.0379           0.0003           21.65s\n",
            "        90           1.0308           0.0003           21.28s\n",
            "        91           1.0392           0.0001           20.90s\n",
            "        92           1.0353           0.0002           20.53s\n",
            "        93           1.0347           0.0002           20.16s\n",
            "        94           1.0337           0.0003           19.79s\n",
            "        95           1.0331           0.0001           19.42s\n",
            "        96           1.0313           0.0001           19.05s\n",
            "        97           1.0325           0.0002           18.68s\n",
            "        98           1.0332           0.0001           18.32s\n",
            "        99           1.0340           0.0002           17.95s\n",
            "       100           1.0288           0.0002           17.59s\n",
            "       101           1.0288           0.0002           17.22s\n",
            "       102           1.0315           0.0001           16.88s\n",
            "       103           1.0315           0.0000           16.58s\n",
            "       104           1.0336           0.0002           16.28s\n",
            "       105           1.0288           0.0001           15.98s\n",
            "       106           1.0258           0.0001           15.67s\n",
            "       107           1.0294           0.0000           15.36s\n",
            "       108           1.0339           0.0001           15.05s\n",
            "       109           1.0296           0.0001           14.73s\n",
            "       110           1.0293           0.0001           14.41s\n",
            "       111           1.0312           0.0002           14.09s\n",
            "       112           1.0308           0.0001           13.71s\n",
            "       113           1.0256           0.0001           13.34s\n",
            "       114           1.0277           0.0000           12.97s\n",
            "       115           1.0300           0.0001           12.60s\n",
            "       116           1.0295           0.0001           12.23s\n",
            "       117           1.0262           0.0001           11.87s\n",
            "       118           1.0269           0.0001           11.50s\n",
            "       119           1.0270           0.0002           11.13s\n",
            "       120           1.0270          -0.0000           10.77s\n",
            "       121           1.0274           0.0001           10.40s\n",
            "       122           1.0230           0.0002           10.03s\n",
            "       123           1.0253           0.0000            9.67s\n",
            "       124           1.0276           0.0001            9.30s\n",
            "       125           1.0214           0.0001            8.94s\n",
            "       126           1.0244           0.0001            8.57s\n",
            "       127           1.0238           0.0001            8.21s\n",
            "       128           1.0259           0.0001            7.84s\n",
            "       129           1.0249           0.0001            7.48s\n",
            "       130           1.0262          -0.0000            7.12s\n",
            "       131           1.0255           0.0000            6.76s\n",
            "       132           1.0235           0.0001            6.40s\n",
            "       133           1.0220           0.0000            6.04s\n",
            "       134           1.0225           0.0001            5.68s\n",
            "       135           1.0231           0.0000            5.32s\n",
            "       136           1.0212          -0.0000            4.96s\n",
            "       137           1.0211           0.0000            4.61s\n",
            "       138           1.0228           0.0000            4.25s\n",
            "       139           1.0217          -0.0000            3.89s\n",
            "       140           1.0242           0.0000            3.54s\n",
            "       141           1.0191          -0.0000            3.18s\n",
            "       142           1.0222          -0.0000            2.83s\n",
            "       143           1.0220           0.0000            2.48s\n",
            "       144           1.0207          -0.0001            2.13s\n",
            "       145           1.0216           0.0000            1.78s\n",
            "       146           1.0204           0.0000            1.43s\n",
            "       147           1.0209           0.0000            1.07s\n",
            "       148           1.0208           0.0001            0.72s\n",
            "       149           1.0213          -0.0000            0.36s\n",
            "       150           1.0202           0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.69      0.73      7010\n",
            "         1.0       0.72      0.80      0.76      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.75      0.74      0.74     14139\n",
            "weighted avg       0.75      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model(pca_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKDs0BosD2bD",
        "outputId": "14a77e3d-2814-4166-8b9f-58149df27dda"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "509/509 - 2s - loss: 2.3796 - precision_3: 0.6652 - recall_3: 0.7258 - val_loss: 1.4643 - val_precision_3: 0.6958 - val_recall_3: 0.8428 - 2s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "509/509 - 1s - loss: 1.3018 - precision_3: 0.7009 - recall_3: 0.8050 - val_loss: 1.1626 - val_precision_3: 0.7156 - val_recall_3: 0.8170 - 1s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "509/509 - 1s - loss: 1.1179 - precision_3: 0.7057 - recall_3: 0.8109 - val_loss: 1.0455 - val_precision_3: 0.7190 - val_recall_3: 0.8106 - 1s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "509/509 - 1s - loss: 1.0198 - precision_3: 0.7054 - recall_3: 0.8101 - val_loss: 0.9676 - val_precision_3: 0.7048 - val_recall_3: 0.8342 - 1s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "509/509 - 1s - loss: 0.9596 - precision_3: 0.7064 - recall_3: 0.8102 - val_loss: 0.9235 - val_precision_3: 0.7047 - val_recall_3: 0.8339 - 1s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "509/509 - 1s - loss: 0.9125 - precision_3: 0.7080 - recall_3: 0.8127 - val_loss: 0.8833 - val_precision_3: 0.6910 - val_recall_3: 0.8536 - 1s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "509/509 - 1s - loss: 0.8807 - precision_3: 0.7070 - recall_3: 0.8130 - val_loss: 0.8481 - val_precision_3: 0.7143 - val_recall_3: 0.8210 - 1s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "509/509 - 1s - loss: 0.8488 - precision_3: 0.7099 - recall_3: 0.8139 - val_loss: 0.8218 - val_precision_3: 0.7193 - val_recall_3: 0.8092 - 1s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "509/509 - 1s - loss: 0.8243 - precision_3: 0.7068 - recall_3: 0.8119 - val_loss: 0.8013 - val_precision_3: 0.7122 - val_recall_3: 0.8249 - 1s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "509/509 - 2s - loss: 0.8082 - precision_3: 0.7074 - recall_3: 0.8124 - val_loss: 0.7868 - val_precision_3: 0.7155 - val_recall_3: 0.8195 - 2s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "509/509 - 2s - loss: 0.7944 - precision_3: 0.7083 - recall_3: 0.8116 - val_loss: 0.7768 - val_precision_3: 0.7182 - val_recall_3: 0.8095 - 2s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "509/509 - 1s - loss: 0.7793 - precision_3: 0.7083 - recall_3: 0.8109 - val_loss: 0.7671 - val_precision_3: 0.7183 - val_recall_3: 0.8117 - 1s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "509/509 - 1s - loss: 0.7670 - precision_3: 0.7086 - recall_3: 0.8122 - val_loss: 0.7456 - val_precision_3: 0.7192 - val_recall_3: 0.8070 - 1s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "509/509 - 1s - loss: 0.7510 - precision_3: 0.7089 - recall_3: 0.8088 - val_loss: 0.7437 - val_precision_3: 0.7078 - val_recall_3: 0.8310 - 1s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "509/509 - 1s - loss: 0.7411 - precision_3: 0.7118 - recall_3: 0.8124 - val_loss: 0.7186 - val_precision_3: 0.6980 - val_recall_3: 0.8450 - 1s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "509/509 - 1s - loss: 0.7295 - precision_3: 0.7106 - recall_3: 0.8104 - val_loss: 0.7226 - val_precision_3: 0.7155 - val_recall_3: 0.8138 - 1s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "509/509 - 1s - loss: 0.7263 - precision_3: 0.7115 - recall_3: 0.8084 - val_loss: 0.7039 - val_precision_3: 0.7139 - val_recall_3: 0.8203 - 1s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "509/509 - 1s - loss: 0.7158 - precision_3: 0.7119 - recall_3: 0.8094 - val_loss: 0.7028 - val_precision_3: 0.7165 - val_recall_3: 0.8110 - 1s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "509/509 - 1s - loss: 0.7075 - precision_3: 0.7121 - recall_3: 0.8086 - val_loss: 0.6875 - val_precision_3: 0.7197 - val_recall_3: 0.8027 - 1s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "509/509 - 1s - loss: 0.6996 - precision_3: 0.7141 - recall_3: 0.8064 - val_loss: 0.6870 - val_precision_3: 0.7253 - val_recall_3: 0.7931 - 1s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "509/509 - 2s - loss: 0.6927 - precision_3: 0.7131 - recall_3: 0.8047 - val_loss: 0.6826 - val_precision_3: 0.7064 - val_recall_3: 0.8321 - 2s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "509/509 - 2s - loss: 0.6866 - precision_3: 0.7138 - recall_3: 0.8046 - val_loss: 0.6720 - val_precision_3: 0.7191 - val_recall_3: 0.8049 - 2s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "509/509 - 2s - loss: 0.6796 - precision_3: 0.7151 - recall_3: 0.8038 - val_loss: 0.6674 - val_precision_3: 0.7290 - val_recall_3: 0.7830 - 2s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "509/509 - 1s - loss: 0.6767 - precision_3: 0.7145 - recall_3: 0.8045 - val_loss: 0.6679 - val_precision_3: 0.7079 - val_recall_3: 0.8303 - 1s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "509/509 - 1s - loss: 0.6755 - precision_3: 0.7178 - recall_3: 0.8050 - val_loss: 0.6660 - val_precision_3: 0.7160 - val_recall_3: 0.8131 - 1s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "509/509 - 1s - loss: 0.6732 - precision_3: 0.7163 - recall_3: 0.8033 - val_loss: 0.6671 - val_precision_3: 0.7220 - val_recall_3: 0.7988 - 1s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "509/509 - 1s - loss: 0.6708 - precision_3: 0.7151 - recall_3: 0.8017 - val_loss: 0.6532 - val_precision_3: 0.7113 - val_recall_3: 0.8221 - 1s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "509/509 - 1s - loss: 0.6631 - precision_3: 0.7161 - recall_3: 0.8048 - val_loss: 0.6547 - val_precision_3: 0.7221 - val_recall_3: 0.7956 - 1s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "509/509 - 1s - loss: 0.6602 - precision_3: 0.7164 - recall_3: 0.8014 - val_loss: 0.6556 - val_precision_3: 0.7204 - val_recall_3: 0.8016 - 1s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "509/509 - 1s - loss: 0.6587 - precision_3: 0.7170 - recall_3: 0.8002 - val_loss: 0.6442 - val_precision_3: 0.7194 - val_recall_3: 0.8042 - 1s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "509/509 - 1s - loss: 0.6548 - precision_3: 0.7171 - recall_3: 0.8028 - val_loss: 0.6419 - val_precision_3: 0.7190 - val_recall_3: 0.8045 - 1s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "509/509 - 1s - loss: 0.6514 - precision_3: 0.7176 - recall_3: 0.8008 - val_loss: 0.6421 - val_precision_3: 0.7202 - val_recall_3: 0.7999 - 1s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "509/509 - 2s - loss: 0.6482 - precision_3: 0.7177 - recall_3: 0.7987 - val_loss: 0.6380 - val_precision_3: 0.7151 - val_recall_3: 0.8160 - 2s/epoch - 3ms/step\n",
            "Epoch 34/100\n",
            "509/509 - 2s - loss: 0.6485 - precision_3: 0.7167 - recall_3: 0.7992 - val_loss: 0.6329 - val_precision_3: 0.7186 - val_recall_3: 0.8020 - 2s/epoch - 3ms/step\n",
            "Epoch 35/100\n",
            "509/509 - 1s - loss: 0.6469 - precision_3: 0.7170 - recall_3: 0.8009 - val_loss: 0.6438 - val_precision_3: 0.7152 - val_recall_3: 0.8181 - 1s/epoch - 3ms/step\n",
            "Epoch 36/100\n",
            "509/509 - 1s - loss: 0.6460 - precision_3: 0.7172 - recall_3: 0.7996 - val_loss: 0.6336 - val_precision_3: 0.7303 - val_recall_3: 0.7805 - 1s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "509/509 - 1s - loss: 0.6419 - precision_3: 0.7196 - recall_3: 0.7983 - val_loss: 0.6403 - val_precision_3: 0.7164 - val_recall_3: 0.8113 - 1s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "509/509 - 1s - loss: 0.6411 - precision_3: 0.7183 - recall_3: 0.7999 - val_loss: 0.6275 - val_precision_3: 0.7209 - val_recall_3: 0.7974 - 1s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "509/509 - 1s - loss: 0.6389 - precision_3: 0.7201 - recall_3: 0.7998 - val_loss: 0.6342 - val_precision_3: 0.7266 - val_recall_3: 0.7898 - 1s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "509/509 - 1s - loss: 0.6378 - precision_3: 0.7189 - recall_3: 0.7989 - val_loss: 0.6239 - val_precision_3: 0.7163 - val_recall_3: 0.8117 - 1s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "509/509 - 1s - loss: 0.6347 - precision_3: 0.7183 - recall_3: 0.8009 - val_loss: 0.6274 - val_precision_3: 0.7207 - val_recall_3: 0.7974 - 1s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "509/509 - 1s - loss: 0.6331 - precision_3: 0.7194 - recall_3: 0.7985 - val_loss: 0.6193 - val_precision_3: 0.7187 - val_recall_3: 0.8042 - 1s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "509/509 - 1s - loss: 0.6310 - precision_3: 0.7199 - recall_3: 0.7978 - val_loss: 0.6247 - val_precision_3: 0.7191 - val_recall_3: 0.8009 - 1s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "509/509 - 1s - loss: 0.6280 - precision_3: 0.7208 - recall_3: 0.8000 - val_loss: 0.6128 - val_precision_3: 0.7201 - val_recall_3: 0.7988 - 1s/epoch - 3ms/step\n",
            "Epoch 45/100\n",
            "509/509 - 2s - loss: 0.6251 - precision_3: 0.7202 - recall_3: 0.7966 - val_loss: 0.6191 - val_precision_3: 0.7156 - val_recall_3: 0.8127 - 2s/epoch - 3ms/step\n",
            "Epoch 46/100\n",
            "509/509 - 2s - loss: 0.6228 - precision_3: 0.7210 - recall_3: 0.7992 - val_loss: 0.6113 - val_precision_3: 0.7280 - val_recall_3: 0.7859 - 2s/epoch - 3ms/step\n",
            "Epoch 47/100\n",
            "509/509 - 1s - loss: 0.6206 - precision_3: 0.7209 - recall_3: 0.7973 - val_loss: 0.6183 - val_precision_3: 0.7109 - val_recall_3: 0.8242 - 1s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "509/509 - 1s - loss: 0.6198 - precision_3: 0.7217 - recall_3: 0.7981 - val_loss: 0.6143 - val_precision_3: 0.7118 - val_recall_3: 0.8199 - 1s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "509/509 - 1s - loss: 0.6183 - precision_3: 0.7232 - recall_3: 0.8007 - val_loss: 0.6116 - val_precision_3: 0.7244 - val_recall_3: 0.7916 - 1s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "509/509 - 1s - loss: 0.6158 - precision_3: 0.7211 - recall_3: 0.7963 - val_loss: 0.6054 - val_precision_3: 0.7153 - val_recall_3: 0.8095 - 1s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "509/509 - 1s - loss: 0.6158 - precision_3: 0.7217 - recall_3: 0.7971 - val_loss: 0.6120 - val_precision_3: 0.7202 - val_recall_3: 0.7981 - 1s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "509/509 - 1s - loss: 0.6150 - precision_3: 0.7193 - recall_3: 0.7968 - val_loss: 0.5997 - val_precision_3: 0.7189 - val_recall_3: 0.8031 - 1s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "509/509 - 1s - loss: 0.6131 - precision_3: 0.7217 - recall_3: 0.8004 - val_loss: 0.6123 - val_precision_3: 0.7242 - val_recall_3: 0.7916 - 1s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "509/509 - 1s - loss: 0.6121 - precision_3: 0.7202 - recall_3: 0.7993 - val_loss: 0.5988 - val_precision_3: 0.7169 - val_recall_3: 0.8077 - 1s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "509/509 - 1s - loss: 0.6105 - precision_3: 0.7202 - recall_3: 0.7979 - val_loss: 0.6059 - val_precision_3: 0.7229 - val_recall_3: 0.7959 - 1s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "509/509 - 1s - loss: 0.6098 - precision_3: 0.7210 - recall_3: 0.7979 - val_loss: 0.6020 - val_precision_3: 0.7052 - val_recall_3: 0.8335 - 1s/epoch - 3ms/step\n",
            "Epoch 57/100\n",
            "509/509 - 2s - loss: 0.6077 - precision_3: 0.7205 - recall_3: 0.7983 - val_loss: 0.6042 - val_precision_3: 0.7274 - val_recall_3: 0.7863 - 2s/epoch - 3ms/step\n",
            "Epoch 58/100\n",
            "509/509 - 2s - loss: 0.6066 - precision_3: 0.7213 - recall_3: 0.7965 - val_loss: 0.5926 - val_precision_3: 0.7146 - val_recall_3: 0.8088 - 2s/epoch - 3ms/step\n",
            "Epoch 59/100\n",
            "509/509 - 1s - loss: 0.6057 - precision_3: 0.7209 - recall_3: 0.7987 - val_loss: 0.6009 - val_precision_3: 0.7126 - val_recall_3: 0.8192 - 1s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "509/509 - 1s - loss: 0.6050 - precision_3: 0.7202 - recall_3: 0.7986 - val_loss: 0.5954 - val_precision_3: 0.7183 - val_recall_3: 0.8052 - 1s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "509/509 - 1s - loss: 0.6023 - precision_3: 0.7224 - recall_3: 0.7994 - val_loss: 0.5958 - val_precision_3: 0.7192 - val_recall_3: 0.8016 - 1s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "509/509 - 1s - loss: 0.6021 - precision_3: 0.7219 - recall_3: 0.7975 - val_loss: 0.5937 - val_precision_3: 0.7213 - val_recall_3: 0.7977 - 1s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "509/509 - 1s - loss: 0.6005 - precision_3: 0.7224 - recall_3: 0.7970 - val_loss: 0.5924 - val_precision_3: 0.7173 - val_recall_3: 0.8049 - 1s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "509/509 - 1s - loss: 0.5978 - precision_3: 0.7199 - recall_3: 0.8001 - val_loss: 0.5874 - val_precision_3: 0.7215 - val_recall_3: 0.7988 - 1s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "509/509 - 1s - loss: 0.5963 - precision_3: 0.7205 - recall_3: 0.7975 - val_loss: 0.5891 - val_precision_3: 0.7157 - val_recall_3: 0.8067 - 1s/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "509/509 - 1s - loss: 0.5951 - precision_3: 0.7202 - recall_3: 0.7966 - val_loss: 0.5859 - val_precision_3: 0.7217 - val_recall_3: 0.7956 - 1s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "509/509 - 1s - loss: 0.5923 - precision_3: 0.7223 - recall_3: 0.7986 - val_loss: 0.5879 - val_precision_3: 0.7123 - val_recall_3: 0.8156 - 1s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "509/509 - 1s - loss: 0.5918 - precision_3: 0.7208 - recall_3: 0.7967 - val_loss: 0.5847 - val_precision_3: 0.7126 - val_recall_3: 0.8124 - 1s/epoch - 3ms/step\n",
            "Epoch 69/100\n",
            "509/509 - 2s - loss: 0.5908 - precision_3: 0.7204 - recall_3: 0.7992 - val_loss: 0.5832 - val_precision_3: 0.7232 - val_recall_3: 0.7941 - 2s/epoch - 3ms/step\n",
            "Epoch 70/100\n",
            "509/509 - 2s - loss: 0.5910 - precision_3: 0.7207 - recall_3: 0.7966 - val_loss: 0.5875 - val_precision_3: 0.7161 - val_recall_3: 0.8038 - 2s/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "509/509 - 1s - loss: 0.5933 - precision_3: 0.7224 - recall_3: 0.7980 - val_loss: 0.5838 - val_precision_3: 0.7101 - val_recall_3: 0.8278 - 1s/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "509/509 - 1s - loss: 0.5918 - precision_3: 0.7222 - recall_3: 0.7970 - val_loss: 0.5851 - val_precision_3: 0.7124 - val_recall_3: 0.8142 - 1s/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "509/509 - 1s - loss: 0.5891 - precision_3: 0.7226 - recall_3: 0.7994 - val_loss: 0.5793 - val_precision_3: 0.7127 - val_recall_3: 0.8153 - 1s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "509/509 - 1s - loss: 0.5875 - precision_3: 0.7235 - recall_3: 0.7997 - val_loss: 0.5829 - val_precision_3: 0.7124 - val_recall_3: 0.8149 - 1s/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "509/509 - 1s - loss: 0.5869 - precision_3: 0.7223 - recall_3: 0.8013 - val_loss: 0.5772 - val_precision_3: 0.7137 - val_recall_3: 0.8106 - 1s/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "509/509 - 1s - loss: 0.5870 - precision_3: 0.7208 - recall_3: 0.7983 - val_loss: 0.5836 - val_precision_3: 0.7290 - val_recall_3: 0.7841 - 1s/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "509/509 - 1s - loss: 0.5855 - precision_3: 0.7224 - recall_3: 0.8006 - val_loss: 0.5742 - val_precision_3: 0.7252 - val_recall_3: 0.7909 - 1s/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "509/509 - 1s - loss: 0.5863 - precision_3: 0.7221 - recall_3: 0.7965 - val_loss: 0.5832 - val_precision_3: 0.7139 - val_recall_3: 0.8113 - 1s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "509/509 - 1s - loss: 0.5849 - precision_3: 0.7218 - recall_3: 0.7975 - val_loss: 0.5727 - val_precision_3: 0.7162 - val_recall_3: 0.8088 - 1s/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "509/509 - 1s - loss: 0.5843 - precision_3: 0.7218 - recall_3: 0.7983 - val_loss: 0.5795 - val_precision_3: 0.7250 - val_recall_3: 0.7902 - 1s/epoch - 3ms/step\n",
            "Epoch 81/100\n",
            "509/509 - 2s - loss: 0.5826 - precision_3: 0.7210 - recall_3: 0.7974 - val_loss: 0.5708 - val_precision_3: 0.7145 - val_recall_3: 0.8092 - 2s/epoch - 3ms/step\n",
            "Epoch 82/100\n",
            "509/509 - 2s - loss: 0.5805 - precision_3: 0.7225 - recall_3: 0.7970 - val_loss: 0.5801 - val_precision_3: 0.7089 - val_recall_3: 0.8267 - 2s/epoch - 3ms/step\n",
            "Epoch 83/100\n",
            "509/509 - 1s - loss: 0.5793 - precision_3: 0.7219 - recall_3: 0.8000 - val_loss: 0.5699 - val_precision_3: 0.7316 - val_recall_3: 0.7759 - 1s/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "509/509 - 1s - loss: 0.5803 - precision_3: 0.7212 - recall_3: 0.7967 - val_loss: 0.5762 - val_precision_3: 0.7142 - val_recall_3: 0.8088 - 1s/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "509/509 - 1s - loss: 0.5790 - precision_3: 0.7219 - recall_3: 0.7986 - val_loss: 0.5692 - val_precision_3: 0.7186 - val_recall_3: 0.7981 - 1s/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "509/509 - 1s - loss: 0.5787 - precision_3: 0.7223 - recall_3: 0.7989 - val_loss: 0.5742 - val_precision_3: 0.7138 - val_recall_3: 0.8117 - 1s/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "509/509 - 1s - loss: 0.5789 - precision_3: 0.7231 - recall_3: 0.7974 - val_loss: 0.5693 - val_precision_3: 0.7173 - val_recall_3: 0.8042 - 1s/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "509/509 - 1s - loss: 0.5772 - precision_3: 0.7231 - recall_3: 0.7985 - val_loss: 0.5700 - val_precision_3: 0.7190 - val_recall_3: 0.7988 - 1s/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "509/509 - 1s - loss: 0.5776 - precision_3: 0.7220 - recall_3: 0.7973 - val_loss: 0.5705 - val_precision_3: 0.7233 - val_recall_3: 0.7927 - 1s/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "509/509 - 1s - loss: 0.5776 - precision_3: 0.7227 - recall_3: 0.7992 - val_loss: 0.5729 - val_precision_3: 0.7299 - val_recall_3: 0.7820 - 1s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "509/509 - 1s - loss: 0.5768 - precision_3: 0.7231 - recall_3: 0.7969 - val_loss: 0.5692 - val_precision_3: 0.7166 - val_recall_3: 0.8049 - 1s/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "509/509 - 2s - loss: 0.5768 - precision_3: 0.7213 - recall_3: 0.7980 - val_loss: 0.5718 - val_precision_3: 0.7227 - val_recall_3: 0.7931 - 2s/epoch - 3ms/step\n",
            "Epoch 93/100\n",
            "509/509 - 2s - loss: 0.5767 - precision_3: 0.7221 - recall_3: 0.7989 - val_loss: 0.5693 - val_precision_3: 0.7237 - val_recall_3: 0.7923 - 2s/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "509/509 - 1s - loss: 0.5782 - precision_3: 0.7240 - recall_3: 0.7978 - val_loss: 0.5770 - val_precision_3: 0.7121 - val_recall_3: 0.8156 - 1s/epoch - 3ms/step\n",
            "Epoch 95/100\n",
            "509/509 - 1s - loss: 0.5772 - precision_3: 0.7226 - recall_3: 0.7970 - val_loss: 0.5658 - val_precision_3: 0.7161 - val_recall_3: 0.8081 - 1s/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "509/509 - 1s - loss: 0.5764 - precision_3: 0.7232 - recall_3: 0.7970 - val_loss: 0.5710 - val_precision_3: 0.7180 - val_recall_3: 0.8031 - 1s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "509/509 - 1s - loss: 0.5756 - precision_3: 0.7225 - recall_3: 0.7970 - val_loss: 0.5661 - val_precision_3: 0.7323 - val_recall_3: 0.7798 - 1s/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "509/509 - 1s - loss: 0.5770 - precision_3: 0.7219 - recall_3: 0.7979 - val_loss: 0.5746 - val_precision_3: 0.7231 - val_recall_3: 0.7931 - 1s/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "509/509 - 1s - loss: 0.5748 - precision_3: 0.7250 - recall_3: 0.7969 - val_loss: 0.5630 - val_precision_3: 0.7156 - val_recall_3: 0.8081 - 1s/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "509/509 - 1s - loss: 0.5747 - precision_3: 0.7209 - recall_3: 0.7987 - val_loss: 0.5732 - val_precision_3: 0.7198 - val_recall_3: 0.8002 - 1s/epoch - 2ms/step\n",
            "442/442 [==============================] - 1s 2ms/step - loss: 0.5724 - precision_3: 0.7300 - recall_3: 0.7886\n",
            "binary cross-entropy loss :  0.5724135637283325  precision:  0.7300350666046143  recal:  0.7886099219322205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature reduction: PCA\n",
        "Principle component analysis is a technique which is used to decrease feature count such that their information is preserved, only in lower dimensions. It mainly aims to keep the variance of the original data in fewer columns. Result however, did not show any significant improvement, suggesting that important relations between features and target are already captured.  \n"
      ],
      "metadata": {
        "id": "P2SPP1tvHTyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Final result\n",
        "for the purpose of our research, we will use the following weights obtained from the Logistic regression model to apply on users' input.\n",
        "\n",
        "### feature selected model:\n",
        "$$[0.0702186,  0.08500237, 0.29923042, 0.03782045, 0.39012718, 0.30067847]$$\n",
        "\n",
        "\n",
        "### original model:\n",
        "$$[ 0.37257845,  0.28995716,  0.66909954,  0.0368971,  -0.00249678,  0.09599408,\n",
        "   0.11823623, -0.01565236, -0.01979998, -0.05413245, -0.37604701,  0.00626531,\n",
        "   0.02791119,  0.30086121, -0.00214749, -0.00390029,  0.08630184,  0.11340987,\n",
        "   0.07794295, -0.03393694]$$"
      ],
      "metadata": {
        "id": "G_ytRJFYKK2t"
      }
    }
  ]
}