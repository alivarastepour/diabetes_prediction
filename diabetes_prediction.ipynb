{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ7mJJGD/ALxvtFszADJ5R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alivarastepour/diabetes_prediction/blob/master/diabetes_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purpose of this notebook\n",
        "This notebook aims to build a model that determines whether a person is prone to diabetes or not. Additionally, it seeks to identify a subset of features (risk factors) that can accurately predict the risk of diabetes. The weights of the optimal solution will be utilized in another project, where they will be applied to users' inputs in real time.\n",
        "\n",
        "## Dataset\n",
        "This notebook makes use of a subset of a larger dataset which aimed to collect uniform, state-specific data on preventive health practices and risk behaviors that are associated with chronic diseases, injuries, and preventable infectious diseases in the adult population. The subset used in this notebook can be accessed [here](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_binary_5050split_health_indicators_BRFSS2015.csv)."
      ],
      "metadata": {
        "id": "q2IpptVI_N1M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EtkfdEZs9h7u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LeakyReLU,Dropout\n",
        "from keras.optimizers import Adagrad, RMSprop, Adam\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.initializers import he_normal\n",
        "from keras.activations import selu\n",
        "from keras.metrics import Precision, Recall"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/drive')\n",
        "DATASET_ADDRESS = '/drive/MyDrive/diabetes_info.csv'\n",
        "raw_dataset = pd.read_csv(DATASET_ADDRESS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF0Coc72--j8",
        "outputId": "51fc9db1-18ec-426f-92b5-c4e464b2f20d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5AdQ7V9_LDr",
        "outputId": "44efd5fd-e57d-412d-b243-108fd3863833"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 70692 entries, 0 to 70691\n",
            "Data columns (total 22 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   Diabetes_binary       70692 non-null  float64\n",
            " 1   HighBP                70692 non-null  float64\n",
            " 2   HighChol              70692 non-null  float64\n",
            " 3   CholCheck             70692 non-null  float64\n",
            " 4   BMI                   70692 non-null  float64\n",
            " 5   Smoker                70692 non-null  float64\n",
            " 6   Stroke                70692 non-null  float64\n",
            " 7   HeartDiseaseorAttack  70692 non-null  float64\n",
            " 8   PhysActivity          70692 non-null  float64\n",
            " 9   Fruits                70692 non-null  float64\n",
            " 10  Veggies               70692 non-null  float64\n",
            " 11  HvyAlcoholConsump     70692 non-null  float64\n",
            " 12  AnyHealthcare         70692 non-null  float64\n",
            " 13  NoDocbcCost           70692 non-null  float64\n",
            " 14  GenHlth               70692 non-null  float64\n",
            " 15  MentHlth              70692 non-null  float64\n",
            " 16  PhysHlth              70692 non-null  float64\n",
            " 17  DiffWalk              70692 non-null  float64\n",
            " 18  Sex                   70692 non-null  float64\n",
            " 19  Age                   70692 non-null  float64\n",
            " 20  Education             70692 non-null  float64\n",
            " 21  Income                70692 non-null  float64\n",
            "dtypes: float64(22)\n",
            "memory usage: 11.9 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = raw_dataset[\"Diabetes_binary\"]\n",
        "x = raw_dataset.drop(columns=[\"Diabetes_binary\"])"
      ],
      "metadata": {
        "id": "Di2eBqgLFkFT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=9)"
      ],
      "metadata": {
        "id": "SSYmWVclFr_V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection\n",
        "While our data may appear relatively clean, this does not guarantee optimal performance. Therefore, we must leverage a range of machine learning models to assess their effectiveness and identify potential modifications to the original data that can enhance the performance of our models."
      ],
      "metadata": {
        "id": "8k-7qJWsJsml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First model: Gradient boost classifier\n",
        "Boosting algorithms have been widely recognized as effective choices for handling tabular data. Among them, gradient boosting stands out as a prominent technique that leverages decision trees to create a powerful ensemble model. Nonetheless, to ensure its optimal performance, careful consideration should be given to hyperparameter tuning."
      ],
      "metadata": {
        "id": "xpB6qQ79KbGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(dataset):\n",
        "  y = dataset[\"Diabetes_binary\"]\n",
        "  x = dataset.drop(columns=[\"Diabetes_binary\"])\n",
        "  return train_test_split(x, y, test_size=0.20, random_state=9)"
      ],
      "metadata": {
        "id": "2bZnlwkq8_PX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradienBoostClassifierModel(dataset, learning_rate=0.05, n_estimators=150, subsample=0.8):\n",
        "  x_train, x_test, y_train, y_test = get_data(dataset)\n",
        "  reg = GradientBoostingClassifier(random_state=90,\n",
        "                                loss='deviance',\n",
        "                                learning_rate=learning_rate,\n",
        "                                n_estimators=n_estimators,\n",
        "                                subsample=subsample,\n",
        "                                criterion='friedman_mse',\n",
        "                                verbose=2,\n",
        "                                )\n",
        "  reg.fit(x_train, y_train)\n",
        "  y_pred = reg.predict(x_test)\n",
        "  report = classification_report(y_test, y_pred)\n",
        "  print(report)"
      ],
      "metadata": {
        "id": "WGLlf7wgCSDR"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradienBoostClassifierModel(raw_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJHCJ4TfCzce",
        "outputId": "83a67d70-120b-401d-f095-7617b0095c1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3619           0.0245            7.23s\n",
            "         2           1.3400           0.0224            7.08s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         3           1.3200           0.0203            7.03s\n",
            "         4           1.3008           0.0178            6.92s\n",
            "         5           1.2841           0.0163            6.93s\n",
            "         6           1.2685           0.0151            6.84s\n",
            "         7           1.2545           0.0136            6.75s\n",
            "         8           1.2417           0.0127            6.74s\n",
            "         9           1.2303           0.0117            6.69s\n",
            "        10           1.2191           0.0112            6.73s\n",
            "        11           1.2090           0.0101            6.66s\n",
            "        12           1.1982           0.0100            6.64s\n",
            "        13           1.1890           0.0094            6.59s\n",
            "        14           1.1793           0.0088            6.57s\n",
            "        15           1.1707           0.0078            6.54s\n",
            "        16           1.1641           0.0067            6.52s\n",
            "        17           1.1571           0.0076            6.52s\n",
            "        18           1.1502           0.0062            6.44s\n",
            "        19           1.1433           0.0058            6.40s\n",
            "        20           1.1379           0.0066            6.34s\n",
            "        21           1.1312           0.0059            6.26s\n",
            "        22           1.1278           0.0050            6.20s\n",
            "        23           1.1231           0.0050            6.14s\n",
            "        24           1.1165           0.0051            6.07s\n",
            "        25           1.1117           0.0040            6.02s\n",
            "        26           1.1087           0.0043            5.95s\n",
            "        27           1.1027           0.0045            5.89s\n",
            "        28           1.0968           0.0040            5.84s\n",
            "        29           1.0943           0.0034            5.81s\n",
            "        30           1.0898           0.0036            5.76s\n",
            "        31           1.0874           0.0029            5.71s\n",
            "        32           1.0858           0.0034            5.65s\n",
            "        33           1.0806           0.0030            5.62s\n",
            "        34           1.0780           0.0027            5.59s\n",
            "        35           1.0782           0.0027            5.56s\n",
            "        36           1.0716           0.0025            5.51s\n",
            "        37           1.0690           0.0025            5.46s\n",
            "        38           1.0686           0.0026            5.42s\n",
            "        39           1.0640           0.0022            5.37s\n",
            "        40           1.0635           0.0019            5.32s\n",
            "        41           1.0597           0.0017            5.27s\n",
            "        42           1.0594           0.0019            5.21s\n",
            "        43           1.0554           0.0017            5.16s\n",
            "        44           1.0557           0.0017            5.11s\n",
            "        45           1.0558           0.0018            5.06s\n",
            "        46           1.0505           0.0015            5.00s\n",
            "        47           1.0505           0.0018            4.95s\n",
            "        48           1.0462           0.0015            4.90s\n",
            "        49           1.0476           0.0012            4.84s\n",
            "        50           1.0454           0.0014            4.80s\n",
            "        51           1.0443           0.0012            4.75s\n",
            "        52           1.0434           0.0012            4.71s\n",
            "        53           1.0391           0.0011            4.67s\n",
            "        54           1.0407           0.0011            4.62s\n",
            "        55           1.0360           0.0011            4.57s\n",
            "        56           1.0381           0.0011            4.52s\n",
            "        57           1.0365           0.0010            4.47s\n",
            "        58           1.0363           0.0012            4.42s\n",
            "        59           1.0311           0.0008            4.38s\n",
            "        60           1.0329           0.0010            4.34s\n",
            "        61           1.0322           0.0009            4.29s\n",
            "        62           1.0295           0.0009            4.24s\n",
            "        63           1.0297           0.0009            4.19s\n",
            "        64           1.0304           0.0007            4.14s\n",
            "        65           1.0277           0.0008            4.10s\n",
            "        66           1.0294           0.0007            4.05s\n",
            "        67           1.0280           0.0009            4.00s\n",
            "        68           1.0247           0.0006            3.96s\n",
            "        69           1.0263           0.0006            3.91s\n",
            "        70           1.0220           0.0005            3.87s\n",
            "        71           1.0246           0.0006            3.82s\n",
            "        72           1.0214           0.0007            3.77s\n",
            "        73           1.0221           0.0007            3.72s\n",
            "        74           1.0198           0.0004            3.67s\n",
            "        75           1.0226           0.0006            3.62s\n",
            "        76           1.0190           0.0004            3.57s\n",
            "        77           1.0218           0.0006            3.52s\n",
            "        78           1.0171           0.0005            3.48s\n",
            "        79           1.0174           0.0004            3.42s\n",
            "        80           1.0173           0.0003            3.38s\n",
            "        81           1.0168           0.0005            3.32s\n",
            "        82           1.0148           0.0004            3.27s\n",
            "        83           1.0181           0.0004            3.22s\n",
            "        84           1.0173           0.0003            3.17s\n",
            "        85           1.0159           0.0003            3.12s\n",
            "        86           1.0146           0.0004            3.07s\n",
            "        87           1.0181           0.0003            3.02s\n",
            "        88           1.0144           0.0004            2.98s\n",
            "        89           1.0153           0.0003            2.93s\n",
            "        90           1.0082           0.0001            2.88s\n",
            "        91           1.0150           0.0004            2.83s\n",
            "        92           1.0121           0.0002            2.78s\n",
            "        93           1.0112           0.0003            2.73s\n",
            "        94           1.0107           0.0003            2.68s\n",
            "        95           1.0098           0.0002            2.63s\n",
            "        96           1.0082           0.0003            2.59s\n",
            "        97           1.0098           0.0003            2.54s\n",
            "        98           1.0100           0.0003            2.49s\n",
            "        99           1.0104           0.0001            2.44s\n",
            "       100           1.0070           0.0002            2.39s\n",
            "       101           1.0056           0.0001            2.34s\n",
            "       102           1.0088           0.0001            2.30s\n",
            "       103           1.0087           0.0002            2.25s\n",
            "       104           1.0112           0.0002            2.20s\n",
            "       105           1.0057           0.0002            2.16s\n",
            "       106           1.0039           0.0002            2.11s\n",
            "       107           1.0075           0.0001            2.06s\n",
            "       108           1.0104           0.0002            2.01s\n",
            "       109           1.0068           0.0001            1.96s\n",
            "       110           1.0056           0.0001            1.91s\n",
            "       111           1.0090           0.0001            1.87s\n",
            "       112           1.0080           0.0001            1.82s\n",
            "       113           1.0031           0.0001            1.77s\n",
            "       114           1.0058           0.0002            1.72s\n",
            "       115           1.0075           0.0002            1.67s\n",
            "       116           1.0070           0.0001            1.63s\n",
            "       117           1.0035           0.0001            1.58s\n",
            "       118           1.0052           0.0001            1.53s\n",
            "       119           1.0044           0.0001            1.48s\n",
            "       120           1.0032           0.0001            1.43s\n",
            "       121           1.0045           0.0001            1.39s\n",
            "       122           1.0009           0.0001            1.34s\n",
            "       123           1.0028           0.0001            1.29s\n",
            "       124           1.0061           0.0002            1.25s\n",
            "       125           0.9986           0.0000            1.20s\n",
            "       126           1.0018           0.0001            1.15s\n",
            "       127           1.0029           0.0001            1.11s\n",
            "       128           1.0038           0.0000            1.06s\n",
            "       129           1.0029           0.0000            1.01s\n",
            "       130           1.0039           0.0001            0.96s\n",
            "       131           1.0027           0.0001            0.91s\n",
            "       132           1.0010           0.0001            0.86s\n",
            "       133           1.0008          -0.0000            0.82s\n",
            "       134           1.0019          -0.0000            0.77s\n",
            "       135           1.0023           0.0000            0.72s\n",
            "       136           1.0003           0.0000            0.67s\n",
            "       137           1.0014           0.0001            0.62s\n",
            "       138           1.0026           0.0000            0.58s\n",
            "       139           0.9997           0.0001            0.53s\n",
            "       140           1.0030           0.0000            0.48s\n",
            "       141           0.9968           0.0000            0.43s\n",
            "       142           1.0005           0.0000            0.38s\n",
            "       143           1.0022           0.0000            0.34s\n",
            "       144           0.9993           0.0001            0.29s\n",
            "       145           1.0011           0.0000            0.24s\n",
            "       146           1.0000           0.0000            0.19s\n",
            "       147           0.9979           0.0000            0.14s\n",
            "       148           1.0005           0.0000            0.10s\n",
            "       149           1.0005           0.0000            0.05s\n",
            "       150           0.9998          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.73      7010\n",
            "         1.0       0.73      0.78      0.76      7129\n",
            "\n",
            "    accuracy                           0.75     14139\n",
            "   macro avg       0.75      0.75      0.75     14139\n",
            "weighted avg       0.75      0.75      0.75     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The deviance loss\n",
        "Deviance loss is a commonly used loss function in binary classification problems. With a glance at its formula, we can easily unserstand why:\n",
        "\n",
        "$$\n",
        "L(y, p) = \\left(y \\log(p) + (1 - y) \\log(1 - p)\\right)\n",
        "$$\n",
        "\n",
        "where y is true class and p is statistical probability.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uZs3QLvqMiLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F-1 score\n",
        "F-1 score uses precision(ratio of true possitives to true possitves and false possitives) and recall(ratio of true possitives to true possitves and false negatives) scores to prvoide a balance between them:\n",
        "\n",
        "$$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n"
      ],
      "metadata": {
        "id": "EQZOaVr-tgUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = cross_val_score(reg, x_train, y_train, cv=5, scoring='f1_macro')\n",
        "# print(\"cross validation scores(F-1) where k=5: \", scores)"
      ],
      "metadata": {
        "id": "DXcnTyapIGf-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = cross_val_score(reg, x_train, y_train, cv=10, scoring='f1_macro')\n",
        "# print(\"cross validation scores(F-1) where k=10: \", scores)"
      ],
      "metadata": {
        "id": "qMnr_XTivGkY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial evaluataion result\n",
        "As demonstrated above, whether employing Gradient Boosting with or without cross-validation, the F1 score hovers around 0.75. While this performance is acceptable, there is room for improvement."
      ],
      "metadata": {
        "id": "k3bKt0KavyOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second model: Logistic regression\n",
        "While Logistic Regression is typically considered a more linear model compared to ensemble methods, it remains a highly prevalent choice in classification problems. It offers several distinct advantages, such as strong interpretability, feature importance insights, and the ability to not only make binary classifications but also provide class probabilities. This probabilistic aspect can prove particularly valuable in certain situations.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nee1jLoqwq3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logisticRegressionModel(dataset):\n",
        "  x_train, x_test, y_train, y_test = get_data(dataset)\n",
        "  log_reg = LogisticRegression(random_state=32, solver='sag', multi_class='multinomial', verbose=2, max_iter=500)\n",
        "  log_reg.fit(x_train, y_train)\n",
        "  y_pred_log_reg = log_reg.predict(x_test)\n",
        "  report_log_reg = classification_report(y_test, y_pred_log_reg)\n",
        "  print(log_reg.coef_)\n",
        "  print(report_log_reg)"
      ],
      "metadata": {
        "id": "Wf0DjjTz8ZgA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logisticRegressionModel(raw_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_vmMs0o8ohm",
        "outputId": "35287a9b-1285-4844-8f1a-6b007d973d9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 182 epochs took 5 seconds\n",
            "[[ 0.3682435   0.29301023  0.68196175  0.03713967 -0.00414803  0.08711372\n",
            "   0.11512442 -0.01482644 -0.0180894  -0.04394051 -0.36520999  0.03005322\n",
            "   0.00677025  0.29309753 -0.00264087 -0.00402706  0.06804295  0.13130041\n",
            "   0.07615977 -0.01306131 -0.02943937]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation result\n",
        "Logistic regression exhibited slightly lower performance compared to Gradient Boosting, indicating that additional data preprocessing may be necessary to enhance model outcomes."
      ],
      "metadata": {
        "id": "4ogNdchF1lL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for class imbalance"
      ],
      "metadata": {
        "id": "Z8cK7DTC4WB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.bincount(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hRAk6_d3_f-",
        "outputId": "eed274ca-356e-448b-91e6-92eca75b98fc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([35346, 35346])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardizing features\n",
        "In this section we standardize featuers that their domain may mislead oue models."
      ],
      "metadata": {
        "id": "xruScgS86yxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_standardize = list(x.keys())"
      ],
      "metadata": {
        "id": "yhunvldR63yy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "standarized_features = scaler.fit_transform(raw_dataset[columns_to_standardize])\n",
        "standardized_dataset = pd.DataFrame()\n",
        "standardized_dataset[\"Diabetes_binary\"] = raw_dataset[\"Diabetes_binary\"]\n",
        "standardized_dataset[columns_to_standardize] = standarized_features"
      ],
      "metadata": {
        "id": "BwgHyv_V7aZ9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradienBoostClassifierModel(standardized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAo1SN_EC-WW",
        "outputId": "540e08cb-9474-4936-82cd-16e4c455476f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3619           0.0245            7.32s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         2           1.3400           0.0224            7.08s\n",
            "         3           1.3200           0.0203            7.00s\n",
            "         4           1.3008           0.0178            6.98s\n",
            "         5           1.2841           0.0163            6.97s\n",
            "         6           1.2685           0.0151            6.96s\n",
            "         7           1.2545           0.0136            6.89s\n",
            "         8           1.2417           0.0127            6.82s\n",
            "         9           1.2303           0.0117            6.77s\n",
            "        10           1.2191           0.0112            6.73s\n",
            "        11           1.2090           0.0101            6.69s\n",
            "        12           1.1982           0.0100            6.62s\n",
            "        13           1.1890           0.0094            6.55s\n",
            "        14           1.1793           0.0088            6.49s\n",
            "        15           1.1707           0.0078            6.43s\n",
            "        16           1.1641           0.0067            6.39s\n",
            "        17           1.1571           0.0076            6.35s\n",
            "        18           1.1502           0.0062            6.29s\n",
            "        19           1.1433           0.0058            6.23s\n",
            "        20           1.1379           0.0066            6.18s\n",
            "        21           1.1312           0.0059            6.11s\n",
            "        22           1.1278           0.0050            6.08s\n",
            "        23           1.1231           0.0050            6.01s\n",
            "        24           1.1165           0.0051            5.97s\n",
            "        25           1.1117           0.0040            5.95s\n",
            "        26           1.1087           0.0043            5.89s\n",
            "        27           1.1027           0.0045            5.85s\n",
            "        28           1.0968           0.0040            5.79s\n",
            "        29           1.0943           0.0034            5.74s\n",
            "        30           1.0898           0.0036            5.70s\n",
            "        31           1.0874           0.0029            5.65s\n",
            "        32           1.0858           0.0034            5.61s\n",
            "        33           1.0806           0.0030            5.55s\n",
            "        34           1.0780           0.0027            5.50s\n",
            "        35           1.0782           0.0027            5.45s\n",
            "        36           1.0716           0.0025            5.40s\n",
            "        37           1.0690           0.0025            5.34s\n",
            "        38           1.0686           0.0026            5.30s\n",
            "        39           1.0640           0.0022            5.26s\n",
            "        40           1.0635           0.0019            5.21s\n",
            "        41           1.0597           0.0017            5.16s\n",
            "        42           1.0594           0.0019            5.11s\n",
            "        43           1.0554           0.0017            5.07s\n",
            "        44           1.0557           0.0017            5.02s\n",
            "        45           1.0558           0.0018            4.97s\n",
            "        46           1.0505           0.0015            4.94s\n",
            "        47           1.0505           0.0018            4.89s\n",
            "        48           1.0462           0.0015            4.85s\n",
            "        49           1.0476           0.0012            4.80s\n",
            "        50           1.0454           0.0014            4.75s\n",
            "        51           1.0443           0.0012            4.71s\n",
            "        52           1.0434           0.0012            4.67s\n",
            "        53           1.0391           0.0011            4.62s\n",
            "        54           1.0407           0.0011            4.56s\n",
            "        55           1.0360           0.0011            4.51s\n",
            "        56           1.0381           0.0011            4.46s\n",
            "        57           1.0365           0.0010            4.41s\n",
            "        58           1.0363           0.0012            4.36s\n",
            "        59           1.0311           0.0008            4.31s\n",
            "        60           1.0329           0.0010            4.27s\n",
            "        61           1.0322           0.0009            4.22s\n",
            "        62           1.0295           0.0009            4.17s\n",
            "        63           1.0297           0.0009            4.12s\n",
            "        64           1.0304           0.0007            4.07s\n",
            "        65           1.0277           0.0008            4.02s\n",
            "        66           1.0294           0.0007            3.97s\n",
            "        67           1.0280           0.0009            3.92s\n",
            "        68           1.0247           0.0006            3.90s\n",
            "        69           1.0263           0.0006            3.87s\n",
            "        70           1.0220           0.0005            3.82s\n",
            "        71           1.0246           0.0006            3.77s\n",
            "        72           1.0214           0.0007            3.72s\n",
            "        73           1.0221           0.0007            3.67s\n",
            "        74           1.0198           0.0004            3.63s\n",
            "        75           1.0226           0.0006            3.58s\n",
            "        76           1.0190           0.0004            3.54s\n",
            "        77           1.0218           0.0006            3.49s\n",
            "        78           1.0171           0.0005            3.44s\n",
            "        79           1.0174           0.0004            3.39s\n",
            "        80           1.0173           0.0003            3.35s\n",
            "        81           1.0168           0.0005            3.30s\n",
            "        82           1.0148           0.0004            3.25s\n",
            "        83           1.0181           0.0004            3.20s\n",
            "        84           1.0173           0.0003            3.15s\n",
            "        85           1.0159           0.0003            3.11s\n",
            "        86           1.0146           0.0004            3.06s\n",
            "        87           1.0181           0.0003            3.01s\n",
            "        88           1.0144           0.0004            2.96s\n",
            "        89           1.0153           0.0003            2.92s\n",
            "        90           1.0082           0.0001            2.87s\n",
            "        91           1.0150           0.0004            2.82s\n",
            "        92           1.0121           0.0002            2.77s\n",
            "        93           1.0112           0.0003            2.72s\n",
            "        94           1.0107           0.0003            2.68s\n",
            "        95           1.0098           0.0002            2.63s\n",
            "        96           1.0082           0.0003            2.58s\n",
            "        97           1.0098           0.0003            2.57s\n",
            "        98           1.0100           0.0003            2.60s\n",
            "        99           1.0104           0.0001            2.58s\n",
            "       100           1.0070           0.0002            2.53s\n",
            "       101           1.0056           0.0001            2.48s\n",
            "       102           1.0088           0.0001            2.43s\n",
            "       103           1.0087           0.0002            2.38s\n",
            "       104           1.0112           0.0002            2.32s\n",
            "       105           1.0057           0.0002            2.27s\n",
            "       106           1.0039           0.0002            2.22s\n",
            "       107           1.0075           0.0001            2.17s\n",
            "       108           1.0104           0.0002            2.12s\n",
            "       109           1.0068           0.0001            2.07s\n",
            "       110           1.0056           0.0001            2.02s\n",
            "       111           1.0090           0.0001            2.00s\n",
            "       112           1.0080           0.0001            1.99s\n",
            "       113           1.0031           0.0001            1.95s\n",
            "       114           1.0058           0.0002            1.90s\n",
            "       115           1.0075           0.0002            1.84s\n",
            "       116           1.0070           0.0001            1.79s\n",
            "       117           1.0035           0.0001            1.73s\n",
            "       118           1.0052           0.0001            1.68s\n",
            "       119           1.0044           0.0001            1.63s\n",
            "       120           1.0032           0.0001            1.57s\n",
            "       121           1.0045           0.0001            1.52s\n",
            "       122           1.0009           0.0001            1.46s\n",
            "       123           1.0028           0.0001            1.41s\n",
            "       124           1.0061           0.0002            1.36s\n",
            "       125           0.9986           0.0000            1.32s\n",
            "       126           1.0018           0.0001            1.29s\n",
            "       127           1.0029           0.0001            1.24s\n",
            "       128           1.0038           0.0000            1.19s\n",
            "       129           1.0029           0.0000            1.14s\n",
            "       130           1.0039           0.0001            1.08s\n",
            "       131           1.0027           0.0001            1.03s\n",
            "       132           1.0010           0.0001            0.97s\n",
            "       133           1.0008          -0.0000            0.92s\n",
            "       134           1.0019          -0.0000            0.86s\n",
            "       135           1.0023           0.0000            0.81s\n",
            "       136           1.0003           0.0000            0.75s\n",
            "       137           1.0014           0.0001            0.70s\n",
            "       138           1.0026           0.0000            0.64s\n",
            "       139           0.9997           0.0001            0.59s\n",
            "       140           1.0030           0.0000            0.54s\n",
            "       141           0.9968           0.0000            0.48s\n",
            "       142           1.0005           0.0000            0.43s\n",
            "       143           1.0022           0.0000            0.37s\n",
            "       144           0.9993           0.0001            0.32s\n",
            "       145           1.0011           0.0000            0.27s\n",
            "       146           1.0000           0.0000            0.21s\n",
            "       147           0.9979           0.0000            0.16s\n",
            "       148           1.0005           0.0000            0.11s\n",
            "       149           1.0005           0.0000            0.05s\n",
            "       150           0.9998          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.73      7010\n",
            "         1.0       0.73      0.78      0.76      7129\n",
            "\n",
            "    accuracy                           0.75     14139\n",
            "   macro avg       0.75      0.75      0.75     14139\n",
            "weighted avg       0.75      0.75      0.75     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logisticRegressionModel(standardized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxiEcthP8uY3",
        "outputId": "1bb4fd09-18e8-4c1b-94f2-88b226e49cb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 35 epochs took 2 seconds\n",
            "[[ 0.1826531   0.14632449  0.10666609  0.26429747 -0.0020377   0.02104763\n",
            "   0.04085052 -0.00675311 -0.00880712 -0.01792627 -0.07395524  0.00629132\n",
            "   0.00203814  0.32646677 -0.02152548 -0.04053807  0.02956818  0.06543934\n",
            "   0.21726925 -0.01336101 -0.0640264 ]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing features\n",
        "Standardization helped the convergance of our model, but didn't countribute to the evaluation metrics. Now we try with normalized data."
      ],
      "metadata": {
        "id": "0MOiYC7bBXQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_normalize = list(x.keys())"
      ],
      "metadata": {
        "id": "bO4AayIo_wCw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "normalized_features = min_max_scaler.fit_transform(raw_dataset[columns_to_standardize])\n",
        "normalized_dataset = pd.DataFrame()\n",
        "normalized_dataset[\"Diabetes_binary\"] = raw_dataset[\"Diabetes_binary\"]\n",
        "normalized_dataset[columns_to_standardize] = normalized_features"
      ],
      "metadata": {
        "id": "0LIS3MbUBroL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradienBoostClassifierModel(normalized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGrBEi4iDFzt",
        "outputId": "e35f0c60-6bde-44b1-fe2d-5b09254717da"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "         1           1.3619           0.0245            9.55s\n",
            "         2           1.3400           0.0224            9.48s\n",
            "         3           1.3200           0.0203            9.33s\n",
            "         4           1.3008           0.0178            9.23s\n",
            "         5           1.2841           0.0163            9.08s\n",
            "         6           1.2685           0.0151            9.04s\n",
            "         7           1.2545           0.0136            8.95s\n",
            "         8           1.2417           0.0127            8.99s\n",
            "         9           1.2303           0.0117            8.87s\n",
            "        10           1.2191           0.0112            8.86s\n",
            "        11           1.2090           0.0101            8.78s\n",
            "        12           1.1982           0.0100            8.75s\n",
            "        13           1.1890           0.0094            8.67s\n",
            "        14           1.1793           0.0088            8.67s\n",
            "        15           1.1707           0.0078            8.56s\n",
            "        16           1.1641           0.0067            8.52s\n",
            "        17           1.1571           0.0076            8.48s\n",
            "        18           1.1502           0.0062            8.30s\n",
            "        19           1.1433           0.0058            8.13s\n",
            "        20           1.1379           0.0066            7.98s\n",
            "        21           1.1312           0.0059            7.82s\n",
            "        22           1.1278           0.0050            7.68s\n",
            "        23           1.1231           0.0050            7.56s\n",
            "        24           1.1165           0.0051            7.44s\n",
            "        25           1.1117           0.0040            7.32s\n",
            "        26           1.1087           0.0043            7.20s\n",
            "        27           1.1027           0.0045            7.08s\n",
            "        28           1.0968           0.0040            7.02s\n",
            "        29           1.0943           0.0034            6.94s\n",
            "        30           1.0898           0.0036            6.86s\n",
            "        31           1.0874           0.0029            6.76s\n",
            "        32           1.0858           0.0034            6.68s\n",
            "        33           1.0806           0.0030            6.59s\n",
            "        34           1.0780           0.0027            6.50s\n",
            "        35           1.0782           0.0027            6.42s\n",
            "        36           1.0716           0.0025            6.33s\n",
            "        37           1.0690           0.0025            6.26s\n",
            "        38           1.0686           0.0026            6.18s\n",
            "        39           1.0640           0.0022            6.10s\n",
            "        40           1.0635           0.0019            6.03s\n",
            "        41           1.0597           0.0017            5.96s\n",
            "        42           1.0594           0.0019            5.90s\n",
            "        43           1.0554           0.0017            5.83s\n",
            "        44           1.0557           0.0017            5.75s\n",
            "        45           1.0558           0.0018            5.68s\n",
            "        46           1.0505           0.0015            5.61s\n",
            "        47           1.0505           0.0018            5.54s\n",
            "        48           1.0462           0.0015            5.46s\n",
            "        49           1.0476           0.0012            5.43s\n",
            "        50           1.0454           0.0014            5.41s\n",
            "        51           1.0443           0.0012            5.35s\n",
            "        52           1.0434           0.0012            5.29s\n",
            "        53           1.0391           0.0011            5.22s\n",
            "        54           1.0407           0.0011            5.16s\n",
            "        55           1.0360           0.0011            5.09s\n",
            "        56           1.0381           0.0011            5.03s\n",
            "        57           1.0365           0.0010            4.97s\n",
            "        58           1.0363           0.0012            4.91s\n",
            "        59           1.0311           0.0008            4.84s\n",
            "        60           1.0329           0.0010            4.78s\n",
            "        61           1.0322           0.0009            4.72s\n",
            "        62           1.0295           0.0009            4.66s\n",
            "        63           1.0297           0.0009            4.61s\n",
            "        64           1.0304           0.0007            4.54s\n",
            "        65           1.0277           0.0008            4.49s\n",
            "        66           1.0294           0.0007            4.43s\n",
            "        67           1.0280           0.0009            4.37s\n",
            "        68           1.0247           0.0006            4.32s\n",
            "        69           1.0263           0.0006            4.27s\n",
            "        70           1.0220           0.0005            4.23s\n",
            "        71           1.0246           0.0006            4.18s\n",
            "        72           1.0214           0.0007            4.11s\n",
            "        73           1.0221           0.0007            4.06s\n",
            "        74           1.0198           0.0004            4.00s\n",
            "        75           1.0226           0.0006            3.94s\n",
            "        76           1.0190           0.0004            3.88s\n",
            "        77           1.0218           0.0006            3.82s\n",
            "        78           1.0171           0.0005            3.77s\n",
            "        79           1.0174           0.0004            3.71s\n",
            "        80           1.0173           0.0003            3.65s\n",
            "        81           1.0168           0.0005            3.59s\n",
            "        82           1.0148           0.0004            3.54s\n",
            "        83           1.0181           0.0004            3.48s\n",
            "        84           1.0173           0.0003            3.43s\n",
            "        85           1.0159           0.0003            3.37s\n",
            "        86           1.0146           0.0004            3.31s\n",
            "        87           1.0181           0.0003            3.26s\n",
            "        88           1.0144           0.0004            3.21s\n",
            "        89           1.0153           0.0003            3.15s\n",
            "        90           1.0082           0.0001            3.09s\n",
            "        91           1.0150           0.0004            3.04s\n",
            "        92           1.0121           0.0002            2.99s\n",
            "        93           1.0112           0.0003            2.94s\n",
            "        94           1.0107           0.0003            2.88s\n",
            "        95           1.0098           0.0002            2.83s\n",
            "        96           1.0082           0.0003            2.77s\n",
            "        97           1.0098           0.0003            2.72s\n",
            "        98           1.0100           0.0003            2.67s\n",
            "        99           1.0104           0.0001            2.61s\n",
            "       100           1.0070           0.0002            2.56s\n",
            "       101           1.0056           0.0001            2.51s\n",
            "       102           1.0088           0.0001            2.45s\n",
            "       103           1.0087           0.0002            2.40s\n",
            "       104           1.0112           0.0002            2.34s\n",
            "       105           1.0057           0.0002            2.29s\n",
            "       106           1.0039           0.0002            2.24s\n",
            "       107           1.0075           0.0001            2.18s\n",
            "       108           1.0104           0.0002            2.13s\n",
            "       109           1.0068           0.0001            2.08s\n",
            "       110           1.0056           0.0001            2.02s\n",
            "       111           1.0090           0.0001            1.97s\n",
            "       112           1.0080           0.0001            1.92s\n",
            "       113           1.0031           0.0001            1.87s\n",
            "       114           1.0058           0.0002            1.82s\n",
            "       115           1.0075           0.0002            1.76s\n",
            "       116           1.0070           0.0001            1.71s\n",
            "       117           1.0035           0.0001            1.66s\n",
            "       118           1.0052           0.0001            1.61s\n",
            "       119           1.0044           0.0001            1.56s\n",
            "       120           1.0032           0.0001            1.51s\n",
            "       121           1.0045           0.0001            1.46s\n",
            "       122           1.0009           0.0001            1.40s\n",
            "       123           1.0028           0.0001            1.35s\n",
            "       124           1.0061           0.0002            1.30s\n",
            "       125           0.9986           0.0000            1.25s\n",
            "       126           1.0018           0.0001            1.20s\n",
            "       127           1.0029           0.0001            1.15s\n",
            "       128           1.0038           0.0000            1.10s\n",
            "       129           1.0029           0.0000            1.05s\n",
            "       130           1.0039           0.0001            1.00s\n",
            "       131           1.0027           0.0001            0.95s\n",
            "       132           1.0010           0.0001            0.90s\n",
            "       133           1.0008          -0.0000            0.85s\n",
            "       134           1.0019          -0.0000            0.80s\n",
            "       135           1.0023           0.0000            0.75s\n",
            "       136           1.0003           0.0000            0.70s\n",
            "       137           1.0014           0.0001            0.65s\n",
            "       138           1.0026           0.0000            0.60s\n",
            "       139           0.9997           0.0001            0.55s\n",
            "       140           1.0030           0.0000            0.50s\n",
            "       141           0.9968           0.0000            0.45s\n",
            "       142           1.0005           0.0000            0.40s\n",
            "       143           1.0022           0.0000            0.35s\n",
            "       144           0.9993           0.0001            0.30s\n",
            "       145           1.0011           0.0000            0.25s\n",
            "       146           1.0000           0.0000            0.20s\n",
            "       147           0.9979           0.0000            0.15s\n",
            "       148           1.0005           0.0000            0.10s\n",
            "       149           1.0005           0.0000            0.05s\n",
            "       150           0.9998          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.73      7010\n",
            "         1.0       0.73      0.78      0.76      7129\n",
            "\n",
            "    accuracy                           0.75     14139\n",
            "   macro avg       0.75      0.75      0.75     14139\n",
            "weighted avg       0.75      0.75      0.75     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logisticRegressionModel(normalized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQZQbxroB6V3",
        "outputId": "0f76a212-f6f3-46c9-9059-21a58ee7cfa1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 38 epochs took 1 seconds\n",
            "[[ 0.36918431  0.29307524  0.68381231  3.15904998 -0.00423009  0.08667325\n",
            "   0.11512165 -0.01518912 -0.01825656 -0.04387366 -0.36555518  0.03063572\n",
            "   0.00687011  1.17187081 -0.07924642 -0.1206813   0.06894083  0.13120518\n",
            "   0.91025488 -0.06528242 -0.2056759 ]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What happened?\n",
        "It turned out that algorithms like Logitstic regression and Gradientboost are robust to data scale due to a number of factors like their loss functions, use of decision trees and regularization factors, etc. So we have to find another way to reach our goal.\n",
        "\n",
        "# Next model: DNN\n",
        "neural networks are the master of finding complex relations between featurse. In addition to that, they can be combined with various functionalities to improve model's behavoir even further, e.g. optimizers, regularization factors, etc."
      ],
      "metadata": {
        "id": "UN0ILPX_Bh9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dnnModel(dataset):\n",
        "  x_train, x_test, y_train, y_test = get_data(dataset)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, input_dim=x_train.shape[1], activation=LeakyReLU(alpha=0.1), kernel_initializer=he_normal()))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(32, activation='relu', kernel_regularizer=l1(0.1)))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  adam = Adagrad(learning_rate=0.1)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[Precision(), Recall()])\n",
        "  model.fit(x_train, y_train, epochs=100, verbose=2, validation_split=0.1, batch_size=100,)\n",
        "  res = model.evaluate(x_test, y_test)\n",
        "  print(\"binary cross-entropy loss : \", res[0], \" precision: \", res[1], \" recal: \", res[2])"
      ],
      "metadata": {
        "id": "FcQCSaw-665S"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnnModel(standardized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPxwqurHQ7eu",
        "outputId": "7a0fc856-3201-42dd-d618-f05cf5408710"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "509/509 - 1s - loss: 2.3903 - precision_3: 0.6628 - recall_3: 0.6294 - val_loss: 1.4111 - val_precision_3: 0.7265 - val_recall_3: 0.7981 - 1s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "509/509 - 1s - loss: 1.2994 - precision_3: 0.7163 - recall_3: 0.7952 - val_loss: 1.1808 - val_precision_3: 0.7140 - val_recall_3: 0.8249 - 650ms/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "509/509 - 1s - loss: 1.1139 - precision_3: 0.7187 - recall_3: 0.7987 - val_loss: 1.0265 - val_precision_3: 0.7211 - val_recall_3: 0.8006 - 683ms/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "509/509 - 1s - loss: 1.0158 - precision_3: 0.7204 - recall_3: 0.8041 - val_loss: 0.9819 - val_precision_3: 0.7178 - val_recall_3: 0.8142 - 651ms/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "509/509 - 1s - loss: 0.9505 - precision_3: 0.7198 - recall_3: 0.8015 - val_loss: 0.8984 - val_precision_3: 0.7191 - val_recall_3: 0.8092 - 668ms/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "509/509 - 1s - loss: 0.9063 - precision_3: 0.7223 - recall_3: 0.8061 - val_loss: 0.8985 - val_precision_3: 0.7012 - val_recall_3: 0.8496 - 650ms/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "509/509 - 1s - loss: 0.8671 - precision_3: 0.7225 - recall_3: 0.8046 - val_loss: 0.8323 - val_precision_3: 0.7088 - val_recall_3: 0.8332 - 636ms/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "509/509 - 1s - loss: 0.8415 - precision_3: 0.7232 - recall_3: 0.8061 - val_loss: 0.8290 - val_precision_3: 0.7294 - val_recall_3: 0.7873 - 691ms/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "509/509 - 1s - loss: 0.8239 - precision_3: 0.7251 - recall_3: 0.8050 - val_loss: 0.7906 - val_precision_3: 0.7172 - val_recall_3: 0.8163 - 655ms/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "509/509 - 1s - loss: 0.8066 - precision_3: 0.7231 - recall_3: 0.8069 - val_loss: 0.7990 - val_precision_3: 0.7251 - val_recall_3: 0.7963 - 643ms/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "509/509 - 1s - loss: 0.7919 - precision_3: 0.7239 - recall_3: 0.8040 - val_loss: 0.7638 - val_precision_3: 0.7084 - val_recall_3: 0.8332 - 672ms/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "509/509 - 1s - loss: 0.7755 - precision_3: 0.7236 - recall_3: 0.8046 - val_loss: 0.7726 - val_precision_3: 0.7198 - val_recall_3: 0.8120 - 661ms/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "509/509 - 1s - loss: 0.7612 - precision_3: 0.7249 - recall_3: 0.8083 - val_loss: 0.7457 - val_precision_3: 0.7384 - val_recall_3: 0.7608 - 657ms/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "509/509 - 1s - loss: 0.7479 - precision_3: 0.7242 - recall_3: 0.8077 - val_loss: 0.7399 - val_precision_3: 0.7202 - val_recall_3: 0.8102 - 783ms/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "509/509 - 1s - loss: 0.7351 - precision_3: 0.7251 - recall_3: 0.8052 - val_loss: 0.7116 - val_precision_3: 0.7220 - val_recall_3: 0.8052 - 849ms/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "509/509 - 1s - loss: 0.7250 - precision_3: 0.7241 - recall_3: 0.8073 - val_loss: 0.7200 - val_precision_3: 0.7274 - val_recall_3: 0.7948 - 940ms/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "509/509 - 1s - loss: 0.7203 - precision_3: 0.7255 - recall_3: 0.8062 - val_loss: 0.6966 - val_precision_3: 0.7242 - val_recall_3: 0.7991 - 798ms/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "509/509 - 1s - loss: 0.7178 - precision_3: 0.7235 - recall_3: 0.8082 - val_loss: 0.7109 - val_precision_3: 0.7247 - val_recall_3: 0.7963 - 668ms/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "509/509 - 1s - loss: 0.7117 - precision_3: 0.7246 - recall_3: 0.8065 - val_loss: 0.6886 - val_precision_3: 0.7202 - val_recall_3: 0.8056 - 649ms/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "509/509 - 1s - loss: 0.7061 - precision_3: 0.7254 - recall_3: 0.8057 - val_loss: 0.7060 - val_precision_3: 0.7182 - val_recall_3: 0.8113 - 656ms/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "509/509 - 1s - loss: 0.7025 - precision_3: 0.7253 - recall_3: 0.8058 - val_loss: 0.6819 - val_precision_3: 0.7170 - val_recall_3: 0.8127 - 692ms/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "509/509 - 1s - loss: 0.6969 - precision_3: 0.7238 - recall_3: 0.8078 - val_loss: 0.6908 - val_precision_3: 0.7178 - val_recall_3: 0.8106 - 669ms/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "509/509 - 1s - loss: 0.6898 - precision_3: 0.7273 - recall_3: 0.8091 - val_loss: 0.6716 - val_precision_3: 0.7140 - val_recall_3: 0.8206 - 641ms/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "509/509 - 1s - loss: 0.6832 - precision_3: 0.7264 - recall_3: 0.8078 - val_loss: 0.6809 - val_precision_3: 0.7259 - val_recall_3: 0.7974 - 651ms/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "509/509 - 1s - loss: 0.6794 - precision_3: 0.7256 - recall_3: 0.8084 - val_loss: 0.6619 - val_precision_3: 0.7126 - val_recall_3: 0.8238 - 631ms/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "509/509 - 1s - loss: 0.6726 - precision_3: 0.7260 - recall_3: 0.8082 - val_loss: 0.6718 - val_precision_3: 0.7202 - val_recall_3: 0.8074 - 654ms/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "509/509 - 1s - loss: 0.6690 - precision_3: 0.7260 - recall_3: 0.8069 - val_loss: 0.6551 - val_precision_3: 0.7175 - val_recall_3: 0.8120 - 683ms/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "509/509 - 1s - loss: 0.6659 - precision_3: 0.7261 - recall_3: 0.8060 - val_loss: 0.6619 - val_precision_3: 0.7198 - val_recall_3: 0.8049 - 637ms/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "509/509 - 1s - loss: 0.6610 - precision_3: 0.7254 - recall_3: 0.8091 - val_loss: 0.6509 - val_precision_3: 0.7068 - val_recall_3: 0.8389 - 624ms/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "509/509 - 1s - loss: 0.6607 - precision_3: 0.7255 - recall_3: 0.8076 - val_loss: 0.6658 - val_precision_3: 0.7160 - val_recall_3: 0.8142 - 650ms/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "509/509 - 1s - loss: 0.6613 - precision_3: 0.7273 - recall_3: 0.8075 - val_loss: 0.6482 - val_precision_3: 0.7093 - val_recall_3: 0.8360 - 661ms/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "509/509 - 1s - loss: 0.6597 - precision_3: 0.7252 - recall_3: 0.8094 - val_loss: 0.6573 - val_precision_3: 0.7145 - val_recall_3: 0.8170 - 776ms/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "509/509 - 1s - loss: 0.6559 - precision_3: 0.7263 - recall_3: 0.8093 - val_loss: 0.6426 - val_precision_3: 0.7152 - val_recall_3: 0.8181 - 934ms/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "509/509 - 1s - loss: 0.6516 - precision_3: 0.7261 - recall_3: 0.8074 - val_loss: 0.6513 - val_precision_3: 0.7178 - val_recall_3: 0.8095 - 849ms/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "509/509 - 1s - loss: 0.6502 - precision_3: 0.7262 - recall_3: 0.8084 - val_loss: 0.6341 - val_precision_3: 0.7214 - val_recall_3: 0.8038 - 841ms/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "509/509 - 1s - loss: 0.6496 - precision_3: 0.7255 - recall_3: 0.8085 - val_loss: 0.6487 - val_precision_3: 0.7173 - val_recall_3: 0.8084 - 663ms/epoch - 1ms/step\n",
            "Epoch 37/100\n",
            "509/509 - 1s - loss: 0.6454 - precision_3: 0.7254 - recall_3: 0.8081 - val_loss: 0.6272 - val_precision_3: 0.7146 - val_recall_3: 0.8142 - 685ms/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "509/509 - 1s - loss: 0.6426 - precision_3: 0.7261 - recall_3: 0.8080 - val_loss: 0.6440 - val_precision_3: 0.7134 - val_recall_3: 0.8217 - 686ms/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "509/509 - 1s - loss: 0.6405 - precision_3: 0.7266 - recall_3: 0.8094 - val_loss: 0.6279 - val_precision_3: 0.7077 - val_recall_3: 0.8357 - 640ms/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "509/509 - 1s - loss: 0.6387 - precision_3: 0.7271 - recall_3: 0.8094 - val_loss: 0.6427 - val_precision_3: 0.7269 - val_recall_3: 0.7959 - 680ms/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "509/509 - 1s - loss: 0.6360 - precision_3: 0.7260 - recall_3: 0.8078 - val_loss: 0.6136 - val_precision_3: 0.7151 - val_recall_3: 0.8135 - 659ms/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "509/509 - 1s - loss: 0.6325 - precision_3: 0.7266 - recall_3: 0.8107 - val_loss: 0.6332 - val_precision_3: 0.7143 - val_recall_3: 0.8192 - 667ms/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "509/509 - 1s - loss: 0.6279 - precision_3: 0.7272 - recall_3: 0.8070 - val_loss: 0.6096 - val_precision_3: 0.7147 - val_recall_3: 0.8170 - 659ms/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "509/509 - 1s - loss: 0.6250 - precision_3: 0.7279 - recall_3: 0.8086 - val_loss: 0.6297 - val_precision_3: 0.7186 - val_recall_3: 0.8081 - 653ms/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "509/509 - 1s - loss: 0.6239 - precision_3: 0.7255 - recall_3: 0.8085 - val_loss: 0.6051 - val_precision_3: 0.7227 - val_recall_3: 0.8042 - 689ms/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "509/509 - 1s - loss: 0.6210 - precision_3: 0.7266 - recall_3: 0.8110 - val_loss: 0.6271 - val_precision_3: 0.7194 - val_recall_3: 0.8059 - 689ms/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "509/509 - 1s - loss: 0.6202 - precision_3: 0.7268 - recall_3: 0.8071 - val_loss: 0.5968 - val_precision_3: 0.7241 - val_recall_3: 0.8016 - 652ms/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "509/509 - 1s - loss: 0.6178 - precision_3: 0.7266 - recall_3: 0.8105 - val_loss: 0.6225 - val_precision_3: 0.7230 - val_recall_3: 0.8016 - 742ms/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "509/509 - 1s - loss: 0.6179 - precision_3: 0.7271 - recall_3: 0.8088 - val_loss: 0.6011 - val_precision_3: 0.7243 - val_recall_3: 0.8006 - 674ms/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "509/509 - 1s - loss: 0.6160 - precision_3: 0.7277 - recall_3: 0.8072 - val_loss: 0.6163 - val_precision_3: 0.7176 - val_recall_3: 0.8099 - 833ms/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "509/509 - 1s - loss: 0.6138 - precision_3: 0.7277 - recall_3: 0.8087 - val_loss: 0.6000 - val_precision_3: 0.7152 - val_recall_3: 0.8192 - 861ms/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "509/509 - 1s - loss: 0.6132 - precision_3: 0.7276 - recall_3: 0.8086 - val_loss: 0.6185 - val_precision_3: 0.7205 - val_recall_3: 0.8056 - 882ms/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "509/509 - 1s - loss: 0.6101 - precision_3: 0.7284 - recall_3: 0.8083 - val_loss: 0.5958 - val_precision_3: 0.7234 - val_recall_3: 0.8034 - 827ms/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "509/509 - 1s - loss: 0.6076 - precision_3: 0.7276 - recall_3: 0.8059 - val_loss: 0.6096 - val_precision_3: 0.7085 - val_recall_3: 0.8389 - 673ms/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "509/509 - 1s - loss: 0.6053 - precision_3: 0.7262 - recall_3: 0.8077 - val_loss: 0.5922 - val_precision_3: 0.7252 - val_recall_3: 0.7984 - 671ms/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "509/509 - 1s - loss: 0.6058 - precision_3: 0.7271 - recall_3: 0.8064 - val_loss: 0.6059 - val_precision_3: 0.7138 - val_recall_3: 0.8206 - 659ms/epoch - 1ms/step\n",
            "Epoch 57/100\n",
            "509/509 - 1s - loss: 0.6024 - precision_3: 0.7279 - recall_3: 0.8081 - val_loss: 0.5886 - val_precision_3: 0.7180 - val_recall_3: 0.8084 - 671ms/epoch - 1ms/step\n",
            "Epoch 58/100\n",
            "509/509 - 1s - loss: 0.6012 - precision_3: 0.7261 - recall_3: 0.8086 - val_loss: 0.6001 - val_precision_3: 0.7166 - val_recall_3: 0.8131 - 682ms/epoch - 1ms/step\n",
            "Epoch 59/100\n",
            "509/509 - 1s - loss: 0.6009 - precision_3: 0.7281 - recall_3: 0.8087 - val_loss: 0.5835 - val_precision_3: 0.7233 - val_recall_3: 0.8013 - 661ms/epoch - 1ms/step\n",
            "Epoch 60/100\n",
            "509/509 - 1s - loss: 0.6011 - precision_3: 0.7288 - recall_3: 0.8086 - val_loss: 0.6035 - val_precision_3: 0.7215 - val_recall_3: 0.8042 - 658ms/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "509/509 - 1s - loss: 0.6010 - precision_3: 0.7282 - recall_3: 0.8085 - val_loss: 0.5852 - val_precision_3: 0.7128 - val_recall_3: 0.8228 - 678ms/epoch - 1ms/step\n",
            "Epoch 62/100\n",
            "509/509 - 1s - loss: 0.5977 - precision_3: 0.7278 - recall_3: 0.8068 - val_loss: 0.5976 - val_precision_3: 0.7221 - val_recall_3: 0.8027 - 659ms/epoch - 1ms/step\n",
            "Epoch 63/100\n",
            "509/509 - 1s - loss: 0.5958 - precision_3: 0.7276 - recall_3: 0.8091 - val_loss: 0.5797 - val_precision_3: 0.7160 - val_recall_3: 0.8117 - 692ms/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "509/509 - 1s - loss: 0.5939 - precision_3: 0.7264 - recall_3: 0.8095 - val_loss: 0.5965 - val_precision_3: 0.7178 - val_recall_3: 0.8095 - 665ms/epoch - 1ms/step\n",
            "Epoch 65/100\n",
            "509/509 - 1s - loss: 0.5923 - precision_3: 0.7283 - recall_3: 0.8079 - val_loss: 0.5752 - val_precision_3: 0.7206 - val_recall_3: 0.8052 - 651ms/epoch - 1ms/step\n",
            "Epoch 66/100\n",
            "509/509 - 1s - loss: 0.5907 - precision_3: 0.7271 - recall_3: 0.8088 - val_loss: 0.5920 - val_precision_3: 0.7142 - val_recall_3: 0.8195 - 660ms/epoch - 1ms/step\n",
            "Epoch 67/100\n",
            "509/509 - 1s - loss: 0.5885 - precision_3: 0.7285 - recall_3: 0.8090 - val_loss: 0.5736 - val_precision_3: 0.7146 - val_recall_3: 0.8195 - 684ms/epoch - 1ms/step\n",
            "Epoch 68/100\n",
            "509/509 - 1s - loss: 0.5886 - precision_3: 0.7280 - recall_3: 0.8061 - val_loss: 0.5919 - val_precision_3: 0.7170 - val_recall_3: 0.8127 - 920ms/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "509/509 - 1s - loss: 0.5872 - precision_3: 0.7270 - recall_3: 0.8082 - val_loss: 0.5699 - val_precision_3: 0.7179 - val_recall_3: 0.8081 - 861ms/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "509/509 - 1s - loss: 0.5864 - precision_3: 0.7264 - recall_3: 0.8105 - val_loss: 0.5915 - val_precision_3: 0.7197 - val_recall_3: 0.8063 - 909ms/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "509/509 - 1s - loss: 0.5877 - precision_3: 0.7261 - recall_3: 0.8070 - val_loss: 0.5700 - val_precision_3: 0.7140 - val_recall_3: 0.8278 - 732ms/epoch - 1ms/step\n",
            "Epoch 72/100\n",
            "509/509 - 1s - loss: 0.5850 - precision_3: 0.7272 - recall_3: 0.8097 - val_loss: 0.5871 - val_precision_3: 0.7244 - val_recall_3: 0.8009 - 683ms/epoch - 1ms/step\n",
            "Epoch 73/100\n",
            "509/509 - 1s - loss: 0.5836 - precision_3: 0.7263 - recall_3: 0.8077 - val_loss: 0.5651 - val_precision_3: 0.7167 - val_recall_3: 0.8106 - 691ms/epoch - 1ms/step\n",
            "Epoch 74/100\n",
            "509/509 - 1s - loss: 0.5812 - precision_3: 0.7295 - recall_3: 0.8054 - val_loss: 0.5845 - val_precision_3: 0.7163 - val_recall_3: 0.8117 - 667ms/epoch - 1ms/step\n",
            "Epoch 75/100\n",
            "509/509 - 1s - loss: 0.5810 - precision_3: 0.7285 - recall_3: 0.8066 - val_loss: 0.5644 - val_precision_3: 0.7224 - val_recall_3: 0.8024 - 658ms/epoch - 1ms/step\n",
            "Epoch 76/100\n",
            "509/509 - 1s - loss: 0.5813 - precision_3: 0.7279 - recall_3: 0.8064 - val_loss: 0.5834 - val_precision_3: 0.7141 - val_recall_3: 0.8246 - 631ms/epoch - 1ms/step\n",
            "Epoch 77/100\n",
            "509/509 - 1s - loss: 0.5795 - precision_3: 0.7274 - recall_3: 0.8084 - val_loss: 0.5624 - val_precision_3: 0.7188 - val_recall_3: 0.8081 - 685ms/epoch - 1ms/step\n",
            "Epoch 78/100\n",
            "509/509 - 1s - loss: 0.5769 - precision_3: 0.7281 - recall_3: 0.8066 - val_loss: 0.5780 - val_precision_3: 0.7210 - val_recall_3: 0.8042 - 660ms/epoch - 1ms/step\n",
            "Epoch 79/100\n",
            "509/509 - 1s - loss: 0.5767 - precision_3: 0.7265 - recall_3: 0.8066 - val_loss: 0.5605 - val_precision_3: 0.7269 - val_recall_3: 0.7966 - 653ms/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "509/509 - 1s - loss: 0.5746 - precision_3: 0.7281 - recall_3: 0.8068 - val_loss: 0.5754 - val_precision_3: 0.7282 - val_recall_3: 0.7952 - 665ms/epoch - 1ms/step\n",
            "Epoch 81/100\n",
            "509/509 - 1s - loss: 0.5725 - precision_3: 0.7286 - recall_3: 0.8090 - val_loss: 0.5564 - val_precision_3: 0.7190 - val_recall_3: 0.8088 - 644ms/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "509/509 - 1s - loss: 0.5724 - precision_3: 0.7284 - recall_3: 0.8083 - val_loss: 0.5777 - val_precision_3: 0.7177 - val_recall_3: 0.8102 - 649ms/epoch - 1ms/step\n",
            "Epoch 83/100\n",
            "509/509 - 1s - loss: 0.5726 - precision_3: 0.7275 - recall_3: 0.8085 - val_loss: 0.5535 - val_precision_3: 0.7194 - val_recall_3: 0.8088 - 675ms/epoch - 1ms/step\n",
            "Epoch 84/100\n",
            "509/509 - 1s - loss: 0.5709 - precision_3: 0.7281 - recall_3: 0.8068 - val_loss: 0.5754 - val_precision_3: 0.7225 - val_recall_3: 0.8034 - 667ms/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "509/509 - 1s - loss: 0.5708 - precision_3: 0.7269 - recall_3: 0.8091 - val_loss: 0.5563 - val_precision_3: 0.7199 - val_recall_3: 0.8088 - 655ms/epoch - 1ms/step\n",
            "Epoch 86/100\n",
            "509/509 - 1s - loss: 0.5708 - precision_3: 0.7282 - recall_3: 0.8092 - val_loss: 0.5722 - val_precision_3: 0.7178 - val_recall_3: 0.8124 - 875ms/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "509/509 - 1s - loss: 0.5682 - precision_3: 0.7274 - recall_3: 0.8079 - val_loss: 0.5531 - val_precision_3: 0.7274 - val_recall_3: 0.7959 - 949ms/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "509/509 - 1s - loss: 0.5672 - precision_3: 0.7269 - recall_3: 0.8086 - val_loss: 0.5690 - val_precision_3: 0.7219 - val_recall_3: 0.8038 - 951ms/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "509/509 - 1s - loss: 0.5657 - precision_3: 0.7287 - recall_3: 0.8068 - val_loss: 0.5495 - val_precision_3: 0.7211 - val_recall_3: 0.8045 - 663ms/epoch - 1ms/step\n",
            "Epoch 90/100\n",
            "509/509 - 1s - loss: 0.5648 - precision_3: 0.7283 - recall_3: 0.8081 - val_loss: 0.5678 - val_precision_3: 0.7291 - val_recall_3: 0.7948 - 640ms/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "509/509 - 1s - loss: 0.5648 - precision_3: 0.7282 - recall_3: 0.8071 - val_loss: 0.5480 - val_precision_3: 0.7176 - val_recall_3: 0.8124 - 650ms/epoch - 1ms/step\n",
            "Epoch 92/100\n",
            "509/509 - 1s - loss: 0.5622 - precision_3: 0.7296 - recall_3: 0.8048 - val_loss: 0.5659 - val_precision_3: 0.7171 - val_recall_3: 0.8131 - 654ms/epoch - 1ms/step\n",
            "Epoch 93/100\n",
            "509/509 - 1s - loss: 0.5634 - precision_3: 0.7266 - recall_3: 0.8112 - val_loss: 0.5554 - val_precision_3: 0.7166 - val_recall_3: 0.8221 - 656ms/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "509/509 - 1s - loss: 0.5655 - precision_3: 0.7289 - recall_3: 0.8083 - val_loss: 0.5633 - val_precision_3: 0.7235 - val_recall_3: 0.8038 - 656ms/epoch - 1ms/step\n",
            "Epoch 95/100\n",
            "509/509 - 1s - loss: 0.5651 - precision_3: 0.7282 - recall_3: 0.8070 - val_loss: 0.5550 - val_precision_3: 0.7158 - val_recall_3: 0.8253 - 651ms/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "509/509 - 1s - loss: 0.5630 - precision_3: 0.7269 - recall_3: 0.8099 - val_loss: 0.5613 - val_precision_3: 0.7276 - val_recall_3: 0.7948 - 641ms/epoch - 1ms/step\n",
            "Epoch 97/100\n",
            "509/509 - 1s - loss: 0.5615 - precision_3: 0.7284 - recall_3: 0.8087 - val_loss: 0.5496 - val_precision_3: 0.7231 - val_recall_3: 0.8024 - 677ms/epoch - 1ms/step\n",
            "Epoch 98/100\n",
            "509/509 - 1s - loss: 0.5605 - precision_3: 0.7284 - recall_3: 0.8083 - val_loss: 0.5605 - val_precision_3: 0.7207 - val_recall_3: 0.8092 - 667ms/epoch - 1ms/step\n",
            "Epoch 99/100\n",
            "509/509 - 1s - loss: 0.5616 - precision_3: 0.7297 - recall_3: 0.8089 - val_loss: 0.5488 - val_precision_3: 0.7174 - val_recall_3: 0.8153 - 678ms/epoch - 1ms/step\n",
            "Epoch 100/100\n",
            "509/509 - 1s - loss: 0.5593 - precision_3: 0.7281 - recall_3: 0.8092 - val_loss: 0.5606 - val_precision_3: 0.7281 - val_recall_3: 0.7931 - 627ms/epoch - 1ms/step\n",
            "442/442 [==============================] - 0s 1ms/step - loss: 0.5599 - precision_3: 0.7393 - recall_3: 0.7798\n",
            "binary cross-entropy loss :  0.5599175095558167  precision:  0.7393270134925842  recal:  0.7797727584838867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result\n",
        "As we saw, different models are not showing significant result improvements. So we may need to make some changes to our data"
      ],
      "metadata": {
        "id": "dAVWhISP03jL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The correlation matrix and its usage\n",
        "Correlation matrix simply explains the relationship between columns of a dataset. The correlation coefficient ranges between -1 and 1. A correlation coefficient of 1 indicates a perfect positive correlation, meaning that the two variables increase or decrease together in a linear relationship. A correlation coefficient of -1 indicates a perfect negative correlation, meaning that the two variables move in opposite directions in a linear relationship. A correlation coefficient close to 0 suggests no linear relationship between the variables.\n",
        "\n",
        "This matrix can be helpful when finding an optimal subset of features."
      ],
      "metadata": {
        "id": "xLjc4tO9DSgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_correlations(correlation_matrix):\n",
        "  return sorted(correlation_matrix.items(), key=lambda x:abs(x[1]))"
      ],
      "metadata": {
        "id": "-8WPCmWe4cF7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correlations(dataset):\n",
        "  columns = dataset.keys()\n",
        "  correlation = dataset[columns].corr()\n",
        "  return correlation[\"Diabetes_binary\"]"
      ],
      "metadata": {
        "id": "nAo1fc4AMXdy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_map = sort_correlations(get_correlations(raw_dataset))"
      ],
      "metadata": {
        "id": "sxLDZpig1bFM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd1e-mUK2-IJ",
        "outputId": "70ea8efb-f87d-4ab4-d767-6c0dd8e415d8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('AnyHealthcare', 0.02319074853112824),\n",
              " ('NoDocbcCost', 0.040976573266643494),\n",
              " ('Sex', 0.044412858371260695),\n",
              " ('Fruits', -0.05407655628666651),\n",
              " ('Veggies', -0.07929314561269872),\n",
              " ('Smoker', 0.08599896420800192),\n",
              " ('MentHlth', 0.08702877147509416),\n",
              " ('HvyAlcoholConsump', -0.09485313995926549),\n",
              " ('CholCheck', 0.11538161710270915),\n",
              " ('Stroke', 0.12542678468516733),\n",
              " ('PhysActivity', -0.15866560486405157),\n",
              " ('Education', -0.17048063498806143),\n",
              " ('HeartDiseaseorAttack', 0.21152340436022687),\n",
              " ('PhysHlth', 0.21308101903810317),\n",
              " ('Income', -0.2244487149638171),\n",
              " ('DiffWalk', 0.272646006159808),\n",
              " ('Age', 0.27873806628188813),\n",
              " ('HighChol', 0.28921280708865016),\n",
              " ('BMI', 0.29337274476103575),\n",
              " ('HighBP', 0.3815155489073117),\n",
              " ('GenHlth', 0.4076115984949182),\n",
              " ('Diabetes_binary', 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keep_features = map(lambda b: b[0], filter(lambda a: abs(a[1]) > 0.25, correlation_map))"
      ],
      "metadata": {
        "id": "mgYZgcME11WI"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_dataset = raw_dataset[list(keep_features)]"
      ],
      "metadata": {
        "id": "uGKhE_F72VFN"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logisticRegressionModel(modified_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ1ed4oV2rZH",
        "outputId": "97476bd9-5dd3-4f8b-81b7-bf02c14dc184"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 38 epochs took 0 seconds\n",
            "[[0.0702186  0.08500237 0.29923042 0.03782045 0.39012718 0.30067847]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.73      0.74      7010\n",
            "         1.0       0.74      0.76      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature reduction result: Logistic regression\n",
        "By reducing the feature count using the correlation matrix and only keeping faetures that have more meaningful relationship with the target featurse, Logistic regression not only converged faster, it also kept its accuracy."
      ],
      "metadata": {
        "id": "-5gt8cFd4ZRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradienBoostClassifierModel(modified_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMTIIVdU5Grg",
        "outputId": "22316860-b572-447c-d373-bcaff41403ef"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.3619           0.0245            3.49s\n",
            "         2           1.3400           0.0224            3.49s\n",
            "         3           1.3200           0.0203            3.49s\n",
            "         4           1.3008           0.0178            3.51s\n",
            "         5           1.2841           0.0163            3.56s\n",
            "         6           1.2685           0.0151            3.56s\n",
            "         7           1.2545           0.0136            3.50s\n",
            "         8           1.2417           0.0127            3.46s\n",
            "         9           1.2303           0.0117            3.52s\n",
            "        10           1.2191           0.0112            3.48s\n",
            "        11           1.2090           0.0101            3.45s\n",
            "        12           1.1982           0.0100            3.43s\n",
            "        13           1.1890           0.0094            3.46s\n",
            "        14           1.1793           0.0088            3.46s\n",
            "        15           1.1707           0.0078            3.42s\n",
            "        16           1.1641           0.0067            3.38s\n",
            "        17           1.1571           0.0076            3.37s\n",
            "        18           1.1502           0.0062            3.35s\n",
            "        19           1.1433           0.0058            3.33s\n",
            "        20           1.1379           0.0066            3.30s\n",
            "        21           1.1312           0.0059            3.27s\n",
            "        22           1.1278           0.0050            3.23s\n",
            "        23           1.1231           0.0050            3.20s\n",
            "        24           1.1165           0.0051            3.17s\n",
            "        25           1.1116           0.0041            3.14s\n",
            "        26           1.1086           0.0043            3.12s\n",
            "        27           1.1027           0.0044            3.09s\n",
            "        28           1.0970           0.0040            3.06s\n",
            "        29           1.0944           0.0038            3.03s\n",
            "        30           1.0900           0.0036            3.00s\n",
            "        31           1.0874           0.0029            2.97s\n",
            "        32           1.0857           0.0036            2.94s\n",
            "        33           1.0808           0.0026            2.92s\n",
            "        34           1.0781           0.0026            2.89s\n",
            "        35           1.0782           0.0031            2.87s\n",
            "        36           1.0720           0.0021            2.84s\n",
            "        37           1.0696           0.0023            2.81s\n",
            "        38           1.0695           0.0021            2.79s\n",
            "        39           1.0648           0.0022            2.76s\n",
            "        40           1.0644           0.0019            2.73s\n",
            "        41           1.0604           0.0020            2.70s\n",
            "        42           1.0601           0.0017            2.68s\n",
            "        43           1.0566           0.0014            2.65s\n",
            "        44           1.0571           0.0015            2.63s\n",
            "        45           1.0577           0.0017            2.61s\n",
            "        46           1.0524           0.0013            2.58s\n",
            "        47           1.0527           0.0014            2.55s\n",
            "        48           1.0483           0.0014            2.52s\n",
            "        49           1.0504           0.0012            2.50s\n",
            "        50           1.0480           0.0011            2.47s\n",
            "        51           1.0470           0.0012            2.45s\n",
            "        52           1.0462           0.0011            2.42s\n",
            "        53           1.0420           0.0010            2.40s\n",
            "        54           1.0439           0.0010            2.38s\n",
            "        55           1.0393           0.0007            2.36s\n",
            "        56           1.0415           0.0008            2.33s\n",
            "        57           1.0402           0.0007            2.31s\n",
            "        58           1.0404           0.0008            2.28s\n",
            "        59           1.0355           0.0007            2.25s\n",
            "        60           1.0376           0.0007            2.23s\n",
            "        61           1.0370           0.0007            2.21s\n",
            "        62           1.0347           0.0006            2.18s\n",
            "        63           1.0350           0.0006            2.16s\n",
            "        64           1.0356           0.0006            2.13s\n",
            "        65           1.0330           0.0005            2.10s\n",
            "        66           1.0351           0.0005            2.08s\n",
            "        67           1.0345           0.0005            2.06s\n",
            "        68           1.0309           0.0004            2.03s\n",
            "        69           1.0330           0.0005            2.01s\n",
            "        70           1.0284           0.0004            1.98s\n",
            "        71           1.0312           0.0004            1.96s\n",
            "        72           1.0286           0.0004            1.93s\n",
            "        73           1.0294           0.0004            1.90s\n",
            "        74           1.0274           0.0003            1.88s\n",
            "        75           1.0304           0.0003            1.86s\n",
            "        76           1.0274           0.0002            1.84s\n",
            "        77           1.0306           0.0004            1.81s\n",
            "        78           1.0249           0.0003            1.78s\n",
            "        79           1.0261           0.0003            1.76s\n",
            "        80           1.0262           0.0003            1.74s\n",
            "        81           1.0256           0.0003            1.71s\n",
            "        82           1.0238           0.0002            1.69s\n",
            "        83           1.0270           0.0002            1.66s\n",
            "        84           1.0269           0.0003            1.63s\n",
            "        85           1.0256           0.0001            1.61s\n",
            "        86           1.0239           0.0002            1.58s\n",
            "        87           1.0283           0.0002            1.56s\n",
            "        88           1.0240           0.0002            1.53s\n",
            "        89           1.0250           0.0002            1.51s\n",
            "        90           1.0180           0.0001            1.48s\n",
            "        91           1.0255           0.0003            1.46s\n",
            "        92           1.0225           0.0002            1.43s\n",
            "        93           1.0221           0.0001            1.41s\n",
            "        94           1.0214           0.0001            1.38s\n",
            "        95           1.0207           0.0002            1.36s\n",
            "        96           1.0190           0.0001            1.34s\n",
            "        97           1.0218           0.0001            1.31s\n",
            "        98           1.0213           0.0001            1.29s\n",
            "        99           1.0222           0.0001            1.27s\n",
            "       100           1.0190           0.0001            1.24s\n",
            "       101           1.0176           0.0001            1.22s\n",
            "       102           1.0208           0.0000            1.19s\n",
            "       103           1.0218           0.0001            1.17s\n",
            "       104           1.0241           0.0001            1.14s\n",
            "       105           1.0176           0.0000            1.12s\n",
            "       106           1.0163          -0.0000            1.09s\n",
            "       107           1.0198           0.0000            1.07s\n",
            "       108           1.0230           0.0001            1.04s\n",
            "       109           1.0202           0.0000            1.02s\n",
            "       110           1.0191           0.0000            1.00s\n",
            "       111           1.0228           0.0000            0.97s\n",
            "       112           1.0212           0.0000            0.95s\n",
            "       113           1.0165           0.0000            0.92s\n",
            "       114           1.0194           0.0000            0.90s\n",
            "       115           1.0215           0.0001            0.87s\n",
            "       116           1.0212           0.0000            0.85s\n",
            "       117           1.0174          -0.0000            0.82s\n",
            "       118           1.0188          -0.0000            0.79s\n",
            "       119           1.0187           0.0000            0.77s\n",
            "       120           1.0172           0.0000            0.74s\n",
            "       121           1.0195           0.0000            0.72s\n",
            "       122           1.0155          -0.0000            0.70s\n",
            "       123           1.0172           0.0000            0.67s\n",
            "       124           1.0196           0.0000            0.65s\n",
            "       125           1.0133          -0.0000            0.62s\n",
            "       126           1.0162          -0.0000            0.60s\n",
            "       127           1.0184           0.0000            0.57s\n",
            "       128           1.0189          -0.0000            0.55s\n",
            "       129           1.0192           0.0000            0.52s\n",
            "       130           1.0197           0.0000            0.50s\n",
            "       131           1.0183          -0.0000            0.47s\n",
            "       132           1.0172          -0.0000            0.45s\n",
            "       133           1.0165          -0.0000            0.42s\n",
            "       134           1.0183          -0.0000            0.40s\n",
            "       135           1.0182          -0.0000            0.37s\n",
            "       136           1.0159          -0.0000            0.35s\n",
            "       137           1.0182           0.0000            0.32s\n",
            "       138           1.0194           0.0000            0.30s\n",
            "       139           1.0150          -0.0001            0.27s\n",
            "       140           1.0194           0.0000            0.25s\n",
            "       141           1.0140          -0.0000            0.22s\n",
            "       142           1.0169          -0.0000            0.20s\n",
            "       143           1.0190          -0.0000            0.17s\n",
            "       144           1.0157          -0.0000            0.15s\n",
            "       145           1.0176          -0.0000            0.12s\n",
            "       146           1.0167          -0.0000            0.10s\n",
            "       147           1.0149          -0.0000            0.07s\n",
            "       148           1.0178          -0.0000            0.05s\n",
            "       149           1.0178          -0.0000            0.02s\n",
            "       150           1.0166          -0.0000            0.00s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.70      0.73      7010\n",
            "         1.0       0.73      0.78      0.75      7129\n",
            "\n",
            "    accuracy                           0.74     14139\n",
            "   macro avg       0.74      0.74      0.74     14139\n",
            "weighted avg       0.74      0.74      0.74     14139\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature reduction result: GradientBoost\n",
        "GradientBoost was also capable of keeping its performance after feature reduction. It is worthy of noting that tuning hyperparameters had a mild effect on this model but it was negligible."
      ],
      "metadata": {
        "id": "lLWu9esz6ZYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnnModel(modified_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK9KPK-l6Y_S",
        "outputId": "bc02d9a6-7802-45fb-f908-6533b12c9854"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "509/509 - 3s - loss: 3.1584 - precision_4: 0.5469 - recall_4: 0.5559 - val_loss: 1.4402 - val_precision_4: 0.6672 - val_recall_4: 0.8528 - 3s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "509/509 - 1s - loss: 1.2957 - precision_4: 0.6091 - recall_4: 0.6824 - val_loss: 1.1607 - val_precision_4: 0.6665 - val_recall_4: 0.8829 - 666ms/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "509/509 - 1s - loss: 1.0902 - precision_4: 0.6516 - recall_4: 0.6901 - val_loss: 0.9929 - val_precision_4: 0.7289 - val_recall_4: 0.7537 - 665ms/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "509/509 - 1s - loss: 0.9712 - precision_4: 0.6779 - recall_4: 0.6978 - val_loss: 0.8886 - val_precision_4: 0.7095 - val_recall_4: 0.8149 - 658ms/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "509/509 - 1s - loss: 0.9002 - precision_4: 0.6904 - recall_4: 0.7011 - val_loss: 0.8506 - val_precision_4: 0.7366 - val_recall_4: 0.7501 - 659ms/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "509/509 - 1s - loss: 0.8531 - precision_4: 0.6990 - recall_4: 0.7087 - val_loss: 0.7980 - val_precision_4: 0.7094 - val_recall_4: 0.8059 - 655ms/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "509/509 - 1s - loss: 0.8208 - precision_4: 0.6986 - recall_4: 0.7142 - val_loss: 0.7776 - val_precision_4: 0.7341 - val_recall_4: 0.7540 - 638ms/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "509/509 - 1s - loss: 0.7922 - precision_4: 0.7038 - recall_4: 0.7125 - val_loss: 0.7382 - val_precision_4: 0.7244 - val_recall_4: 0.7830 - 663ms/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "509/509 - 1s - loss: 0.7733 - precision_4: 0.7073 - recall_4: 0.7149 - val_loss: 0.7440 - val_precision_4: 0.7533 - val_recall_4: 0.7096 - 669ms/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "509/509 - 1s - loss: 0.7582 - precision_4: 0.7054 - recall_4: 0.7124 - val_loss: 0.7138 - val_precision_4: 0.7302 - val_recall_4: 0.7587 - 640ms/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "509/509 - 1s - loss: 0.7458 - precision_4: 0.7094 - recall_4: 0.7192 - val_loss: 0.7127 - val_precision_4: 0.7066 - val_recall_4: 0.8138 - 669ms/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "509/509 - 1s - loss: 0.7371 - precision_4: 0.7089 - recall_4: 0.7164 - val_loss: 0.6987 - val_precision_4: 0.7479 - val_recall_4: 0.7172 - 648ms/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "509/509 - 1s - loss: 0.7291 - precision_4: 0.7088 - recall_4: 0.7206 - val_loss: 0.7003 - val_precision_4: 0.7368 - val_recall_4: 0.7426 - 677ms/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "509/509 - 1s - loss: 0.7183 - precision_4: 0.7111 - recall_4: 0.7226 - val_loss: 0.6787 - val_precision_4: 0.6996 - val_recall_4: 0.8314 - 660ms/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "509/509 - 1s - loss: 0.7085 - precision_4: 0.7097 - recall_4: 0.7212 - val_loss: 0.6809 - val_precision_4: 0.7339 - val_recall_4: 0.7515 - 786ms/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "509/509 - 1s - loss: 0.7030 - precision_4: 0.7125 - recall_4: 0.7236 - val_loss: 0.6643 - val_precision_4: 0.7182 - val_recall_4: 0.7938 - 859ms/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "509/509 - 1s - loss: 0.6958 - precision_4: 0.7129 - recall_4: 0.7247 - val_loss: 0.6713 - val_precision_4: 0.7521 - val_recall_4: 0.7007 - 926ms/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "509/509 - 1s - loss: 0.6882 - precision_4: 0.7126 - recall_4: 0.7285 - val_loss: 0.6568 - val_precision_4: 0.7062 - val_recall_4: 0.8178 - 769ms/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "509/509 - 1s - loss: 0.6872 - precision_4: 0.7116 - recall_4: 0.7232 - val_loss: 0.6585 - val_precision_4: 0.7312 - val_recall_4: 0.7587 - 665ms/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "509/509 - 1s - loss: 0.6796 - precision_4: 0.7137 - recall_4: 0.7275 - val_loss: 0.6463 - val_precision_4: 0.7041 - val_recall_4: 0.8256 - 656ms/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "509/509 - 1s - loss: 0.6738 - precision_4: 0.7131 - recall_4: 0.7296 - val_loss: 0.6481 - val_precision_4: 0.7291 - val_recall_4: 0.7669 - 644ms/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "509/509 - 1s - loss: 0.6694 - precision_4: 0.7152 - recall_4: 0.7312 - val_loss: 0.6375 - val_precision_4: 0.7200 - val_recall_4: 0.7863 - 644ms/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "509/509 - 1s - loss: 0.6651 - precision_4: 0.7132 - recall_4: 0.7294 - val_loss: 0.6395 - val_precision_4: 0.7069 - val_recall_4: 0.8153 - 673ms/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "509/509 - 1s - loss: 0.6617 - precision_4: 0.7138 - recall_4: 0.7301 - val_loss: 0.6299 - val_precision_4: 0.7327 - val_recall_4: 0.7576 - 646ms/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "509/509 - 1s - loss: 0.6588 - precision_4: 0.7118 - recall_4: 0.7290 - val_loss: 0.6328 - val_precision_4: 0.7043 - val_recall_4: 0.8264 - 645ms/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "509/509 - 1s - loss: 0.6548 - precision_4: 0.7139 - recall_4: 0.7306 - val_loss: 0.6230 - val_precision_4: 0.7206 - val_recall_4: 0.7913 - 642ms/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "509/509 - 1s - loss: 0.6506 - precision_4: 0.7130 - recall_4: 0.7326 - val_loss: 0.6248 - val_precision_4: 0.7140 - val_recall_4: 0.8045 - 631ms/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "509/509 - 1s - loss: 0.6490 - precision_4: 0.7135 - recall_4: 0.7333 - val_loss: 0.6191 - val_precision_4: 0.7094 - val_recall_4: 0.8092 - 654ms/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "509/509 - 1s - loss: 0.6477 - precision_4: 0.7136 - recall_4: 0.7328 - val_loss: 0.6241 - val_precision_4: 0.7014 - val_recall_4: 0.8324 - 654ms/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "509/509 - 1s - loss: 0.6428 - precision_4: 0.7163 - recall_4: 0.7368 - val_loss: 0.6140 - val_precision_4: 0.7277 - val_recall_4: 0.7712 - 647ms/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "509/509 - 1s - loss: 0.6382 - precision_4: 0.7150 - recall_4: 0.7390 - val_loss: 0.6156 - val_precision_4: 0.7319 - val_recall_4: 0.7565 - 645ms/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "509/509 - 1s - loss: 0.6387 - precision_4: 0.7124 - recall_4: 0.7351 - val_loss: 0.6122 - val_precision_4: 0.7360 - val_recall_4: 0.7515 - 677ms/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "509/509 - 1s - loss: 0.6372 - precision_4: 0.7130 - recall_4: 0.7327 - val_loss: 0.6115 - val_precision_4: 0.7038 - val_recall_4: 0.8242 - 801ms/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "509/509 - 1s - loss: 0.6333 - precision_4: 0.7151 - recall_4: 0.7383 - val_loss: 0.6107 - val_precision_4: 0.7461 - val_recall_4: 0.7175 - 905ms/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "509/509 - 1s - loss: 0.6319 - precision_4: 0.7167 - recall_4: 0.7381 - val_loss: 0.6066 - val_precision_4: 0.7190 - val_recall_4: 0.7877 - 945ms/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "509/509 - 1s - loss: 0.6300 - precision_4: 0.7150 - recall_4: 0.7367 - val_loss: 0.6010 - val_precision_4: 0.7159 - val_recall_4: 0.7948 - 767ms/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "509/509 - 1s - loss: 0.6282 - precision_4: 0.7138 - recall_4: 0.7403 - val_loss: 0.6084 - val_precision_4: 0.7282 - val_recall_4: 0.7655 - 662ms/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "509/509 - 1s - loss: 0.6277 - precision_4: 0.7144 - recall_4: 0.7356 - val_loss: 0.5987 - val_precision_4: 0.7235 - val_recall_4: 0.7805 - 690ms/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "509/509 - 1s - loss: 0.6238 - precision_4: 0.7163 - recall_4: 0.7423 - val_loss: 0.6018 - val_precision_4: 0.7132 - val_recall_4: 0.8006 - 673ms/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "509/509 - 1s - loss: 0.6234 - precision_4: 0.7155 - recall_4: 0.7425 - val_loss: 0.5942 - val_precision_4: 0.7127 - val_recall_4: 0.8038 - 658ms/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "509/509 - 1s - loss: 0.6215 - precision_4: 0.7150 - recall_4: 0.7417 - val_loss: 0.5994 - val_precision_4: 0.7223 - val_recall_4: 0.7830 - 651ms/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "509/509 - 1s - loss: 0.6205 - precision_4: 0.7134 - recall_4: 0.7400 - val_loss: 0.5920 - val_precision_4: 0.7157 - val_recall_4: 0.7941 - 655ms/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "509/509 - 1s - loss: 0.6167 - precision_4: 0.7161 - recall_4: 0.7419 - val_loss: 0.5958 - val_precision_4: 0.7153 - val_recall_4: 0.7941 - 670ms/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "509/509 - 1s - loss: 0.6174 - precision_4: 0.7161 - recall_4: 0.7404 - val_loss: 0.5887 - val_precision_4: 0.7105 - val_recall_4: 0.8084 - 677ms/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "509/509 - 1s - loss: 0.6164 - precision_4: 0.7164 - recall_4: 0.7427 - val_loss: 0.5952 - val_precision_4: 0.7212 - val_recall_4: 0.7809 - 650ms/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "509/509 - 1s - loss: 0.6125 - precision_4: 0.7156 - recall_4: 0.7423 - val_loss: 0.5859 - val_precision_4: 0.7078 - val_recall_4: 0.8135 - 681ms/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "509/509 - 1s - loss: 0.6132 - precision_4: 0.7146 - recall_4: 0.7454 - val_loss: 0.5949 - val_precision_4: 0.7322 - val_recall_4: 0.7576 - 662ms/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "509/509 - 1s - loss: 0.6124 - precision_4: 0.7172 - recall_4: 0.7450 - val_loss: 0.5864 - val_precision_4: 0.7336 - val_recall_4: 0.7544 - 666ms/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "509/509 - 1s - loss: 0.6108 - precision_4: 0.7167 - recall_4: 0.7432 - val_loss: 0.5914 - val_precision_4: 0.7253 - val_recall_4: 0.7791 - 684ms/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "509/509 - 1s - loss: 0.6094 - precision_4: 0.7153 - recall_4: 0.7456 - val_loss: 0.5855 - val_precision_4: 0.6985 - val_recall_4: 0.8378 - 651ms/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "509/509 - 1s - loss: 0.6096 - precision_4: 0.7158 - recall_4: 0.7474 - val_loss: 0.5889 - val_precision_4: 0.7123 - val_recall_4: 0.8059 - 828ms/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "509/509 - 1s - loss: 0.6072 - precision_4: 0.7170 - recall_4: 0.7471 - val_loss: 0.5820 - val_precision_4: 0.7080 - val_recall_4: 0.8127 - 957ms/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "509/509 - 1s - loss: 0.6059 - precision_4: 0.7152 - recall_4: 0.7503 - val_loss: 0.5871 - val_precision_4: 0.7161 - val_recall_4: 0.7920 - 937ms/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "509/509 - 1s - loss: 0.6059 - precision_4: 0.7155 - recall_4: 0.7470 - val_loss: 0.5801 - val_precision_4: 0.7308 - val_recall_4: 0.7612 - 697ms/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "509/509 - 1s - loss: 0.6027 - precision_4: 0.7182 - recall_4: 0.7524 - val_loss: 0.5853 - val_precision_4: 0.7284 - val_recall_4: 0.7683 - 671ms/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "509/509 - 1s - loss: 0.6033 - precision_4: 0.7177 - recall_4: 0.7503 - val_loss: 0.5780 - val_precision_4: 0.7232 - val_recall_4: 0.7791 - 654ms/epoch - 1ms/step\n",
            "Epoch 57/100\n",
            "509/509 - 1s - loss: 0.6022 - precision_4: 0.7170 - recall_4: 0.7513 - val_loss: 0.5839 - val_precision_4: 0.7223 - val_recall_4: 0.7852 - 683ms/epoch - 1ms/step\n",
            "Epoch 58/100\n",
            "509/509 - 1s - loss: 0.6014 - precision_4: 0.7158 - recall_4: 0.7511 - val_loss: 0.5769 - val_precision_4: 0.7245 - val_recall_4: 0.7794 - 658ms/epoch - 1ms/step\n",
            "Epoch 59/100\n",
            "509/509 - 1s - loss: 0.6012 - precision_4: 0.7176 - recall_4: 0.7528 - val_loss: 0.5835 - val_precision_4: 0.7307 - val_recall_4: 0.7608 - 666ms/epoch - 1ms/step\n",
            "Epoch 60/100\n",
            "509/509 - 1s - loss: 0.5987 - precision_4: 0.7162 - recall_4: 0.7510 - val_loss: 0.5758 - val_precision_4: 0.7019 - val_recall_4: 0.8253 - 676ms/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "509/509 - 1s - loss: 0.5979 - precision_4: 0.7163 - recall_4: 0.7517 - val_loss: 0.5807 - val_precision_4: 0.7213 - val_recall_4: 0.7859 - 665ms/epoch - 1ms/step\n",
            "Epoch 62/100\n",
            "509/509 - 1s - loss: 0.5971 - precision_4: 0.7161 - recall_4: 0.7521 - val_loss: 0.5728 - val_precision_4: 0.7109 - val_recall_4: 0.8049 - 636ms/epoch - 1ms/step\n",
            "Epoch 63/100\n",
            "509/509 - 1s - loss: 0.5971 - precision_4: 0.7149 - recall_4: 0.7474 - val_loss: 0.5788 - val_precision_4: 0.7265 - val_recall_4: 0.7762 - 659ms/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "509/509 - 1s - loss: 0.5955 - precision_4: 0.7172 - recall_4: 0.7529 - val_loss: 0.5710 - val_precision_4: 0.7184 - val_recall_4: 0.7920 - 659ms/epoch - 1ms/step\n",
            "Epoch 65/100\n",
            "509/509 - 1s - loss: 0.5930 - precision_4: 0.7175 - recall_4: 0.7539 - val_loss: 0.5771 - val_precision_4: 0.7320 - val_recall_4: 0.7598 - 659ms/epoch - 1ms/step\n",
            "Epoch 66/100\n",
            "509/509 - 1s - loss: 0.5928 - precision_4: 0.7174 - recall_4: 0.7533 - val_loss: 0.5694 - val_precision_4: 0.7176 - val_recall_4: 0.7905 - 677ms/epoch - 1ms/step\n",
            "Epoch 67/100\n",
            "509/509 - 1s - loss: 0.5904 - precision_4: 0.7168 - recall_4: 0.7554 - val_loss: 0.5754 - val_precision_4: 0.7288 - val_recall_4: 0.7648 - 665ms/epoch - 1ms/step\n",
            "Epoch 68/100\n",
            "509/509 - 1s - loss: 0.5928 - precision_4: 0.7163 - recall_4: 0.7559 - val_loss: 0.5682 - val_precision_4: 0.7253 - val_recall_4: 0.7741 - 660ms/epoch - 1ms/step\n",
            "Epoch 69/100\n",
            "509/509 - 1s - loss: 0.5887 - precision_4: 0.7177 - recall_4: 0.7567 - val_loss: 0.5715 - val_precision_4: 0.7126 - val_recall_4: 0.8042 - 926ms/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "509/509 - 1s - loss: 0.5884 - precision_4: 0.7189 - recall_4: 0.7587 - val_loss: 0.5663 - val_precision_4: 0.7171 - val_recall_4: 0.7952 - 931ms/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "509/509 - 1s - loss: 0.5893 - precision_4: 0.7158 - recall_4: 0.7559 - val_loss: 0.5700 - val_precision_4: 0.7141 - val_recall_4: 0.8006 - 894ms/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "509/509 - 1s - loss: 0.5872 - precision_4: 0.7182 - recall_4: 0.7561 - val_loss: 0.5668 - val_precision_4: 0.7284 - val_recall_4: 0.7673 - 673ms/epoch - 1ms/step\n",
            "Epoch 73/100\n",
            "509/509 - 1s - loss: 0.5850 - precision_4: 0.7192 - recall_4: 0.7598 - val_loss: 0.5699 - val_precision_4: 0.7000 - val_recall_4: 0.8346 - 667ms/epoch - 1ms/step\n",
            "Epoch 74/100\n",
            "509/509 - 1s - loss: 0.5850 - precision_4: 0.7171 - recall_4: 0.7559 - val_loss: 0.5632 - val_precision_4: 0.7120 - val_recall_4: 0.8063 - 666ms/epoch - 1ms/step\n",
            "Epoch 75/100\n",
            "509/509 - 1s - loss: 0.5840 - precision_4: 0.7190 - recall_4: 0.7570 - val_loss: 0.5663 - val_precision_4: 0.7225 - val_recall_4: 0.7841 - 671ms/epoch - 1ms/step\n",
            "Epoch 76/100\n",
            "509/509 - 1s - loss: 0.5842 - precision_4: 0.7172 - recall_4: 0.7598 - val_loss: 0.5632 - val_precision_4: 0.7290 - val_recall_4: 0.7676 - 656ms/epoch - 1ms/step\n",
            "Epoch 77/100\n",
            "509/509 - 1s - loss: 0.5838 - precision_4: 0.7195 - recall_4: 0.7609 - val_loss: 0.5657 - val_precision_4: 0.7238 - val_recall_4: 0.7834 - 670ms/epoch - 1ms/step\n",
            "Epoch 78/100\n",
            "509/509 - 1s - loss: 0.5804 - precision_4: 0.7167 - recall_4: 0.7591 - val_loss: 0.5620 - val_precision_4: 0.7163 - val_recall_4: 0.7974 - 637ms/epoch - 1ms/step\n",
            "Epoch 79/100\n",
            "509/509 - 1s - loss: 0.5811 - precision_4: 0.7194 - recall_4: 0.7609 - val_loss: 0.5638 - val_precision_4: 0.7233 - val_recall_4: 0.7834 - 658ms/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "509/509 - 1s - loss: 0.5810 - precision_4: 0.7174 - recall_4: 0.7614 - val_loss: 0.5609 - val_precision_4: 0.7093 - val_recall_4: 0.8106 - 686ms/epoch - 1ms/step\n",
            "Epoch 81/100\n",
            "509/509 - 1s - loss: 0.5805 - precision_4: 0.7183 - recall_4: 0.7603 - val_loss: 0.5608 - val_precision_4: 0.7165 - val_recall_4: 0.7974 - 660ms/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "509/509 - 1s - loss: 0.5801 - precision_4: 0.7173 - recall_4: 0.7608 - val_loss: 0.5607 - val_precision_4: 0.7191 - val_recall_4: 0.7948 - 662ms/epoch - 1ms/step\n",
            "Epoch 83/100\n",
            "509/509 - 1s - loss: 0.5804 - precision_4: 0.7154 - recall_4: 0.7583 - val_loss: 0.5612 - val_precision_4: 0.7204 - val_recall_4: 0.7934 - 666ms/epoch - 1ms/step\n",
            "Epoch 84/100\n",
            "509/509 - 1s - loss: 0.5769 - precision_4: 0.7186 - recall_4: 0.7642 - val_loss: 0.5584 - val_precision_4: 0.7141 - val_recall_4: 0.8006 - 641ms/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "509/509 - 1s - loss: 0.5779 - precision_4: 0.7177 - recall_4: 0.7616 - val_loss: 0.5610 - val_precision_4: 0.7102 - val_recall_4: 0.8088 - 668ms/epoch - 1ms/step\n",
            "Epoch 86/100\n",
            "509/509 - 1s - loss: 0.5769 - precision_4: 0.7175 - recall_4: 0.7599 - val_loss: 0.5582 - val_precision_4: 0.7100 - val_recall_4: 0.8092 - 673ms/epoch - 1ms/step\n",
            "Epoch 87/100\n",
            "509/509 - 1s - loss: 0.5770 - precision_4: 0.7160 - recall_4: 0.7611 - val_loss: 0.5595 - val_precision_4: 0.7156 - val_recall_4: 0.7974 - 933ms/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "509/509 - 1s - loss: 0.5768 - precision_4: 0.7178 - recall_4: 0.7625 - val_loss: 0.5577 - val_precision_4: 0.7117 - val_recall_4: 0.8034 - 898ms/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "509/509 - 1s - loss: 0.5756 - precision_4: 0.7177 - recall_4: 0.7630 - val_loss: 0.5589 - val_precision_4: 0.7199 - val_recall_4: 0.7931 - 1s/epoch - 3ms/step\n",
            "Epoch 90/100\n",
            "509/509 - 1s - loss: 0.5746 - precision_4: 0.7186 - recall_4: 0.7647 - val_loss: 0.5578 - val_precision_4: 0.7109 - val_recall_4: 0.8081 - 1s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "509/509 - 1s - loss: 0.5730 - precision_4: 0.7181 - recall_4: 0.7675 - val_loss: 0.5562 - val_precision_4: 0.7240 - val_recall_4: 0.7812 - 962ms/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "509/509 - 1s - loss: 0.5739 - precision_4: 0.7197 - recall_4: 0.7642 - val_loss: 0.5560 - val_precision_4: 0.7219 - val_recall_4: 0.7855 - 806ms/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "509/509 - 1s - loss: 0.5741 - precision_4: 0.7178 - recall_4: 0.7629 - val_loss: 0.5568 - val_precision_4: 0.7264 - val_recall_4: 0.7701 - 656ms/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "509/509 - 1s - loss: 0.5751 - precision_4: 0.7188 - recall_4: 0.7611 - val_loss: 0.5573 - val_precision_4: 0.7205 - val_recall_4: 0.7884 - 686ms/epoch - 1ms/step\n",
            "Epoch 95/100\n",
            "509/509 - 1s - loss: 0.5736 - precision_4: 0.7157 - recall_4: 0.7620 - val_loss: 0.5549 - val_precision_4: 0.7227 - val_recall_4: 0.7848 - 673ms/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "509/509 - 1s - loss: 0.5737 - precision_4: 0.7193 - recall_4: 0.7628 - val_loss: 0.5578 - val_precision_4: 0.7280 - val_recall_4: 0.7780 - 657ms/epoch - 1ms/step\n",
            "Epoch 97/100\n",
            "509/509 - 1s - loss: 0.5741 - precision_4: 0.7182 - recall_4: 0.7591 - val_loss: 0.5541 - val_precision_4: 0.7124 - val_recall_4: 0.8052 - 630ms/epoch - 1ms/step\n",
            "Epoch 98/100\n",
            "509/509 - 1s - loss: 0.5729 - precision_4: 0.7193 - recall_4: 0.7622 - val_loss: 0.5565 - val_precision_4: 0.7187 - val_recall_4: 0.7931 - 654ms/epoch - 1ms/step\n",
            "Epoch 99/100\n",
            "509/509 - 1s - loss: 0.5732 - precision_4: 0.7208 - recall_4: 0.7613 - val_loss: 0.5533 - val_precision_4: 0.7206 - val_recall_4: 0.7895 - 644ms/epoch - 1ms/step\n",
            "Epoch 100/100\n",
            "509/509 - 1s - loss: 0.5711 - precision_4: 0.7219 - recall_4: 0.7625 - val_loss: 0.5561 - val_precision_4: 0.7159 - val_recall_4: 0.8002 - 667ms/epoch - 1ms/step\n",
            "442/442 [==============================] - 0s 1ms/step - loss: 0.5556 - precision_4: 0.7237 - recall_4: 0.7885\n",
            "binary cross-entropy loss :  0.5556368231773376  precision:  0.7237028479576111  recal:  0.7884696125984192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature reduction result: DNN\n",
        "DNN also proved to be consistant after feature reduction. It even had a mild improvement(which is again, negligible)."
      ],
      "metadata": {
        "id": "tLuJEkwX8gUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature reduction overall result\n",
        "So to conclude, we were able to predict our target feature with an acceptable accuracy even after feature reduction. The following is the list of remaining features which proved to be decisive: DiffWalk, Age, HighChol, BMI, HighBP, GenHlth\n"
      ],
      "metadata": {
        "id": "HDGtLOsC88QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to do: search for improvement techniques\n",
        "# rename functions to snake_case\n",
        "# save final cooef"
      ],
      "metadata": {
        "id": "4DSyQcxh8f66"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}